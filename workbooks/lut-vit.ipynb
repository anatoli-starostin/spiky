{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageNet Baseline - Simple ResNet\n",
    "\n",
    "This notebook implements a simple baseline on ImageNet using a ResNet architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.2+cu121\n",
      "Device: cuda:5\n"
     ]
    }
   ],
   "source": [
    "# Global preparations\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, os.path.expanduser('~/spiky'))\n",
    "device = 'cuda:5'\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.backends.cudnn.enabled = True\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "ImageNet dataset loading with standard augmentations for training and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CIFAR-100 as a proxy dataset. Replace with ImageNet for full experiments.\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train dataset size: 50000\n",
      "Val dataset size: 10000\n",
      "Number of classes: 100\n"
     ]
    }
   ],
   "source": [
    "# ImageNet data paths (adjust these to your ImageNet location)\n",
    "imagenet_train_path = '/path/to/imagenet/train'  # Update this path\n",
    "imagenet_val_path = '/path/to/imagenet/val'      # Update this path\n",
    "\n",
    "# Standard ImageNet normalization\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Training transforms with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# Validation transforms (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# Note: You need to have ImageNet dataset downloaded\n",
    "# For demonstration, we'll use ImageNet-like dataset or CIFAR-100 as a proxy\n",
    "# Uncomment and update paths when you have ImageNet available\n",
    "\"\"\"\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=imagenet_train_path,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=imagenet_val_path,\n",
    "    transform=val_transform\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Using CIFAR-100 as a smaller proxy for testing (same structure as ImageNet)\n",
    "# Replace with ImageNet when available\n",
    "print(\"Using CIFAR-100 as a proxy dataset. Replace with ImageNet for full experiments.\")\n",
    "\n",
    "train_transform_cifar = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "val_transform_cifar = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=train_transform_cifar\n",
    ")\n",
    "\n",
    "val_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=val_transform_cifar\n",
    ")\n",
    "\n",
    "batch_size = 512\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")\n",
    "print(f\"Number of classes: {len(train_dataset.classes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualise few samples\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(train_dataset[i][0].permute(1, 2, 0))\n",
    "    plt.title(train_dataset[i][1])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "We'll use ResNet18 as a simple baseline. This can be easily replaced with ResNet50 or other architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet18 and modify for our dataset\n",
    "# For CIFAR-100, we'll modify the first conv layer to handle 32x32 images\n",
    "# For ImageNet (224x224), use standard ResNet18\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "if hasattr(train_dataset, 'classes'):\n",
    "    # CIFAR-100 or similar small dataset\n",
    "    model = torchvision.models.resnet18(weights=None)  # Start from scratch\n",
    "    # Modify first conv layer for smaller input\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()  # Remove maxpool for smaller images\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "else:\n",
    "    # ImageNet\n",
    "    model = torchvision.models.resnet18(weights=None)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model: ResNet18\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Define loss function, optimizer, and learning rate scheduler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=30,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "print(\"Optimizer: SGD with momentum 0.9, weight decay 1e-4\")\n",
    "print(\"Initial learning rate: 0.1\")\n",
    "print(\"Scheduler: StepLR (reduce by 0.1 every 30 epochs)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, leave=False)\n",
    "    for inputs, targets in pbar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        pbar.set_description(f'Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, leave=False)\n",
    "        for inputs, targets in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            pbar.set_description(f'Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100  # Adjust as needed\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Learning rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    print('-' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(train_losses, label='Train Loss', color='blue')\n",
    "ax1.plot(val_losses, label='Val Loss', color='red')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(train_accs, label='Train Acc', color='blue')\n",
    "ax2.plot(val_accs, label='Val Acc', color='red')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best validation accuracy: {max(val_accs):.2f}% at epoch {val_accs.index(max(val_accs))+1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LUT-based Vision Transformer (LUTViT)\n",
    "\n",
    "Adapting SimpleViT to use LUT layers similar to LUTTransformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUTNet components initialized\n"
     ]
    }
   ],
   "source": [
    "# Import LUTNet components\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.expanduser('~/spiky'))\n",
    "\n",
    "from spiky.lut.LUTLayer import (\n",
    "    Conv2DLUTLayer, LUTLayer, LUTSharedContext, SynapseMeta, GradientPolicy, GradientType\n",
    ")\n",
    "from spiky.util.torch_utils import make_lr_getter\n",
    "\n",
    "# Setup LUTNet shared context and metadata\n",
    "summation_dtype = torch.float32\n",
    "\n",
    "synapse_meta = SynapseMeta(\n",
    "    min_weight=-1.0,\n",
    "    max_weight=1.0,\n",
    "    initial_weight=0.0,\n",
    "    initial_noise_level=0.0\n",
    ")\n",
    "\n",
    "shared_lut_ctx = LUTSharedContext()\n",
    "shared_lut_ctx.to_device(device)\n",
    "g_policy = GradientPolicy(GradientType.Dense, normalized=False)\n",
    "\n",
    "print(\"LUTNet components initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LUT-based Vision Transformer adapted from SimpleViT\n",
    "class LUTViT(nn.Module):\n",
    "    def __init__(self, image_size=32, patch_size=4, in_chans=3,\n",
    "                 num_classes=100, dim=32, depth=6, num_heads=8, \n",
    "                 n_detectors=128, n_anchors_per_detector=10, \n",
    "                 n_anchors_per_detector_attention=None,\n",
    "                 dropout=0.1, use_batch_norm=False,\n",
    "                 device=None, _synapse_meta=None, \n",
    "                 lut_shared_context=None, seed=None, \n",
    "                 summation_dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert image_size % patch_size == 0\n",
    "        n_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = in_chans * patch_size * patch_size\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = n_patches\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        if device is None:\n",
    "            device = torch.device('cpu')\n",
    "        self.device = device\n",
    "        \n",
    "        if _synapse_meta is None:\n",
    "            _synapse_meta = SynapseMeta(\n",
    "                min_weight=-1.0,\n",
    "                max_weight=1.0,\n",
    "                initial_weight=0.0,\n",
    "                initial_noise_level=0.0\n",
    "            )\n",
    "        \n",
    "        if lut_shared_context is None:\n",
    "            lut_shared_context = LUTSharedContext()\n",
    "            lut_shared_context.to_device(device)\n",
    "        \n",
    "        self.lut_shared_context = lut_shared_context\n",
    "        self.weights_gradient_policy = g_policy\n",
    "        \n",
    "        # Patch embedding: linear layer to embed patches\n",
    "        self.patch_to_emb = nn.Linear(patch_dim, dim, device=device)\n",
    "        \n",
    "        # CLS token and positional embeddings\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, dim, device=device))\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize CLS and positional embeddings\n",
    "        nn.init.trunc_normal_(self.cls, std=0.02)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList()\n",
    "        n_anchors_attn = n_anchors_per_detector_attention if n_anchors_per_detector_attention is not None else n_anchors_per_detector\n",
    "        \n",
    "        for layer_idx in range(depth):\n",
    "            block = nn.ModuleDict()\n",
    "            \n",
    "            # Multi-head attention using LUTLayer\n",
    "            # Use a single LUTLayer for attention (similar to LUTTransformer)\n",
    "            # The LUTLayer processes the sequence with sequence_length = 1 + n_patches\n",
    "            block['attention_lut'] = LUTLayer(\n",
    "                n_inputs=dim,\n",
    "                n_outputs=dim,\n",
    "                positional_dim=dim,\n",
    "                concatenation_product=False,\n",
    "                n_detectors=n_detectors * num_heads,  # Scale detectors for multi-head capacity\n",
    "                n_anchors_per_detector=n_anchors_attn,\n",
    "                sequence_length=1 + n_patches,  # CLS + patches\n",
    "                synapse_meta=_synapse_meta,\n",
    "                weights_gradient_policy=self.weights_gradient_policy,\n",
    "                shared_context=self.lut_shared_context,\n",
    "                summation_dtype=summation_dtype,\n",
    "                random_seed=None if seed is None else seed + layer_idx * num_heads,\n",
    "                device=device\n",
    "            )\n",
    "            block['attention_dropout'] = nn.Dropout(dropout)\n",
    "            \n",
    "            if use_batch_norm:\n",
    "                block['attention_bn'] = nn.BatchNorm1d(dim, device=device)\n",
    "            else:\n",
    "                block['attention_bn'] = None\n",
    "            \n",
    "            # FFN using LUTLayer\n",
    "            # FFN processes each token independently (sequence_length=1)\n",
    "            block['ffn'] = LUTLayer(\n",
    "                n_inputs=dim,\n",
    "                n_outputs=dim,\n",
    "                n_detectors=n_detectors,\n",
    "                n_anchors_per_detector=n_anchors_per_detector,\n",
    "                sequence_length=1,  # Process tokens independently\n",
    "                synapse_meta=_synapse_meta,\n",
    "                weights_gradient_policy=self.weights_gradient_policy,\n",
    "                shared_context=self.lut_shared_context,\n",
    "                summation_dtype=summation_dtype,\n",
    "                random_seed=None if seed is None else seed + layer_idx * num_heads + num_heads,\n",
    "                device=device\n",
    "            )\n",
    "            block['ffn_dropout'] = nn.Dropout(dropout)\n",
    "            \n",
    "            if use_batch_norm:\n",
    "                block['ffn_bn'] = nn.BatchNorm1d(dim, device=device)\n",
    "            else:\n",
    "                block['ffn_bn'] = None\n",
    "            \n",
    "            self.blocks.append(block)\n",
    "        \n",
    "        # Final normalization and classification head\n",
    "        self.norm = nn.LayerNorm(dim, device=device)\n",
    "        self.head = LUTLayer(\n",
    "            n_inputs=dim,\n",
    "            n_outputs=num_classes,\n",
    "            n_detectors=n_detectors,\n",
    "            n_anchors_per_detector=n_anchors_per_detector,\n",
    "            sequence_length=1,\n",
    "            synapse_meta=_synapse_meta,\n",
    "            weights_gradient_policy=self.weights_gradient_policy,\n",
    "            shared_context=self.lut_shared_context,\n",
    "            summation_dtype=summation_dtype,\n",
    "            random_seed=seed,\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        P = self.patch_size\n",
    "        \n",
    "        # Patch embedding: (B, C, H, W) -> (B, N, patch_dim) -> (B, N, dim)\n",
    "        x = x.unfold(2, P, P).unfold(3, P, P)  # (B, C, Hp, Wp, P, P)\n",
    "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()  # (B, Hp, Wp, C, P, P)\n",
    "        x = x.view(B, -1, C * P * P)  # (B, N, patch_dim)\n",
    "        x = self.patch_to_emb(x)  # (B, N, dim)\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls.expand(B, -1, -1)  # (B, 1, dim)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (B, 1+N, dim)\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            residual = x\n",
    "            \n",
    "            # Multi-head attention using LUTLayer\n",
    "            # LUTLayer processes sequence: (B, 1+N, dim) -> (B, 1+N, dim)\n",
    "            # The sequence_length=1+N means it processes the whole sequence\n",
    "            attn_out = block['attention_lut'](x)  # (B, 1+N, dim)\n",
    "            attn_out = block['attention_dropout'](attn_out)\n",
    "            \n",
    "            if block['attention_bn'] is not None:\n",
    "                # BatchNorm: (B, 1+N, dim) -> (B*(1+N), dim) -> (B, 1+N, dim)\n",
    "                attn_out_flat = attn_out.reshape(-1, self.dim)\n",
    "                attn_out_flat = block['attention_bn'](attn_out_flat)\n",
    "                attn_out = attn_out_flat.reshape(B, 1 + self.n_patches, self.dim)\n",
    "            \n",
    "            x = residual + attn_out  # Residual connection\n",
    "            \n",
    "            # FFN with residual connection\n",
    "            residual = x\n",
    "            # Reshape for FFN: (B, 1+N, dim) -> (B*(1+N), 1, dim) for sequence_length=1\n",
    "            non_seq_shape = (B * (1 + self.n_patches), 1, self.dim)\n",
    "            seq_shape = (B, 1 + self.n_patches, self.dim)\n",
    "            ffn_out = block['ffn'](x.reshape(non_seq_shape))  # (B*(1+N), 1, dim)\n",
    "            ffn_out = ffn_out.reshape(seq_shape)  # (B, 1+N, dim)\n",
    "            ffn_out = block['ffn_dropout'](ffn_out)\n",
    "            \n",
    "            if block['ffn_bn'] is not None:\n",
    "                # BatchNorm: (B, 1+N, dim) -> (B*(1+N), dim) -> (B, 1+N, dim)\n",
    "                ffn_out_flat = ffn_out.reshape(-1, self.dim)\n",
    "                ffn_out_flat = block['ffn_bn'](ffn_out_flat)\n",
    "                ffn_out = ffn_out_flat.reshape(B, 1 + self.n_patches, self.dim)\n",
    "            \n",
    "            x = residual + ffn_out  # Residual connection\n",
    "        \n",
    "        # Final normalization and classification\n",
    "        x = self.norm(x)  # (B, 1+N, dim)\n",
    "        cls_token = x[:, 0]  # (B, dim) - take CLS token\n",
    "        cls_token = cls_token.unsqueeze(1)  # (B, 1, dim)\n",
    "        logits = self.head(cls_token)  # (B, 1, num_classes)\n",
    "        return logits.squeeze(1)  # (B, num_classes)\n",
    "    \n",
    "    def set_external_learning_rate_hook(self, lr_hook):\n",
    "        \"\"\"Set learning rate hooks for all LUT layers\"\"\"\n",
    "        for block in self.blocks:\n",
    "            block['attention_lut'].set_external_learning_rate_hook(lr_hook)\n",
    "            block['ffn'].set_external_learning_rate_hook(lr_hook)\n",
    "        self.head.set_external_learning_rate_hook(lr_hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUTViT Model\n",
      "Total parameters: 856,164,736\n",
      "Trainable parameters: 856,164,736\n",
      "Number of classes: 100\n"
     ]
    }
   ],
   "source": [
    "# Create LUTViT model for CIFAR-10\n",
    "lut_vit = LUTViT(\n",
    "    image_size=32,\n",
    "    patch_size=4,\n",
    "    in_chans=3,\n",
    "    num_classes=len(train_dataset.classes),\n",
    "    dim=32,\n",
    "    depth=6,\n",
    "    num_heads=8,\n",
    "    n_detectors=128,\n",
    "    n_anchors_per_detector=10,\n",
    "    n_anchors_per_detector_attention=12,\n",
    "    dropout=0.1,\n",
    "    use_batch_norm=True,\n",
    "    device=device,\n",
    "    _synapse_meta=synapse_meta,\n",
    "    lut_shared_context=shared_lut_ctx,\n",
    "    seed=random_seed,\n",
    "    summation_dtype=summation_dtype\n",
    ")\n",
    "\n",
    "lut_vit = lut_vit.to(device)\n",
    "\n",
    "# Count parameters\n",
    "lut_vit_total_params = sum(p.numel() for p in lut_vit.parameters())\n",
    "lut_vit_trainable_params = sum(p.numel() for p in lut_vit.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"LUTViT Model\")\n",
    "print(f\"Total parameters: {lut_vit_total_params:,}\")\n",
    "print(f\"Trainable parameters: {lut_vit_trainable_params:,}\")\n",
    "print(f\"Number of classes: {len(train_dataset.classes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUTViT optimizer: SGD with lr=0.1, momentum=0.9, weight_decay=1e-4\n",
      "Scheduler: StepLR (reduce by 0.1 every 30 epochs)\n"
     ]
    }
   ],
   "source": [
    "# Setup optimizer for LUTViT\n",
    "lut_vit_optimizer = torch.optim.SGD(\n",
    "    lut_vit.parameters(),\n",
    "    lr=0.1,\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "lut_vit_sched = torch.optim.lr_scheduler.StepLR(\n",
    "    lut_vit_optimizer,\n",
    "    step_size=30,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "# Set up learning rate hooks for LUT layers\n",
    "from spiky.util.torch_utils import make_lr_getter\n",
    "lut_vit_lr_getter = make_lr_getter(lut_vit_optimizer)\n",
    "# lut_vit.set_external_learning_rate_hook(lut_vit_lr_getter)\n",
    "\n",
    "print(\"LUTViT optimizer: SGD with lr=0.1, momentum=0.9, weight_decay=1e-4\")\n",
    "print(\"Scheduler: StepLR (reduce by 0.1 every 30 epochs)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions for LUTNet\n",
    "def train_one_epoch_lut(model, train_loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, leave=False)\n",
    "    for inputs, targets in pbar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         if scheduler is not None:\n",
    "#             for _ in range(inputs.shape[0]):\n",
    "#                 scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        pbar.set_description(\n",
    "            f'Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%, '\n",
    "            f'lr: {scheduler.get_last_lr()[0] if scheduler is not None else lr:.4f}'\n",
    "        )\n",
    "    \n",
    "    scheduler.step()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate_lut(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, leave=False)\n",
    "        for inputs, targets in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            pbar.set_description(f'Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LUTViT Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LUTViT training for 100 epochs...\n",
      "Batch size: 512\n",
      "Device: cuda:5\n",
      "\n",
      "LUTViT Epoch 1/100\n",
      "Learning rate: 0.100000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9178d7f372483f8f88e9797ce32267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLearning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlut_vit_sched\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch_lut\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlut_vit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlut_vit_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlut_vit_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlut_vit_sched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m lut_vit_train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     23\u001b[0m lut_vit_train_accs\u001b[38;5;241m.\u001b[39mappend(train_acc)\n",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m, in \u001b[0;36mtrain_one_epoch_lut\u001b[0;34m(model, train_loader, criterion, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/vr_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vr_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 179\u001b[0m, in \u001b[0;36mLUTViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    177\u001b[0m cls_token \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (B, dim) - take CLS token\u001b[39;00m\n\u001b[1;32m    178\u001b[0m cls_token \u001b[38;5;241m=\u001b[39m cls_token\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, 1, dim)\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, 1, num_classes)\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/vr_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vr_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/spiky/src/spiky/lut/LUTLayer.py:1264\u001b[0m, in \u001b[0;36mLUTLayerBasic.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sequence_length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLUTLayerBasic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLUTForwardFN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1266\u001b[0m         pos_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_positional_embeddings()\n",
      "File \u001b[0;32m~/vr_venv/lib/python3.8/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/spiky/src/spiky/lut/LUTLayer.py:1189\u001b[0m, in \u001b[0;36mLUTLayerBasic.LUTForwardFN.forward\u001b[0;34m(ctx, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1184\u001b[0m ctx\u001b[38;5;241m.\u001b[39mlut_layer \u001b[38;5;241m=\u001b[39m lut_layer\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lut_layer\u001b[38;5;241m.\u001b[39m_sequence_length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1186\u001b[0m     (\n\u001b[1;32m   1187\u001b[0m         output, lookup_indices, min_anchor_deltas,\n\u001b[1;32m   1188\u001b[0m         min_anchor_delta_indices\n\u001b[0;32m-> 1189\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mlut_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m     ctx\u001b[38;5;241m.\u001b[39msave_for_backward(x, lookup_indices, min_anchor_deltas, min_anchor_delta_indices)\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lut_layer\u001b[38;5;241m.\u001b[39m_concatenation_product:\n",
      "File \u001b[0;32m~/spiky/src/spiky/lut/LUTLayer.py:690\u001b[0m, in \u001b[0;36mLUTLayerBasic.forward_step\u001b[0;34m(self, x, output)\u001b[0m\n\u001b[1;32m    687\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sequence_length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput sequence_length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match constructor sequence_length 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 690\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m     external_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# LUTViT training\n",
    "lut_vit_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lut_vit_num_epochs = 100\n",
    "lut_vit_train_losses = []\n",
    "lut_vit_train_accs = []\n",
    "lut_vit_val_losses = []\n",
    "lut_vit_val_accs = []\n",
    "\n",
    "print(f\"Starting LUTViT training for {lut_vit_num_epochs} epochs...\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "for epoch in range(lut_vit_num_epochs):\n",
    "    print(f'LUTViT Epoch {epoch+1}/{lut_vit_num_epochs}')\n",
    "    print(f'Learning rate: {lut_vit_sched.get_last_lr()[0]:.6f}')\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch_lut(\n",
    "        lut_vit, train_loader, lut_vit_criterion, lut_vit_optimizer, lut_vit_sched, device\n",
    "    )\n",
    "    lut_vit_train_losses.append(train_loss)\n",
    "    lut_vit_train_accs.append(train_acc)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = evaluate_lut(lut_vit, val_loader, lut_vit_criterion, device)\n",
    "    lut_vit_val_losses.append(val_loss)\n",
    "    lut_vit_val_accs.append(val_acc)\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    print('-' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot LUTViT Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LUTViT training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(lut_vit_train_losses, label='Train Loss', color='blue')\n",
    "ax1.plot(lut_vit_val_losses, label='Val Loss', color='red')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('LUTViT Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(lut_vit_train_accs, label='Train Acc', color='blue')\n",
    "ax2.plot(lut_vit_val_accs, label='Val Acc', color='red')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('LUTViT Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if len(lut_vit_val_accs) > 0:\n",
    "    print(f\"LUTViT Best validation accuracy: {max(lut_vit_val_accs):.2f}% at epoch {lut_vit_val_accs.index(max(lut_vit_val_accs))+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(mlp_dim, dim),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_dim, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn  = nn.MultiheadAttention(dim, heads, dropout=drop, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp   = MLP(dim, mlp_dim, drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x), need_weights=False)\n",
    "        x = x + y\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=16, in_chans=3,\n",
    "                 num_classes=1000, dim=768, depth=12, heads=12, mlp_dim=3072, drop=0.0):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0\n",
    "        n_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = in_chans * patch_size * patch_size\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_to_emb = nn.Linear(patch_dim, dim)\n",
    "\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        self.pos = nn.Parameter(torch.zeros(1, 1 + n_patches, dim))\n",
    "        self.pos_drop = nn.Dropout(drop)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            EncoderBlock(dim, heads, mlp_dim, drop) for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, num_classes)\n",
    "\n",
    "        nn.init.trunc_normal_(self.pos, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        P = self.patch_size\n",
    "        # (B, C, H, W) -> (B, N, patch_dim)\n",
    "        x = x.unfold(2, P, P).unfold(3, P, P)                # B,C,Hp,Wp,P,P\n",
    "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()         # B,Hp,Wp,C,P,P\n",
    "        x = x.view(B, -1, C * P * P)                         # B,N,patch_dim\n",
    "\n",
    "        x = self.patch_to_emb(x)                             # B,N,D\n",
    "        cls = self.cls.expand(B, -1, -1)                     # B,1,D\n",
    "        x = torch.cat([cls, x], dim=1)                       # B,1+N,D\n",
    "        x = self.pos_drop(x + self.pos[:, : x.size(1)])\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        return self.head(x[:, 0])                            # CLS\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
