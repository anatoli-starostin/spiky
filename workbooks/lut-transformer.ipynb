{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import logging\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.insert(0, os.path.expanduser('~/spiky'))\n",
    "device = 'cuda:3'\n",
    "summation_dtype = torch.float32\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.backends.cudnn.enabled = True\n",
    "logger = logging.getLogger('stdout_logger')\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.hasHandlers():\n",
    "    stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "    stdout_handler.setFormatter(logging.Formatter(fmt='%(asctime)s|%(levelname)s|%(message)s'))\n",
    "    logger.addHandler(stdout_handler)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text data preparation\n",
    "\n",
    "Let's read tinyshakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spiky.util.text_snippet_sampler import TextSnippetSampler\n",
    "\n",
    "#snippet_sampler = TextSnippetSampler('tinyshakespeare.txt', CONTEXT_SIZE + 1, 1000, device)\n",
    "snippet_sampler = TextSnippetSampler('fineweb_texts.txt', CONTEXT_SIZE + 1, 1000, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[111, 117, 114,  32, 116, 101,  97,  99, 104, 101, 114,  32, 102, 111,\n",
       "         114,  32,  97,  32, 115, 116, 117, 100, 121,  32, 103, 117, 105, 100,\n",
       "         101,  46,  32,  73, 102],\n",
       "        [115, 104, 105, 111, 110,  32,  97, 110, 100,  32,  99, 108, 111, 116,\n",
       "         104, 105, 110, 103,  32,  97, 114, 101,  97, 115,  44,  32,  97,  32,\n",
       "         118, 101, 114, 121,  32]], device='cuda:3', dtype=torch.int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet_sampler.sample_training_batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d is fallen from heaven, and hath',\n",
       " 'ts in solitary confinement. Adult',\n",
       " 'ECG Technology in the Medical Wor',\n",
       " 're easily.\\n\"Similar projects over']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet_sampler.batch_to_text(snippet_sampler.sample_training_batch(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' in any other higher education se', 'n Vidal discovers, other causes a', 'ers, gorillas, and other celebrit', 'ination of their representatives,']\n"
     ]
    }
   ],
   "source": [
    "for test_batch in snippet_sampler.testing_batches_iterator(4):\n",
    "    print(snippet_sampler.batch_to_text(test_batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LUT transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUTTransformer(\n",
      "  (token_embedder): Embedding(256, 32)\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x ModuleDict(\n",
      "      (attention_lut): LUTLayer(32 inputs, 64 detectors, 32 outputs, 6 anchors per detector)\n",
      "      (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (attention_bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (attention_ln): None\n",
      "      (ffn): LUTLayer(32 inputs, 16 detectors, 32 outputs, 10 anchors per detector)\n",
      "      (ffn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ffn_bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (ffn_ln): None\n",
      "    )\n",
      "  )\n",
      "  (unembedder): LUTLayer(32 inputs, 16 detectors, 256 outputs, 10 anchors per detector)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from spiky.lut.LUTLayer import GradientPolicy, GradientType, SynapseMeta\n",
    "from spiky.lut.LUTTransformer import LUTTransformer\n",
    "from spiky.lut.tests.gt_lut_product import GTLUTProductTransformer\n",
    "lut_transformer = None\n",
    "optimizer = None\n",
    "sched = None\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "lut_transformer = LUTTransformer(\n",
    "    vocab_size=256,\n",
    "    embedding_dim=32,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    positional_dim=4,\n",
    "    # no_ffn=True,\n",
    "    # use_sinusoidal_pe=True,\n",
    "    # inject_pe_once=True,\n",
    "    num_layers=6,\n",
    "    num_heads=4,\n",
    "    n_detectors=16,\n",
    "    n_anchors_per_detector_attention=6,\n",
    "    n_anchors_per_detector=10,\n",
    "    concatenation_product=True,\n",
    "    dropout=0.0,\n",
    "    use_batch_norm=True,\n",
    "    use_biases=True,\n",
    "    layer_norm_d=None,\n",
    "    #_synapse_meta=SynapseMeta(min_weight=0.0, max_weight=5.0, initial_weight=5.0, initial_noise_level=-5.0),\n",
    "    #do_normalise_weights=True,\n",
    "    weights_gradient_policy=GradientPolicy(GradientType.Dense),\n",
    "    relu_before_luts=False,\n",
    "    device=device,\n",
    "    seed=random_seed,\n",
    "    _forward_group_size=32,\n",
    "    _backward_group_size=32\n",
    ")\n",
    "# lut_transformer = GTLUTProductTransformer(\n",
    "#     vocab_size=256,\n",
    "#     embedding_dim=32,\n",
    "#     context_size=CONTEXT_SIZE,\n",
    "#     positional_dim=8,\n",
    "#     num_layers=6,\n",
    "#     num_heads=4,\n",
    "#     n_detectors=32,\n",
    "#     n_anchors_per_detector_attention=8,\n",
    "#     n_anchors_per_detector=10,\n",
    "#     device=device,\n",
    "#     seed=None,\n",
    "#     _forward_group_size=64,\n",
    "#     _backward_group_size=64\n",
    "# )\n",
    "print(lut_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 812703616\n",
      "trainable: 812695424\n",
      "frozen: 8192\n"
     ]
    }
   ],
   "source": [
    "total = sum(p.numel() for p in lut_transformer.parameters())\n",
    "trainable = sum(p.numel() for p in lut_transformer.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"total:\", total)\n",
    "print(\"trainable:\", trainable)\n",
    "print(\"frozen:\", total - trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Iterable, Optional, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class CustomAdam(torch.optim.Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[torch.nn.Parameter],\n",
    "        lr: float = 1e-3,\n",
    "        betas: Tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-8,\n",
    "        weight_decay: float = 0.0,  # L2-style (coupled) like torch.optim.Adam\n",
    "    ):\n",
    "        if lr < 0:\n",
    "            raise ValueError(\"lr must be >= 0\")\n",
    "        b1, b2 = betas\n",
    "        if not (0 <= b1 < 1 and 0 <= b2 < 1):\n",
    "            raise ValueError(\"betas must be in [0,1)\")\n",
    "        if eps <= 0:\n",
    "            raise ValueError(\"eps must be > 0\")\n",
    "        if weight_decay < 0:\n",
    "            raise ValueError(\"weight_decay must be >= 0\")\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure: Optional[callable] = None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr: float = group[\"lr\"]\n",
    "            b1, b2 = group[\"betas\"]\n",
    "            eps: float = group[\"eps\"]\n",
    "            wd: float = group[\"weight_decay\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                if p.grad.is_sparse:\n",
    "                    raise RuntimeError(\"SimpleAdam does not support sparse gradients\")\n",
    "\n",
    "                g = p.grad\n",
    "\n",
    "                # \"Coupled\" weight decay (L2 penalty) like torch.optim.Adam\n",
    "                if wd != 0.0:\n",
    "                    g = g.add(p, alpha=wd)\n",
    "\n",
    "                state: Dict = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"m\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state[\"v\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                m = state[\"m\"]\n",
    "                v = state[\"v\"]\n",
    "                state[\"step\"] += 1\n",
    "                t = state[\"step\"]\n",
    "\n",
    "                # Update biased first/second moments\n",
    "                m.mul_(b1).add_(g, alpha=1.0 - b1)\n",
    "                v.mul_(b2).addcmul_(g, g, value=1.0 - b2)\n",
    "\n",
    "                # Bias correction\n",
    "                m_hat = m / (1.0 - b1 ** t)\n",
    "                v_hat = v / (1.0 - b2 ** t)\n",
    "\n",
    "                # Parameter update\n",
    "                p.addcdiv_(m_hat, v_hat.sqrt().add_(eps), value=-lr)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from spiky.util.torch_utils import make_lr_getter\n",
    "\n",
    "def lr_func(t):\n",
    "    return min(\n",
    "        0.001 / (1 + t)**0.5,\n",
    "        (t / 4000.0) / 4000.0**0.5\n",
    "    )\n",
    "\n",
    "lr = 0.001\n",
    "#optimizer = optim.SGD(lut_transformer.parameters(), lr=lr)\n",
    "#optimizer = optim.Adam(lut_transformer.parameters(), lr=lr)\n",
    "optimizer = CustomAdam(lut_transformer.parameters(), lr=lr)\n",
    "\n",
    "steps=150000\n",
    "batch_size = 64\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=steps\n",
    ")\n",
    "#sched = LambdaLR(optimizer, lr_lambda=lr_func)\n",
    "#sched = None\n",
    "#lut_transformer.set_external_learning_rate_hook(make_lr_getter(optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut_train_losses = []\n",
    "lut_test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "test_batch_size = 128\n",
    "\n",
    "def generate_text_lut(lut_model, prefix, length, device):\n",
    "    ctx = list(prefix.encode(\"utf-8\"))\n",
    "    ctx = ctx[-CONTEXT_SIZE:]\n",
    "\n",
    "    for _ in range(length):\n",
    "        x = torch.zeros([CONTEXT_SIZE], dtype=torch.long, device=device).unsqueeze(0)\n",
    "        trunc_ctx = ctx[-CONTEXT_SIZE:]\n",
    "        x[0, -len(trunc_ctx):] = torch.tensor(trunc_ctx, dtype=torch.long, device=device) \n",
    "        logits = lut_model(x)\n",
    "        probs = torch.softmax(logits[:,-1,:], dim=-1)[0]\n",
    "        next_id = torch.multinomial(probs, 1).item()\n",
    "        ctx.append(next_id)\n",
    "\n",
    "    ctx_safe = [c if c != 0 else 32 for c in ctx]\n",
    "        \n",
    "    return bytes(ctx_safe).decode(\"latin1\", errors=\"ignore\")\n",
    "\n",
    "def evaluate_model(model, sampler, B):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in sampler.testing_batches_iterator(B):   # [B, C]\n",
    "            inp = batch[:, :-1]          # [B, C-1]\n",
    "            tgt = batch[:, 1:].long()    # [B, C-1]\n",
    "\n",
    "            logits = model(inp)   # [B, C-1, 256]\n",
    "\n",
    "            B_, T, V = logits.shape\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(B_ * T, V),\n",
    "                tgt.reshape(B_ * T)\n",
    "            )\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    # ---- small generation demo ----\n",
    "    prefix = \"Once upon a time \"\n",
    "    gen = generate_text_lut(model, prefix, length=80, device=device)\n",
    "    print(\"\\n[GEN]:\", gen, \"\\n\")\n",
    "\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses) #  / (CONTEXT_SIZE * B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3214feddb42d454f9dacf5d7876f2787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GEN]: Once upon a time ínZ»z\u0014Æ®7ð#Yz/µIì¶\u0001\u001d",
      "x6v*[Xð´ë3;ÛÑU¿SÝ\f",
      "µ¹½ñ^\u0004ñ}g#ÿóëmôUÑn$Su\"`äÈ5MU \n",
      "\n",
      "[TEST] step 0: loss=5.5171\n",
      "\n",
      "[GEN]: Once upon a time un she all is hapand skestlan thWigh, Neleas forage hasts an21199\n",
      "E/_ r as  who \n",
      "\n",
      "[TEST] step 1000: loss=2.1456\n",
      "\n",
      "[GEN]: Once upon a time nee this action. (1348%.\n",
      "CMVU propesshouban Surcorabing Kiftwhilosing and seshou \n",
      "\n",
      "[TEST] step 2000: loss=1.9037\n",
      "\n",
      "[GEN]: Once upon a time nativodomeans invour ordinates sivilizate loop to no/gensnens her rieted are is  \n",
      "\n",
      "[TEST] step 3000: loss=1.7954\n",
      "\n",
      "[GEN]: Once upon a time stemge\n",
      "Amer more than 180 and state lift is pice, sip goe, and descresswho myvic \n",
      "\n",
      "[TEST] step 4000: loss=1.7399\n",
      "\n",
      "[GEN]: Once upon a time deformine, or under (hride participation and who work the restevet to the sensis \n",
      "\n",
      "[TEST] step 5000: loss=1.6955\n",
      "\n",
      "[GEN]: Once upon a time meditable presentence a referetage of an itrony nerative its, jained for his a t \n",
      "\n",
      "[TEST] step 6000: loss=1.6760\n",
      "\n",
      "[GEN]: Once upon a time for whibe nd descule, begap, gellwate mane grous whet impulse in the late our a \n",
      "\n",
      "[TEST] step 7000: loss=1.6494\n",
      "\n",
      "[GEN]: Once upon a time patients similation! Througen bonoments for everyptical companided, of its halle \n",
      "\n",
      "[TEST] step 8000: loss=1.6299\n",
      "\n",
      "[GEN]: Once upon a time the answer in it at the term of\n",
      "mass matilled will we mambrife, while humb obser \n",
      "\n",
      "[TEST] step 9000: loss=1.6120\n",
      "\n",
      "[GEN]: Once upon a time in a 2013 May 71 Seattatones before sharl in the Clock a Directory todation. To  \n",
      "\n",
      "[TEST] step 10000: loss=1.5973\n",
      "\n",
      "[GEN]: Once upon a time of mainstreaters\" ellows yoursed Permodery.\n",
      "The have analy. The besonâs worked \n",
      "\n",
      "[TEST] step 11000: loss=1.5858\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m inp \u001b[38;5;241m=\u001b[39m x[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]                                         \u001b[38;5;66;03m# [B, C-1]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m tgt \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mlong()                                   \u001b[38;5;66;03m# [B, C-1]\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mlut_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m# [B, C-1, 256]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m B, T, V \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[1;32m     17\u001b[0m     logits\u001b[38;5;241m.\u001b[39mreshape(B \u001b[38;5;241m*\u001b[39m T, V),\n\u001b[1;32m     18\u001b[0m     tgt\u001b[38;5;241m.\u001b[39mreshape(B \u001b[38;5;241m*\u001b[39m T),\n\u001b[1;32m     19\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m )\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m~/vr_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vr_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/spiky/src/spiky/lut/LUTTransformer.py:305\u001b[0m, in \u001b[0;36mLUTTransformer.forward\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu_before_luts:\n\u001b[1;32m    304\u001b[0m     z \u001b[38;5;241m=\u001b[39m nf\u001b[38;5;241m.\u001b[39mrelu(z)\n\u001b[0;32m--> 305\u001b[0m ffn_result \u001b[38;5;241m=\u001b[39m (\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mffn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnon_seq_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mreshape(seq_shape)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_biases:\n\u001b[1;32m    307\u001b[0m     ffn_result \u001b[38;5;241m=\u001b[39m ffn_result \u001b[38;5;241m+\u001b[39m layer\u001b[38;5;241m.\u001b[39mffn_bias\n",
      "File \u001b[0;32m~/vr_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vr_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/spiky/src/spiky/lut/LUTLayer.py:1272\u001b[0m, in \u001b[0;36mLUTLayerBasic.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sequence_length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1272\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLUTLayerBasic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLUTForwardFN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1274\u001b[0m         pos_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_positional_embeddings()\n",
      "File \u001b[0;32m~/vr_venv/lib/python3.8/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/spiky/src/spiky/lut/LUTLayer.py:1197\u001b[0m, in \u001b[0;36mLUTLayerBasic.LUTForwardFN.forward\u001b[0;34m(ctx, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m ctx\u001b[38;5;241m.\u001b[39mlut_layer \u001b[38;5;241m=\u001b[39m lut_layer\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lut_layer\u001b[38;5;241m.\u001b[39m_sequence_length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1194\u001b[0m     (\n\u001b[1;32m   1195\u001b[0m         output, lookup_indices, min_anchor_deltas,\n\u001b[1;32m   1196\u001b[0m         min_anchor_delta_indices\n\u001b[0;32m-> 1197\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mlut_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m     ctx\u001b[38;5;241m.\u001b[39msave_for_backward(x, lookup_indices, min_anchor_deltas, min_anchor_delta_indices)\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lut_layer\u001b[38;5;241m.\u001b[39m_concatenation_product:\n",
      "File \u001b[0;32m~/spiky/src/spiky/lut/LUTLayer.py:720\u001b[0m, in \u001b[0;36mLUTLayerBasic.forward_step\u001b[0;34m(self, x, output)\u001b[0m\n\u001b[1;32m    716\u001b[0m     min_anchor_delta_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    718\u001b[0m stream_handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shared_context\u001b[38;5;241m.\u001b[39mget_cuda_streams(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_id) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 720\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lut_dm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_detector_anchors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlookup_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shared_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_eps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_handles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_anchor_deltas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_anchor_delta_indices\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m external_output:\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_synchronize()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_every=1000\n",
    "train_loss = None\n",
    "alpha = 0.01\n",
    "\n",
    "pbar = tqdm(total=steps)\n",
    "lut_transformer.train()\n",
    "\n",
    "for step in range(0, steps + 1):\n",
    "    x = snippet_sampler.sample_training_batch(batch_size)   # [B, C]\n",
    "    inp = x[:, :-1]                                         # [B, C-1]\n",
    "    tgt = x[:, 1:].long()                                   # [B, C-1]\n",
    "\n",
    "    logits = lut_transformer(inp)      # [B, C-1, 256]\n",
    "\n",
    "    B, T, V = logits.shape\n",
    "    loss = F.cross_entropy(\n",
    "        logits.reshape(B * T, V),\n",
    "        tgt.reshape(B * T),\n",
    "        reduction='none'\n",
    "    ).sum()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if sched is not None:\n",
    "        # for _ in range(x.shape[0]):\n",
    "        sched.step()\n",
    "\n",
    "    loss_value = loss.item() / (CONTEXT_SIZE * batch_size)\n",
    "    train_loss = loss_value if train_loss is None else (1 - alpha) * train_loss + alpha * loss_value\n",
    "    pbar.update(1)\n",
    "    if step % 10 == 0:\n",
    "        pbar.set_description(f\"loss={train_loss:.4f}, lr {lr if sched is None else sched.get_last_lr()[0]:.8f}\")\n",
    "\n",
    "    if step % test_every == 0:\n",
    "        test_loss = evaluate_model(lut_transformer, snippet_sampler, test_batch_size)\n",
    "        if len(lut_train_losses) == 0 or step > 0:\n",
    "            lut_train_losses.append(train_loss)\n",
    "            lut_test_losses.append(test_loss)\n",
    "        print(f\"[TEST] step {step}: loss={test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# assume train_losses and test_losses are Python lists of equal length\n",
    "\n",
    "steps = [i * 1000 for i in range(len(lut_train_losses))]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(steps, lut_train_losses, label=\"train\")\n",
    "plt.plot(steps, lut_test_losses, label=\"test\")\n",
    "\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4303922802209854"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lut_test_losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# assume train_losses and test_losses are Python lists of equal length\n",
    "\n",
    "steps = [i * 1000 for i in range(len(lut_train_losses))]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(steps, lut_train_losses, label=\"train\")\n",
    "plt.plot(steps, lut_test_losses, label=\"test\")\n",
    "\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text_lut(lut_transformer, 'Once upon a time: ', length=80, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lut_transformer.get_profile_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(lut_transformer, snippet_sampler, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from spiky.lut.LUTLayer import (\n",
    "    LUTLayer,\n",
    "    Conv2DLUTLayer,\n",
    "    LUTSharedContext,\n",
    "    GradientPolicy,\n",
    "    GradientType,\n",
    "    MultiLUT,\n",
    "    SynapseMeta\n",
    ")\n",
    "\n",
    "from spiky.lut.tests.gt_lut_product import GTLUTProduct\n",
    "\n",
    "class LUTTransformerExp(nn.Module):\n",
    "    def _create_single_attention(\n",
    "        self, _synapse_meta, summation_dtype, _int_rescaler, seed,\n",
    "        _forward_group_size, _backward_group_size, num_heads\n",
    "    ):\n",
    "#         return LUTLayer(\n",
    "#             n_inputs=self.embedding_dim,\n",
    "#             n_outputs=self.embedding_dim,\n",
    "#             n_detectors=self.n_detectors * num_heads,\n",
    "#             n_anchors_per_detector=self.n_anchors_per_detector_attention,\n",
    "#             sequence_length=self.context_size,\n",
    "#             synapse_meta=_synapse_meta,\n",
    "#             positional_dim=self.positional_dim,\n",
    "#             weights_gradient_policy=self.weights_gradient_policy,\n",
    "#             shared_context=self.lut_shared_context,\n",
    "#             summation_dtype=summation_dtype,\n",
    "#             _int_rescaler=_int_rescaler,\n",
    "#             device=self.device,\n",
    "#             random_seed=seed,\n",
    "#             _forward_group_size=_forward_group_size,\n",
    "#             _backward_group_size=_backward_group_size\n",
    "#         )\n",
    "        return GTLUTProduct(\n",
    "            n_inputs_1=self.embedding_dim,\n",
    "            n_inputs_2=self.embedding_dim,\n",
    "            positional_dim=self.positional_dim,\n",
    "            n_outputs=self.embedding_dim,\n",
    "            sequence_length=self.context_size,\n",
    "            sliced_mode=False,\n",
    "            n_detectors=self.n_detectors * num_heads,\n",
    "            n_anchors_per_detector=self.n_anchors_per_detector_attention,\n",
    "            synapse_meta=_synapse_meta,\n",
    "            weights_gradient_policy=self.weights_gradient_policy,\n",
    "            shared_context=self.lut_shared_context,\n",
    "            summation_dtype=summation_dtype,\n",
    "            random_seed=seed,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self, vocab_size, embedding_dim, context_size,\n",
    "        positional_dim, num_layers, num_heads,\n",
    "        n_detectors, n_anchors_per_detector, weights_gradient_policy=None,\n",
    "        device=None, _synapse_meta=SynapseMeta(), _use_multi_lut=False,\n",
    "        lut_shared_context=None, seed=None, summation_dtype=torch.float32, _int_rescaler=0.001,\n",
    "        _forward_group_size=32, _backward_group_size=32, dropout=0.1,\n",
    "        n_anchors_per_detector_attention=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.context_size = context_size\n",
    "        self.positional_dim = positional_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.n_detectors = n_detectors\n",
    "        self.n_anchors_per_detector = n_anchors_per_detector\n",
    "        # If not specified, use the same value as n_anchors_per_detector for backward compatibility\n",
    "        self.n_anchors_per_detector_attention = n_anchors_per_detector_attention if n_anchors_per_detector_attention is not None else n_anchors_per_detector\n",
    "        self.weights_gradient_policy = weights_gradient_policy\n",
    "        self.dropout = dropout\n",
    "        if device is None:\n",
    "            device = torch.device('cpu')\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        if lut_shared_context is None:\n",
    "            self.lut_shared_context = LUTSharedContext()\n",
    "            self.lut_shared_context.to_device(device)\n",
    "        else:\n",
    "            self.lut_shared_context = lut_shared_context\n",
    "\n",
    "        n_embeddings = embedding_dim\n",
    "\n",
    "        self.token_embedder = nn.Embedding(vocab_size, n_embeddings, device=device)\n",
    "        self.token_embedder.weight.requires_grad_(False)\n",
    "        if seed is not None:\n",
    "            gen = torch.Generator(device=device)\n",
    "            gen.manual_seed(seed)\n",
    "            w = 2 * torch.rand(self.token_embedder.weight.shape, generator=gen, device=device) - 1.0\n",
    "            self.token_embedder.weight.copy_(w)\n",
    "        else:\n",
    "            nn.init.uniform_(self.token_embedder.weight, -1.0, 1.0)\n",
    "\n",
    "        # Dropout after embeddings\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#         pe = build_sinusoidal_pe(context_size, positional_dim, device)\n",
    "#         self.register_buffer(\"pos_emb\", pe.unsqueeze(0))\n",
    "\n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for layer_idx in range(num_layers):\n",
    "            layer = nn.ModuleDict()\n",
    "\n",
    "            # Attention heads\n",
    "            if _use_multi_lut:\n",
    "                heads = []\n",
    "                for head_idx in range(num_heads):\n",
    "                    attention_lut = self._create_single_attention(\n",
    "                        _synapse_meta=_synapse_meta, summation_dtype=summation_dtype,\n",
    "                        _int_rescaler=_int_rescaler,\n",
    "                        seed=None if seed is None else seed + layer_idx * num_heads + head_idx,\n",
    "                        _forward_group_size=_forward_group_size,\n",
    "                        _backward_group_size=_backward_group_size,\n",
    "                        num_heads=1\n",
    "                    )\n",
    "                    heads.append(attention_lut)\n",
    "                layer['attention_lut'] = MultiLUT(heads)\n",
    "            else:\n",
    "                assert isinstance(embedding_dim, int)\n",
    "                layer['attention_lut'] = self._create_single_attention(\n",
    "                    _synapse_meta=_synapse_meta, summation_dtype=summation_dtype,\n",
    "                    _int_rescaler=_int_rescaler,\n",
    "                    seed=None if seed is None else seed + layer_idx * num_heads,\n",
    "                    _forward_group_size=_forward_group_size,\n",
    "                    _backward_group_size=_backward_group_size,\n",
    "                    num_heads=num_heads\n",
    "                )\n",
    "\n",
    "            # Dropout after attention\n",
    "            layer['attention_dropout'] = nn.Dropout(dropout)\n",
    "\n",
    "            ffn_lut = LUTLayer(\n",
    "                n_inputs=n_embeddings,\n",
    "                n_outputs=n_embeddings,\n",
    "                n_detectors=n_detectors,\n",
    "                n_anchors_per_detector=n_anchors_per_detector,\n",
    "                sequence_length=1,  # sequence is processed via simple reshape: [B, S, E] -> [B * S, 1, E]\n",
    "                synapse_meta=_synapse_meta,\n",
    "                weights_gradient_policy=weights_gradient_policy,\n",
    "                shared_context=self.lut_shared_context,\n",
    "                summation_dtype=summation_dtype,\n",
    "                _int_rescaler=_int_rescaler,\n",
    "                device=device,\n",
    "                random_seed=None if seed is None else seed + layer_idx * num_heads + num_heads,\n",
    "                _forward_group_size=_forward_group_size,\n",
    "                _backward_group_size=_backward_group_size\n",
    "            )\n",
    "            layer['ffn'] = ffn_lut\n",
    "\n",
    "            # Dropout after FFN\n",
    "            layer['ffn_dropout'] = nn.Dropout(dropout)\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.unembedder = LUTLayer(\n",
    "            n_inputs=n_embeddings,\n",
    "            n_outputs=vocab_size,\n",
    "            n_detectors=n_detectors,\n",
    "            n_anchors_per_detector=n_anchors_per_detector,\n",
    "            sequence_length=1,  # sequence is processed via simple reshape: [B, S, E] -> [B * S, 1, E]\n",
    "            synapse_meta=_synapse_meta,\n",
    "            weights_gradient_policy=weights_gradient_policy,\n",
    "            shared_context=self.lut_shared_context,\n",
    "            summation_dtype=summation_dtype,\n",
    "            _int_rescaler=_int_rescaler,\n",
    "            device=device,\n",
    "            random_seed=seed,\n",
    "            _forward_group_size=_forward_group_size,\n",
    "            _backward_group_size=_backward_group_size\n",
    "        )\n",
    "\n",
    "    def set_external_learning_rate_hook(self, lr_hook):\n",
    "        # Set hooks for all LUT layers\n",
    "        for layer in self.layers:\n",
    "            layer['attention_lut'].set_external_learning_rate_hook(lr_hook)\n",
    "            layer['ffn'].set_external_learning_rate_hook(lr_hook)\n",
    "        self.unembedder.set_external_learning_rate_hook(lr_hook)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            tokens: (batch_size, context_size) tensor of token indices\n",
    "\n",
    "        Returns:\n",
    "            logits: (batch_size, context_size, vocab_size) tensor of logits\n",
    "        \"\"\"\n",
    "        batch_size = tokens.shape[0]\n",
    "        # Token embedding: (batch_size, context_size) -> (batch_size, context_size, n_embeddings)\n",
    "        z = self.token_embedder(tokens)  # (batch_size, context_size, n_embeddings)\n",
    "        \n",
    "        # Apply dropout after embeddings\n",
    "        z = self.embedding_dropout(z)\n",
    "\n",
    "        non_seq_shape = (batch_size * self.context_size, 1, self.embedding_dim)\n",
    "        seq_shape = (batch_size, self.context_size, self.embedding_dim)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # Attention with residual connection and dropout\n",
    "#            attention_output = layer['attention_lut'](torch.cat([z, self.pos_emb.repeat(z.shape[0], 1, 1)], dim=-1))\n",
    "            attention_output = layer['attention_lut'](z, z)\n",
    "            attention_output = layer['attention_dropout'](attention_output)\n",
    "            z = z + attention_output\n",
    "            # FFN with residual connection and dropout\n",
    "            ffn_output = (layer['ffn'](z.reshape(non_seq_shape))).reshape(seq_shape)\n",
    "            ffn_output = layer['ffn_dropout'](ffn_output)\n",
    "            z = z + ffn_output\n",
    "\n",
    "        # Unembedder: (batch_size, context_size, n_embeddings) -> (batch_size, context_size, vocab_size)\n",
    "        logits = self.unembedder(z.reshape(non_seq_shape)).reshape(batch_size, self.context_size, self.vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def _reset_shared_context(self, new_context):\n",
    "        for layer in self.layers:\n",
    "            layer['attention_lut']._reset_shared_context(new_context)\n",
    "            layer['ffn']._reset_shared_context(new_context)\n",
    "        self.unembedder._reset_shared_context(new_context)\n",
    "\n",
    "    def get_profile_statistics(self) -> str:\n",
    "        \"\"\"\n",
    "        Get aggregated profiling statistics from all LUT layers in the transformer.\n",
    "        Only includes lut::runtime operations, grouped and averaged by component type.\n",
    "        \n",
    "        Returns:\n",
    "            String with aggregated profiling statistics in the format:\n",
    "            - Average Attention metrics (averaged across all attention layers)\n",
    "            - Average FFN metrics (averaged across all FFN layers)\n",
    "            - Unembedder metrics (single layer)\n",
    "            Format: operation_name: total_time ms / total_count = avg_time ms\n",
    "        \"\"\"\n",
    "        import re\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        # Collect LUT layers by component type\n",
    "        attention_luts = []\n",
    "        ffn_luts = []\n",
    "        unembedder_lut = self.unembedder\n",
    "        \n",
    "        # Collect from transformer layers\n",
    "        for layer in self.layers:\n",
    "            # Handle attention_lut (can be LUTLayer or MultiLUT)\n",
    "            attention_lut = layer['attention_lut']\n",
    "            if isinstance(attention_lut, MultiLUT):\n",
    "                attention_luts.extend(attention_lut.luts)\n",
    "            else:\n",
    "                attention_luts.append(attention_lut)\n",
    "            \n",
    "            # FFN is always a single LUTLayer\n",
    "            ffn_luts.append(layer['ffn'])\n",
    "        \n",
    "        # Aggregate statistics by component type\n",
    "        attention_stats = defaultdict(lambda: {'total_time': 0.0, 'total_count': 0})\n",
    "        ffn_stats = defaultdict(lambda: {'total_time': 0.0, 'total_count': 0})\n",
    "        unembedder_stats = defaultdict(lambda: {'total_time': 0.0, 'total_count': 0})\n",
    "        \n",
    "        def parse_and_aggregate(profiling_stats, stats_dict):\n",
    "            \"\"\"Parse profiling stats and aggregate into stats_dict\"\"\"\n",
    "            for line in profiling_stats.split('\\n'):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                # Only process lut::runtime lines\n",
    "                if 'lut::runtime' not in line:\n",
    "                    continue\n",
    "                \n",
    "                # Parse: operation_name: total_time ms / count = avg_time ms\n",
    "                # Example: lut::runtime::forward_step: 11750.9 ms / 1164 = 10.0952 ms\n",
    "                match = re.match(r'^([^:]+(?:::[^:]+)*):\\s+([\\d.]+)\\s+ms\\s+/\\s+(\\d+)\\s+=\\s+([\\d.-]+)\\s+ms$', line)\n",
    "                if match:\n",
    "                    op_name = match.group(1)\n",
    "                    total_time = float(match.group(2))\n",
    "                    count = int(match.group(3))\n",
    "                    \n",
    "                    stats_dict[op_name]['total_time'] += total_time\n",
    "                    stats_dict[op_name]['total_count'] += count\n",
    "        \n",
    "        # Parse statistics from attention layers\n",
    "        for lut in attention_luts:\n",
    "            parse_and_aggregate(lut.get_profiling_stats(), attention_stats)\n",
    "        \n",
    "        # Parse statistics from FFN layers\n",
    "        for lut in ffn_luts:\n",
    "            parse_and_aggregate(lut.get_profiling_stats(), ffn_stats)\n",
    "        \n",
    "        # Parse statistics from unembedder\n",
    "        parse_and_aggregate(unembedder_lut.get_profiling_stats(), unembedder_stats)\n",
    "        \n",
    "        # Format output\n",
    "        result_lines = []\n",
    "        \n",
    "        # Average Attention metrics\n",
    "        if attention_luts:\n",
    "            result_lines.append(\"Average Attention:\")\n",
    "            n_layers = len(attention_luts)\n",
    "            # Sort by average total_time (total_time / n_layers) descending\n",
    "            sorted_attention = sorted(\n",
    "                attention_stats.items(),\n",
    "                key=lambda x: x[1]['total_time'] / n_layers,\n",
    "                reverse=True\n",
    "            )\n",
    "            for op_name, stats in sorted_attention:\n",
    "                total_time = stats['total_time']\n",
    "                total_count = stats['total_count']\n",
    "                \n",
    "                if total_count > 0:\n",
    "                    avg_time = total_time / total_count\n",
    "                    avg_total_time = total_time / n_layers\n",
    "                    avg_count = total_count / n_layers\n",
    "                    result_lines.append(f\"  {op_name}: {avg_total_time:.6g} ms / {avg_count:.1f} = {avg_time:.6g} ms\")\n",
    "                else:\n",
    "                    avg_total_time = total_time / n_layers\n",
    "                    result_lines.append(f\"  {op_name}: {avg_total_time:.6g} ms / 0 = -nan ms\")\n",
    "            result_lines.append(\"\")\n",
    "        \n",
    "        # Average FFN metrics\n",
    "        if ffn_luts:\n",
    "            result_lines.append(\"Average FFN:\")\n",
    "            n_layers = len(ffn_luts)\n",
    "            # Sort by average total_time (total_time / n_layers) descending\n",
    "            sorted_ffn = sorted(\n",
    "                ffn_stats.items(),\n",
    "                key=lambda x: x[1]['total_time'] / n_layers,\n",
    "                reverse=True\n",
    "            )\n",
    "            for op_name, stats in sorted_ffn:\n",
    "                total_time = stats['total_time']\n",
    "                total_count = stats['total_count']\n",
    "                \n",
    "                if total_count > 0:\n",
    "                    avg_time = total_time / total_count\n",
    "                    avg_total_time = total_time / n_layers\n",
    "                    avg_count = total_count / n_layers\n",
    "                    result_lines.append(f\"  {op_name}: {avg_total_time:.6g} ms / {avg_count:.1f} = {avg_time:.6g} ms\")\n",
    "                else:\n",
    "                    avg_total_time = total_time / n_layers\n",
    "                    result_lines.append(f\"  {op_name}: {avg_total_time:.6g} ms / 0 = -nan ms\")\n",
    "            result_lines.append(\"\")\n",
    "        \n",
    "        # Unembedder metrics\n",
    "        result_lines.append(\"Unembedder:\")\n",
    "        # Sort by total_time descending\n",
    "        sorted_unembedder = sorted(\n",
    "            unembedder_stats.items(),\n",
    "            key=lambda x: x[1]['total_time'],\n",
    "            reverse=True\n",
    "        )\n",
    "        for op_name, stats in sorted_unembedder:\n",
    "            total_time = stats['total_time']\n",
    "            total_count = stats['total_count']\n",
    "            \n",
    "            if total_count > 0:\n",
    "                avg_time = total_time / total_count\n",
    "                result_lines.append(f\"  {op_name}: {total_time:.6g} ms / {total_count} = {avg_time:.6g} ms\")\n",
    "            else:\n",
    "                result_lines.append(f\"  {op_name}: {total_time:.6g} ms / {total_count} = -nan ms\")\n",
    "        \n",
    "        return '\\n'.join(result_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text_lut_greedy(lut_model, prefix: str, length: int, device):\n",
    "    lut_model.eval()\n",
    "\n",
    "    ctx = list(prefix.encode(\"utf-8\"))[-CONTEXT_SIZE:]\n",
    "\n",
    "    for _ in range(length):\n",
    "        x = torch.zeros((1, CONTEXT_SIZE), dtype=torch.long, device=device)\n",
    "        trunc_ctx = ctx[-CONTEXT_SIZE:]\n",
    "        x[0, -len(trunc_ctx):] = torch.tensor(trunc_ctx, dtype=torch.long, device=device)\n",
    "\n",
    "        logits = lut_model(x)              # [1, T, V]\n",
    "        next_id = logits[0, -1].argmax().item()\n",
    "        ctx.append(next_id)\n",
    "\n",
    "    ctx_safe = [c if c != 0 else 32 for c in ctx]\n",
    "    return bytes(ctx_safe).decode(\"latin1\", errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text_lut_greedy(lut_transformer, 'Once upon a time: ', length=80, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = torch.arange(32 - 1, device=device).to(dtype=torch.float32)\n",
    "position = position.unsqueeze(1).repeat(1, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1984])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embeddings = torch.empty(\n",
    "    (32 - 1) * 2 * 32,\n",
    "    dtype=torch.float32,\n",
    "    device=device\n",
    ")\n",
    "# Initialize with random floats in [-1, 1]\n",
    "pos_embeddings.uniform_(-1.0, 1.0)\n",
    "pos_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embeddings = pos_embeddings.reshape(pos_embeddings.numel() // 2, 2)\n",
    "pos_embeddings = torch.sin(position.flatten() * pos_embeddings[:, 0] + pos_embeddings[:, 1])\n",
    "pos_embeddings = pos_embeddings.flatten().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.0992e-01, -6.1644e-01,  3.6863e-01, -1.8879e-01,  5.1036e-02,\n",
       "        -7.6701e-01, -7.2161e-01, -7.8230e-01, -2.0477e-01, -2.7855e-01,\n",
       "         6.1000e-01, -3.0977e-01,  8.1953e-01, -4.1961e-02,  2.4700e-02,\n",
       "         3.4703e-02,  7.8283e-01, -6.2421e-01,  8.2298e-01, -5.9524e-01,\n",
       "         3.3243e-01,  4.2252e-01, -3.4772e-01, -6.5524e-01,  5.9893e-01,\n",
       "        -5.1943e-01,  8.8808e-02, -1.7337e-01,  1.4011e-01, -7.5241e-02,\n",
       "        -1.1445e-01,  7.0579e-01,  7.1246e-02, -7.9627e-01,  2.6739e-01,\n",
       "         3.3919e-01,  3.3141e-01, -7.2335e-01,  4.6721e-02, -6.1252e-01,\n",
       "         1.9261e-03, -8.3799e-01,  7.0087e-01,  9.9026e-01, -9.8443e-01,\n",
       "        -3.6206e-01,  7.7181e-01,  9.7365e-01, -6.4509e-01,  2.9733e-01,\n",
       "        -8.4219e-02,  2.4392e-01,  6.1862e-01,  9.9217e-01,  9.2595e-01,\n",
       "         3.0547e-01,  3.5201e-01, -6.9992e-01, -9.9727e-01, -1.8159e-01,\n",
       "        -3.4251e-02, -3.0791e-01,  2.8575e-01, -9.8135e-02,  9.3120e-02,\n",
       "        -5.9300e-01, -8.4575e-01, -2.5707e-01,  9.6230e-01, -8.0452e-01,\n",
       "        -9.6235e-02,  4.8377e-01,  9.1357e-01,  9.9914e-01, -9.8441e-01,\n",
       "        -9.5605e-01,  1.8838e-01,  9.3929e-01, -9.6969e-01, -5.3851e-01,\n",
       "         8.8337e-01,  9.4621e-01, -8.1187e-01,  1.1431e-02,  4.9833e-01,\n",
       "         8.9388e-01, -9.8582e-01,  9.9121e-01,  9.9249e-01, -8.4049e-01,\n",
       "        -8.5144e-01,  8.7251e-01,  5.7825e-01, -8.3728e-01,  6.6864e-01,\n",
       "        -7.8196e-01, -6.9341e-01, -7.8310e-01,  8.3403e-01, -9.7517e-01,\n",
       "        -9.3620e-01, -7.5552e-01,  9.4029e-01, -9.8770e-01,  6.5099e-01,\n",
       "         9.9875e-01,  8.2020e-01,  1.6848e-01, -8.7163e-01, -3.1981e-01,\n",
       "         5.0608e-01, -9.9244e-01, -2.7633e-01,  6.5928e-01,  9.5039e-01,\n",
       "        -7.7065e-01,  5.3031e-01, -7.0416e-01, -4.9372e-02, -9.8464e-01,\n",
       "         9.8330e-01, -1.5531e-01, -7.2448e-01, -9.7256e-01, -9.8979e-01,\n",
       "        -4.2799e-01,  9.9838e-01, -1.8036e-01,  5.9217e-01, -9.5786e-01,\n",
       "        -1.6114e-01,  6.9443e-01, -9.4193e-01,  9.9736e-01,  5.4154e-01,\n",
       "         8.5727e-01,  6.1639e-01, -6.5519e-01,  2.3594e-02,  8.2652e-01,\n",
       "         1.3375e-01, -7.3514e-01, -9.7275e-01,  9.5181e-01,  9.6486e-01,\n",
       "        -9.0395e-01,  4.1009e-01, -8.1601e-01,  6.7531e-01,  3.3063e-01,\n",
       "        -2.7781e-01, -4.5803e-01, -5.5782e-01,  8.1857e-01,  6.0917e-01,\n",
       "         8.2967e-01,  2.6377e-01, -8.9716e-01, -2.4779e-01, -7.1926e-01,\n",
       "         8.3589e-01,  9.9987e-01,  9.3583e-01, -9.8152e-01, -9.7784e-01,\n",
       "         7.6343e-01, -3.4305e-01, -3.3014e-01, -6.1311e-01, -6.9198e-01,\n",
       "         9.6492e-01,  9.7025e-01,  9.9642e-01,  7.7274e-01, -2.6607e-01,\n",
       "        -2.2934e-01,  9.5817e-01,  3.2662e-01, -6.8895e-01, -8.5289e-03,\n",
       "         9.5082e-01,  4.3979e-01, -1.7466e-01,  1.2666e-01, -9.9970e-01,\n",
       "        -4.0180e-01,  8.4008e-01,  8.5190e-01, -9.9160e-01,  8.3635e-01,\n",
       "         8.5166e-01,  9.9226e-01,  3.9512e-01, -2.9638e-03, -7.7115e-01,\n",
       "        -4.3175e-01,  5.9074e-01, -9.9708e-01,  9.9950e-01,  6.7694e-02,\n",
       "        -7.9919e-01, -1.1737e-01, -7.4087e-01, -9.7194e-01,  9.6527e-01,\n",
       "         9.8808e-01,  4.0952e-01, -9.8296e-01,  9.7677e-01,  8.9356e-01,\n",
       "         5.9727e-01,  8.4889e-01, -6.2665e-01,  6.0281e-02,  8.4136e-01,\n",
       "        -9.4130e-02, -8.9538e-01,  9.0306e-01,  7.0936e-02,  5.1644e-01,\n",
       "        -3.7962e-01,  9.8398e-01, -9.7232e-01, -3.7398e-01, -2.2906e-01,\n",
       "        -7.0181e-01,  5.0471e-01,  7.9059e-01, -9.8413e-01, -9.8274e-01,\n",
       "        -9.2339e-01, -6.1688e-01, -8.0920e-01, -9.9702e-01,  3.1700e-01,\n",
       "        -5.5118e-01, -8.7908e-01,  7.0413e-01, -9.9935e-01,  9.9789e-01,\n",
       "        -2.9666e-01, -1.1586e-01,  3.0736e-01, -6.4023e-01,  3.1000e-01,\n",
       "        -3.7660e-01,  9.9753e-01, -9.9760e-01, -9.5764e-01,  3.3639e-01,\n",
       "         9.9965e-01, -7.8098e-01, -5.0572e-01,  8.1958e-01, -3.8048e-01,\n",
       "         7.5688e-01,  8.2743e-01,  9.9315e-01, -4.0858e-01, -3.8153e-02,\n",
       "         3.0713e-01,  7.5618e-01,  2.5563e-01,  6.4278e-01,  1.2722e-01,\n",
       "        -6.8788e-01, -1.5593e-01, -8.2904e-01,  9.3991e-01, -6.3139e-01,\n",
       "         9.6189e-01,  2.5615e-01, -6.8327e-01, -9.0655e-01,  2.3023e-01,\n",
       "         9.9975e-01,  5.5461e-01, -8.2493e-01, -9.6353e-01, -5.0979e-01,\n",
       "         8.7761e-01,  9.7534e-01,  9.3223e-01, -9.8503e-01, -5.8759e-01,\n",
       "         5.4003e-01, -6.0562e-01, -5.1727e-01, -5.7151e-01,  8.4119e-01,\n",
       "        -8.9277e-01,  2.9546e-01,  9.7623e-01,  7.3936e-01, -8.6711e-01,\n",
       "         1.1146e-01,  1.2792e-01, -1.8053e-01,  8.5027e-01, -6.8687e-01,\n",
       "         9.4008e-01, -2.8305e-01, -3.0347e-01, -5.9949e-01,  8.6115e-01,\n",
       "        -9.8016e-01, -9.9996e-01,  9.3120e-01,  4.0355e-01, -9.0601e-01,\n",
       "        -9.2749e-01, -6.2155e-01, -3.7233e-01,  6.5686e-01, -9.9544e-01,\n",
       "         2.0860e-01, -3.0765e-01, -8.0883e-01, -7.4814e-02,  5.9674e-02,\n",
       "         3.3001e-01,  9.9999e-01,  5.4116e-01, -9.0172e-01,  8.1489e-01,\n",
       "         1.2222e-01, -1.2087e-01, -4.9971e-02, -1.0000e+00, -6.3206e-01,\n",
       "        -7.3339e-01,  9.9962e-01,  1.8914e-01, -6.7836e-01,  9.1874e-01,\n",
       "        -5.3601e-02, -9.7523e-01,  4.6795e-01,  1.5432e-01,  9.2361e-01,\n",
       "        -5.4039e-01, -9.9124e-01, -1.5983e-01, -9.9897e-01,  1.9097e-01,\n",
       "         4.9797e-01, -7.3873e-01,  1.6266e-01, -3.3960e-01,  7.3227e-02,\n",
       "        -9.4318e-01, -4.7195e-01,  8.8992e-01, -7.3406e-01, -9.0633e-01,\n",
       "        -9.8975e-01, -9.9335e-01,  7.0868e-01, -9.5996e-01,  6.5120e-01,\n",
       "        -7.5360e-01,  2.9571e-01, -1.3179e-01, -3.8281e-01, -7.1440e-01,\n",
       "        -1.1968e-02,  4.2573e-01,  9.5667e-01,  9.0188e-01,  9.9830e-01,\n",
       "        -9.7954e-01, -2.1444e-01, -2.2061e-01, -6.6701e-01, -9.9366e-01,\n",
       "         8.3212e-01, -4.1563e-01,  9.3503e-01,  8.3742e-01, -1.7687e-02,\n",
       "         6.6413e-01, -4.5613e-01,  6.0799e-01,  3.2632e-01, -7.9370e-01,\n",
       "        -6.0084e-01,  9.8803e-01, -8.4704e-01,  9.8345e-01, -5.9822e-02,\n",
       "        -9.0560e-01,  7.6041e-01, -9.9014e-01,  5.0661e-01, -9.9677e-01,\n",
       "        -3.8532e-01, -6.0170e-01, -6.4770e-01, -1.1591e-01, -6.3048e-02,\n",
       "        -9.9763e-01,  9.8055e-01,  9.9845e-01,  7.1088e-01,  4.4544e-01,\n",
       "        -9.4174e-01, -5.5055e-01, -9.9877e-01,  5.0093e-02,  5.1279e-02,\n",
       "        -9.5372e-01,  9.8409e-01, -7.9034e-01, -9.9836e-01,  2.8927e-01,\n",
       "         9.5951e-01, -9.9094e-01,  9.8701e-01, -3.9794e-01,  6.0755e-01,\n",
       "        -9.5878e-02, -8.8491e-01, -7.2253e-01, -9.7453e-01,  8.3910e-01,\n",
       "         9.7706e-01,  3.9984e-01, -7.3613e-01,  4.0775e-01, -9.4714e-01,\n",
       "        -7.9596e-01, -7.6233e-01, -8.4833e-01, -6.5915e-01,  9.6760e-01,\n",
       "         7.0395e-01, -1.7580e-01,  2.5105e-01,  9.2379e-01,  3.1091e-01,\n",
       "        -2.9894e-01, -1.7929e-01,  6.4518e-01,  1.1649e-01,  9.8277e-01,\n",
       "         4.5110e-01,  3.2714e-01,  9.2042e-01, -3.1213e-01, -9.2755e-01,\n",
       "        -7.3272e-01,  9.8989e-01, -8.8284e-01, -7.4314e-02, -9.3157e-01,\n",
       "         9.0710e-01, -5.7782e-01, -9.4725e-01, -8.9496e-01, -2.7983e-01,\n",
       "        -9.6862e-01,  1.5473e-01, -3.3468e-01, -9.4185e-01,  9.0338e-01,\n",
       "        -4.9967e-01,  9.6464e-01, -5.8449e-02, -9.5876e-01, -2.1982e-01,\n",
       "         9.5937e-01,  9.7047e-01,  2.3295e-01, -5.2925e-01, -8.5085e-01,\n",
       "         8.3627e-02,  8.6301e-01,  2.4376e-01,  7.2225e-01, -6.5907e-01,\n",
       "         6.9180e-01,  7.9165e-01,  8.1667e-01,  5.8089e-01, -8.7609e-02,\n",
       "        -2.9151e-01,  5.8471e-01,  9.3463e-01, -8.1526e-01, -7.8149e-01,\n",
       "         9.9374e-01, -6.2957e-01, -4.4858e-01,  9.4962e-01,  9.9943e-01,\n",
       "        -9.8972e-01, -9.4866e-01, -7.7894e-01, -3.5231e-01, -3.6765e-01,\n",
       "         9.9708e-01, -8.1311e-01, -3.8172e-01, -8.2955e-02,  9.0532e-01,\n",
       "         3.5264e-01,  1.4568e-01,  9.7405e-01, -6.4065e-01, -7.5286e-01,\n",
       "         3.0429e-01, -9.0797e-01, -1.9975e-01,  1.3474e-01,  7.2281e-01,\n",
       "         5.1857e-01, -5.0593e-01,  6.3296e-01,  3.3103e-01,  4.0967e-02,\n",
       "        -4.7900e-01, -9.4906e-01,  7.4442e-01,  7.0201e-01,  8.8847e-01,\n",
       "        -9.3339e-01, -9.4873e-01,  4.4102e-01,  9.9503e-01, -9.7685e-01,\n",
       "        -8.1518e-01, -9.9362e-01,  4.7042e-01,  2.9136e-01,  8.2221e-01,\n",
       "        -9.7383e-01,  9.4784e-01, -5.7668e-01,  8.9302e-01, -9.9970e-01,\n",
       "        -9.1800e-01, -3.1644e-01,  4.4313e-02, -9.9715e-01,  9.9144e-01,\n",
       "        -1.3471e-01,  2.6241e-02,  6.5442e-01, -6.7844e-01, -7.2615e-02,\n",
       "         8.3410e-01,  1.5673e-01,  3.4933e-01,  1.2659e-01, -3.1766e-01,\n",
       "         5.4745e-01,  5.4671e-01, -9.5135e-01,  9.1887e-01,  5.2116e-01,\n",
       "         3.0471e-01,  9.1552e-01,  9.8767e-01, -7.6171e-01, -5.2410e-01,\n",
       "         3.4305e-01,  9.8776e-01,  9.6795e-01,  6.2558e-01,  9.8154e-01,\n",
       "         1.3840e-01, -6.4791e-01,  8.2388e-01,  9.6909e-01, -9.0854e-01,\n",
       "        -9.9708e-01, -6.4838e-01,  9.9968e-01, -2.3332e-01, -8.1916e-01,\n",
       "         1.5190e-01,  8.8694e-01,  8.3593e-01, -9.9966e-01, -8.9435e-01,\n",
       "         2.0982e-01, -4.2785e-01, -7.8494e-01, -2.0608e-01,  9.8659e-01,\n",
       "         9.5968e-01,  8.0383e-01, -9.8761e-01, -5.4601e-01, -8.3008e-01,\n",
       "        -6.5205e-01,  1.4174e-01, -8.5397e-01,  8.1502e-01, -5.4493e-01,\n",
       "         7.7832e-01, -2.7516e-01, -4.4798e-01, -9.4869e-01,  3.5115e-01,\n",
       "        -5.8285e-01, -9.6271e-01,  9.3411e-01, -2.2247e-01,  6.6949e-01,\n",
       "        -9.9449e-01, -7.6437e-01, -9.9490e-01,  8.3837e-01, -9.7879e-01,\n",
       "        -5.0237e-01,  9.9412e-03,  8.6477e-01,  2.3618e-01,  2.2116e-01,\n",
       "         4.2834e-02,  7.2906e-01, -5.1101e-01,  3.7909e-01,  7.2029e-01,\n",
       "        -4.8911e-01,  9.7588e-01,  4.2894e-01,  3.8786e-01, -9.9119e-01,\n",
       "         9.8579e-01,  5.2969e-01, -9.8533e-01,  4.0500e-01, -5.1240e-04,\n",
       "        -5.4662e-01, -3.5501e-01,  3.8197e-01, -9.9854e-01, -8.6272e-01,\n",
       "         3.3480e-01, -9.9912e-01, -6.2270e-01,  7.4960e-01,  9.9674e-01,\n",
       "        -9.9108e-01,  8.1548e-01, -6.9825e-01,  2.6834e-01, -9.9933e-01,\n",
       "         4.4588e-01,  9.5968e-01,  9.9758e-02,  9.1396e-01, -2.7361e-01,\n",
       "        -9.5752e-02,  4.4867e-01, -9.3205e-01, -9.5483e-01,  9.8707e-01,\n",
       "         9.4718e-01, -9.3031e-02,  5.5668e-01,  9.7681e-01,  9.7357e-01,\n",
       "        -2.9053e-01,  8.9733e-01,  4.7742e-02, -9.8599e-01, -4.7274e-01,\n",
       "        -9.9686e-01, -6.3348e-01,  9.0963e-01,  9.9981e-01,  8.0380e-01,\n",
       "        -8.9698e-01, -7.4646e-01, -9.7441e-01,  6.8903e-01,  3.4572e-01,\n",
       "        -9.3701e-01, -3.3456e-01,  3.0381e-02,  8.9097e-01,  9.0736e-01,\n",
       "         8.0370e-01,  9.9952e-01, -5.8289e-01, -6.2871e-01, -7.3126e-01,\n",
       "        -9.4349e-01, -3.9936e-01, -7.8171e-02,  2.7543e-01,  6.4067e-01,\n",
       "         5.0676e-01,  2.8377e-01,  4.3548e-01, -8.7284e-01,  2.1754e-01,\n",
       "        -9.9258e-01, -1.6744e-01, -9.9200e-01, -9.8963e-01, -9.0046e-01,\n",
       "         4.7034e-01,  9.9052e-01,  1.6191e-01,  8.6799e-01, -1.1762e-01,\n",
       "        -5.0197e-01,  9.9140e-01,  9.8652e-01, -7.9007e-01,  7.2846e-01,\n",
       "         2.2724e-01,  5.4387e-01, -6.9170e-01, -6.6879e-01, -4.9769e-01,\n",
       "         7.1318e-01, -8.4273e-01, -9.9502e-01, -4.7877e-01, -7.9831e-01,\n",
       "        -5.7092e-01, -6.3650e-01, -9.3932e-01, -5.3930e-01, -3.6843e-01,\n",
       "        -2.9641e-01, -2.5348e-02, -8.3949e-01, -9.7429e-01,  3.4961e-01,\n",
       "         9.6475e-01, -2.0223e-01,  2.3489e-01,  9.9139e-01,  6.5103e-01,\n",
       "         8.8060e-01, -3.3480e-01,  1.3945e-01,  5.5778e-01, -4.2543e-01,\n",
       "        -9.8318e-01,  7.2968e-01, -6.6750e-02,  9.9881e-01,  9.6991e-01,\n",
       "        -2.4540e-01,  8.3753e-01,  9.1403e-01,  4.9038e-01,  4.7803e-01,\n",
       "        -5.9280e-01,  8.7123e-01,  1.1278e-01, -7.8901e-01,  9.9634e-01,\n",
       "        -2.1262e-01, -9.6955e-01,  9.9999e-01,  9.8256e-01, -9.9599e-01,\n",
       "         9.3435e-01,  9.4914e-01, -8.2557e-01,  9.9815e-01,  8.8805e-02,\n",
       "         2.6755e-01,  8.0866e-01,  9.8114e-01, -8.1266e-01,  6.1211e-01,\n",
       "         8.8836e-02,  7.8690e-01,  6.0944e-01,  8.8951e-01,  1.1835e-01,\n",
       "         4.0649e-02,  9.1224e-01, -1.2331e-01, -9.9448e-01,  8.2992e-01,\n",
       "         2.2491e-01,  3.4945e-01, -9.0204e-01, -1.1503e-01, -9.9377e-01,\n",
       "        -9.6050e-01,  7.9667e-01,  6.6296e-01,  8.4676e-01,  4.6615e-01,\n",
       "        -1.2796e-01, -2.7132e-01, -9.5942e-01, -9.9976e-01, -8.6901e-01,\n",
       "        -9.9999e-01, -9.7355e-01,  2.5219e-01, -2.5866e-01, -2.5051e-01,\n",
       "        -5.8670e-01,  8.6374e-01,  8.1378e-01, -3.0267e-01,  1.6613e-01,\n",
       "        -4.0929e-02, -9.9968e-01, -9.6139e-01,  3.7500e-01, -6.4903e-01,\n",
       "         7.1363e-01, -5.4008e-01, -9.6247e-01,  9.9878e-01,  9.4590e-01,\n",
       "         6.1750e-01, -8.5543e-01, -9.9019e-01, -2.8730e-02, -8.8661e-01,\n",
       "         5.5298e-01, -8.5870e-01, -4.2489e-01,  8.9702e-01,  9.8982e-01,\n",
       "         6.5267e-01,  9.7122e-01,  5.2948e-01,  9.5081e-01, -1.3841e-01,\n",
       "        -6.5977e-01,  6.8964e-01,  2.9720e-01, -9.4831e-01, -9.4869e-01,\n",
       "         9.3880e-01,  4.9484e-01, -1.7862e-01, -4.9138e-01, -4.2773e-01,\n",
       "        -8.8748e-01,  6.6672e-01, -3.6082e-01, -5.2396e-01,  9.8211e-01,\n",
       "         2.5916e-01,  4.4173e-01, -5.7188e-01,  9.3404e-01,  4.5264e-01,\n",
       "        -6.4591e-01,  7.2736e-01,  4.7950e-01, -6.1148e-01, -4.9981e-01,\n",
       "        -6.6927e-01, -8.6443e-01,  6.6602e-01,  2.2958e-01,  8.6198e-01,\n",
       "         9.9976e-01, -6.0828e-01,  5.0179e-02, -7.8432e-01,  5.7600e-01,\n",
       "        -5.1571e-01,  9.4979e-01, -9.6790e-01,  7.9846e-01,  9.9789e-01,\n",
       "         2.7883e-01,  5.2471e-01,  9.9341e-01,  9.5841e-01, -4.9311e-02,\n",
       "         7.9287e-01, -7.0945e-02, -4.1851e-01, -5.1136e-01, -8.2166e-01,\n",
       "        -2.9471e-01, -8.9196e-01,  4.1264e-02, -9.7875e-01,  1.6068e-01,\n",
       "        -8.1426e-01,  4.8197e-01,  5.9588e-01,  8.4360e-01,  9.9447e-01,\n",
       "         8.5135e-01,  1.9681e-01,  9.1415e-01,  9.0763e-01, -6.8094e-01,\n",
       "         3.0757e-01,  4.2891e-01, -8.4848e-01,  4.3264e-01, -8.7242e-01,\n",
       "        -3.0102e-01, -2.3460e-02, -6.6471e-01,  5.9270e-01,  9.3767e-01,\n",
       "         8.9466e-01,  7.7197e-01,  6.9126e-01, -9.1954e-01, -3.0996e-01,\n",
       "        -5.3832e-01,  2.0863e-01,  1.6443e-01, -1.1850e-01, -8.3849e-01,\n",
       "        -9.9761e-01,  5.2001e-01, -7.1467e-01, -9.9646e-01, -6.2152e-01,\n",
       "        -6.7547e-01,  1.4071e-01, -4.0223e-01, -8.0190e-01, -9.9870e-01,\n",
       "        -9.6774e-01, -3.0237e-03,  8.7363e-01, -4.9600e-01,  9.2651e-01,\n",
       "         3.7340e-01, -3.2544e-01, -8.6305e-02,  9.8312e-01,  2.3672e-01,\n",
       "        -4.5120e-01,  5.6366e-01,  9.9425e-01,  8.5043e-01,  3.2818e-01,\n",
       "        -2.6467e-01, -2.2899e-01, -7.2965e-01,  3.1105e-01,  3.6360e-01,\n",
       "         3.6027e-01,  8.8693e-01,  3.8860e-01,  8.3190e-01, -7.7950e-01,\n",
       "        -7.5019e-01, -1.8256e-01, -7.2685e-01,  9.0893e-01, -9.9981e-01,\n",
       "         9.2407e-01,  8.6459e-01,  7.1740e-01, -9.9076e-01, -7.2924e-01,\n",
       "         9.3638e-01, -5.7534e-01,  6.8770e-01,  3.1706e-01, -9.9934e-01,\n",
       "        -4.4540e-01,  8.8387e-01,  1.9470e-01, -8.4248e-01,  4.0565e-01,\n",
       "        -6.9889e-01, -9.5532e-01, -3.8212e-01, -6.8088e-01,  9.6876e-01,\n",
       "         4.6352e-01,  2.3411e-01, -7.1552e-01, -9.7785e-02, -5.5956e-01,\n",
       "         9.4925e-01, -2.5322e-01,  7.5501e-01, -9.8314e-01,  9.5729e-01,\n",
       "        -1.3439e-01,  6.9462e-01], device='cuda:7')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size=256, d_model=256):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.wx = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wh = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=True)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        return torch.zeros(batch_size, self.d_model, device=device)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        \"\"\"\n",
    "        x: [B, T] int64 tokens\n",
    "        h0: [B, d_model] (optional)\n",
    "        returns: logits [B, T, V], hT [B, d_model]\n",
    "        \"\"\"\n",
    "        B, T = x.shape\n",
    "        if h0 is None:\n",
    "            h = self.init_hidden(B, x.device)\n",
    "        else:\n",
    "            h = h0\n",
    "\n",
    "        x = self.emb(x)  # [B,T,D]\n",
    "        logits = []\n",
    "        for t in range(T):\n",
    "            h = torch.tanh(self.wx(x[:, t]) + self.wh(h))   # [B,D]\n",
    "            logits.append(self.lm_head(h))                  # [B,V]\n",
    "        logits = torch.stack(logits, dim=1)                 # [B,T,V]\n",
    "        return logits, h\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_rnn(model, sampler, B):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for batch in sampler.testing_batches_iterator(B):   # [B, C]\n",
    "        inp = batch[:, :-1].long()       # [B, C-1]\n",
    "        tgt = batch[:, 1:].long()        # [B, C-1]\n",
    "\n",
    "        logits, _ = model(inp, h0=None)  # [B, C-1, 256]\n",
    "\n",
    "        B_, T, V = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.reshape(B_ * T, V),\n",
    "            tgt.reshape(B_ * T)\n",
    "        )\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # ---- small generation demo ----\n",
    "    prefix = \"Once upon a time \"\n",
    "    gen = generate_text_rnn(model, prefix, length=80, device=device)\n",
    "    print(\"\\n[GEN]:\", gen, \"\\n\")\n",
    "\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text_rnn(model, prefix: str, length: int, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    x = torch.tensor(list(prefix.encode(\"utf-8\")), device=device, dtype=torch.long).unsqueeze(0)  # [1,T]\n",
    "    h = None\n",
    "\n",
    "    # warm-up on prefix (consume all but last, then start generating from last)\n",
    "    if x.shape[1] > 1:\n",
    "        _, h = model(x[:, :-1], h0=None)\n",
    "\n",
    "    cur = x[:, -1:]  # last token\n",
    "    out = bytearray(prefix.encode(\"utf-8\"))\n",
    "\n",
    "    for _ in range(length):\n",
    "        logits, h = model(cur, h0=h)          # logits: [1,1,256]\n",
    "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "        nxt = torch.multinomial(probs, num_samples=1)  # [1,1]\n",
    "        out.append(int(nxt.item()))\n",
    "        cur = nxt\n",
    "\n",
    "    return out.decode(\"utf-8\", errors=\"replace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BetterCharRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Still simple, but much stronger:\n",
    "    - 2-layer GRU\n",
    "    - dropout between layers\n",
    "    - tied input/output embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=256, d_model=384, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=d_model,\n",
    "            hidden_size=d_model,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        # weight tying (common for char LMs)\n",
    "        self.lm_head.weight = self.emb.weight\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        \"\"\"\n",
    "        x: [B,T] int64\n",
    "        h0: [num_layers,B,D] or None\n",
    "        returns: logits [B,T,V], hT\n",
    "        \"\"\"\n",
    "        e = self.emb(x)\n",
    "        y, hT = self.rnn(e, h0)\n",
    "        logits = self.lm_head(y)\n",
    "        return logits, hT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = BetterCharRNN(d_model=512, num_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from spiky.util.torch_utils import make_lr_getter\n",
    "\n",
    "def lr_func(t):\n",
    "    return min(\n",
    "        0.001 / (1 + t)**0.5,\n",
    "        (t / 4000.0) / 4000.0**0.5\n",
    "    )\n",
    "\n",
    "lr = 0.001\n",
    "#optimizer = optim.SGD(lut_transformer.parameters(), lr=lr)\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=lr)\n",
    "\n",
    "steps=150000\n",
    "batch_size = 128\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=steps\n",
    ")\n",
    "#sched = LambdaLR(optimizer, lr_lambda=lr_func)\n",
    "#sched = None\n",
    "#lut_transformer.set_external_learning_rate_hook(make_lr_getter(optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8420f2d8ba0476b8669b2e8dd678631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GEN]: Once upon a time        e         e    e  e      e             e     e        e e        e e      \n",
      "\n",
      "[TEST] step 0: loss=11.6529\n",
      "\n",
      "[GEN]: Once upon a time wood that museum now up to the clear appate, totally a voit or society.\n",
      "For poet \n",
      "\n",
      "[TEST] step 1000: loss=1.6585\n",
      "\n",
      "[GEN]: Once upon a time from the form with extend, 1983 to the not expressed to explain through a keilil \n",
      "\n",
      "[TEST] step 2000: loss=1.5701\n",
      "\n",
      "[GEN]: Once upon a time in the “mP effective that from hordor through requests this landscape in April \n",
      "\n",
      "[TEST] step 3000: loss=1.5410\n",
      "\n",
      "[GEN]: Once upon a time is also documentable to create comptendation. It is no test\n",
      "between 13 Jornse an \n",
      "\n",
      "[TEST] step 4000: loss=1.5246\n",
      "\n",
      "[GEN]: Once upon a time was enough, worldward.\n",
      "Handed the: Meegneti is the form.\n",
      "The town of some depres \n",
      "\n",
      "[TEST] step 5000: loss=1.5107\n",
      "\n",
      "[GEN]: Once upon a time and kid-estimated in the buraft. By only 8,363–19, 29, Kurry sleeps. Landed Ex \n",
      "\n",
      "[TEST] step 6000: loss=1.4989\n",
      "\n",
      "[GEN]: Once upon a time of the gaces, see him from Rooc.. supply that the culture of their first particu \n",
      "\n",
      "[TEST] step 7000: loss=1.4935\n",
      "\n",
      "[GEN]: Once upon a time visible cleanse, are collected, or analytical assistance. Loss due to exclude Na \n",
      "\n",
      "[TEST] step 8000: loss=1.4871\n",
      "\n",
      "[GEN]: Once upon a time about the empire participation in students who are held you at Georgia power is  \n",
      "\n",
      "[TEST] step 9000: loss=1.4825\n",
      "\n",
      "[GEN]: Once upon a time and enforcement was important to attend the list of a number of electortens atth \n",
      "\n",
      "[TEST] step 10000: loss=1.4794\n",
      "\n",
      "[GEN]: Once upon a time of her growth responses, equality, calculators, it similarly egms may be in purp \n",
      "\n",
      "[TEST] step 11000: loss=1.4752\n",
      "\n",
      "[GEN]: Once upon a time that comes nidered and rhetical infhultion is told it in traditionally at the co \n",
      "\n",
      "[TEST] step 12000: loss=1.4730\n",
      "\n",
      "[GEN]: Once upon a time your dile users to get a transpers a\n",
      "odt9r - unsubspecification for perpetual tr \n",
      "\n",
      "[TEST] step 13000: loss=1.4721\n",
      "\n",
      "[GEN]: Once upon a time of a average the Earth is published as a world. Long wage, or time.\n",
      "- Reject sma \n",
      "\n",
      "[TEST] step 14000: loss=1.4725\n",
      "\n",
      "[GEN]: Once upon a time prepare that the prcolog bacteria work behere to use it eating after those share \n",
      "\n",
      "[TEST] step 15000: loss=1.4671\n",
      "\n",
      "[GEN]: Once upon a time in the learning to the new teachs may be with in deliverant enemy, too made work \n",
      "\n",
      "[TEST] step 16000: loss=1.4641\n",
      "\n",
      "[GEN]: Once upon a time of a universe of a support, bibliographic energy education if cities correspondi \n",
      "\n",
      "[TEST] step 17000: loss=1.4625\n",
      "\n",
      "[GEN]: Once upon a time not to redoct the energy or recorded mistray\n",
      "is a certainly actively higher to a \n",
      "\n",
      "[TEST] step 18000: loss=1.4579\n",
      "\n",
      "[GEN]: Once upon a time by studying on Jerusalem’s liver functional leep of beaming changes for the bu \n",
      "\n",
      "[TEST] step 19000: loss=1.4606\n",
      "\n",
      "[GEN]: Once upon a time talk to children who are the 1850s, though there was no notions of the vincil ex \n",
      "\n",
      "[TEST] step 20000: loss=1.4538\n",
      "\n",
      "[GEN]: Once upon a time sectors using a building or decided detailed type species through a main populat \n",
      "\n",
      "[TEST] step 21000: loss=1.4533\n",
      "\n",
      "[GEN]: Once upon a time when all burnings are shared reluctants that have an eating skills, shotted by 1 \n",
      "\n",
      "[TEST] step 22000: loss=1.4549\n",
      "\n",
      "[GEN]: Once upon a time in the fine hand.\n",
      "One of these cold would the Asian Lots working in Tenismufocym \n",
      "\n",
      "[TEST] step 23000: loss=1.4534\n",
      "\n",
      "[GEN]: Once upon a time product in YouTube, Sometimes Eventually, to indicate the medical environment, t \n",
      "\n",
      "[TEST] step 24000: loss=1.4571\n",
      "\n",
      "[GEN]: Once upon a time with many of the world generates than the confidulation to the unneithest real a \n",
      "\n",
      "[TEST] step 25000: loss=1.4537\n",
      "\n",
      "[GEN]: Once upon a time reduce the student need for back on the decade of the picture of the standards,  \n",
      "\n",
      "[TEST] step 26000: loss=1.4517\n",
      "\n",
      "[GEN]: Once upon a time ensures that the sounds in the contents of livehouse and following their birds.  \n",
      "\n",
      "[TEST] step 27000: loss=1.4502\n",
      "\n",
      "[GEN]: Once upon a time to cause a true signal parts of clock, preventions, but the campaign of the part \n",
      "\n",
      "[TEST] step 28000: loss=1.4511\n",
      "\n",
      "[GEN]: Once upon a time for the reminded long way to explore coming and helped people are pennen. For in \n",
      "\n",
      "[TEST] step 29000: loss=1.4506\n",
      "\n",
      "[GEN]: Once upon a time a lot of one than the root.\n",
      "As its design is, not the moKel that is called a \"Nu \n",
      "\n",
      "[TEST] step 30000: loss=1.4486\n",
      "\n",
      "[GEN]: Once upon a time sequenced. More than a storug understood extinction, along with a caused populat \n",
      "\n",
      "[TEST] step 31000: loss=1.4470\n",
      "\n",
      "[GEN]: Once upon a time sharb, carbon, anchor, they were not, more transplantation in order to build a f \n",
      "\n",
      "[TEST] step 32000: loss=1.4472\n",
      "\n",
      "[GEN]: Once upon a time of the children are economic rather than are back in this steep!\"\n",
      "Yet, feedling  \n",
      "\n",
      "[TEST] step 33000: loss=1.4456\n",
      "\n",
      "[GEN]: Once upon a time based, a list of fot of Amerishing which took plant a huge biblical halfs of Vis \n",
      "\n",
      "[TEST] step 34000: loss=1.4428\n",
      "\n",
      "[GEN]: Once upon a time Complex Agreement Concentration of Diabetes with it in the character industries  \n",
      "\n",
      "[TEST] step 35000: loss=1.4467\n",
      "\n",
      "[GEN]: Once upon a time there is her high environment. Interest is only otherwise in the brightoon . I n \n",
      "\n",
      "[TEST] step 36000: loss=1.4408\n",
      "\n",
      "[GEN]: Once upon a time study as finding the way to study.\n",
      "Plants of paper concepts for those strategies \n",
      "\n",
      "[TEST] step 37000: loss=1.4414\n",
      "\n",
      "[GEN]: Once upon a time it approaches.\n",
      "A two molecular step: Unable the, so After Valentia, he has made  \n",
      "\n",
      "[TEST] step 38000: loss=1.4409\n",
      "\n",
      "[GEN]: Once upon a time of one-red good, thirdly very discussioned like cultures to stay lasting over a  \n",
      "\n",
      "[TEST] step 39000: loss=1.4424\n",
      "\n",
      "[GEN]: Once upon a time there is also administered.\n",
      "As extravitably as a result of violate students.\n",
      "The \n",
      "\n",
      "[TEST] step 40000: loss=1.4410\n",
      "\n",
      "[GEN]: Once upon a time without levels by pollen withmay flee fewer tracked blake studies that can be co \n",
      "\n",
      "[TEST] step 41000: loss=1.4408\n",
      "\n",
      "[GEN]: Once upon a time of ago, circulation should be able to cause our eye or gaged, it is very importa \n",
      "\n",
      "[TEST] step 42000: loss=1.4346\n",
      "\n",
      "[GEN]: Once upon a time for his poem, but by Pondin’s Pen Shallow died and died. Kenter Mf Natory a Ce \n",
      "\n",
      "[TEST] step 43000: loss=1.4365\n",
      "\n",
      "[GEN]: Once upon a time to hydrogen person, nonsort with a molding or down other entire days, the hadnes \n",
      "\n",
      "[TEST] step 44000: loss=1.4382\n",
      "\n",
      "[GEN]: Once upon a time for moreover shudmens and bodies for many at another abstract? It is her fair, a \n",
      "\n",
      "[TEST] step 45000: loss=1.4381\n",
      "\n",
      "[GEN]: Once upon a time we do not want to see goals utilities occur evenge in counseling and huntation b \n",
      "\n",
      "[TEST] step 46000: loss=1.4363\n",
      "\n",
      "[GEN]: Once upon a time before the \"Island \"criticism\" will view 9-than-year-old manufacturers\n",
      "- \"They w \n",
      "\n",
      "[TEST] step 47000: loss=1.4386\n",
      "\n",
      "[GEN]: Once upon a time we have an older ways to transport area, employed protection of its focusing pla \n",
      "\n",
      "[TEST] step 48000: loss=1.4372\n",
      "\n",
      "[GEN]: Once upon a time of Capital, however, you take out all and that can be restored into about but sh \n",
      "\n",
      "[TEST] step 49000: loss=1.4337\n",
      "\n",
      "[GEN]: Once upon a time urged or have occurred in the Holy Gallodic Group, when inserved once justice wh \n",
      "\n",
      "[TEST] step 50000: loss=1.4336\n",
      "\n",
      "[GEN]: Once upon a time failure of lesson glossy processes that have equal these potes may have been ini \n",
      "\n",
      "[TEST] step 51000: loss=1.4346\n",
      "\n",
      "[GEN]: Once upon a time foreign, thus as the chemical harmful memories on our families have thinked answ \n",
      "\n",
      "[TEST] step 52000: loss=1.4307\n",
      "\n",
      "[GEN]: Once upon a time which is decided to admit broadlyface for the hand of materials and continues of \n",
      "\n",
      "[TEST] step 53000: loss=1.4297\n",
      "\n",
      "[GEN]: Once upon a time that will apart a drug handle, the breadher, to the work. Machines-influenced pr \n",
      "\n",
      "[TEST] step 54000: loss=1.4310\n",
      "\n",
      "[GEN]: Once upon a time a maximum cannot dance during their story. This is an official both children to  \n",
      "\n",
      "[TEST] step 55000: loss=1.4273\n",
      "\n",
      "[GEN]: Once upon a time help to develop in source students.”\n",
      "Develop a listening welfare on the versio \n",
      "\n",
      "[TEST] step 56000: loss=1.4268\n",
      "\n",
      "[GEN]: Once upon a time Chand,  It also made them that, so, that the window is, number products, excludi \n",
      "\n",
      "[TEST] step 57000: loss=1.4282\n",
      "\n",
      "[GEN]: Once upon a time that examines organisms use reputation and activity,” the Street submineties a \n",
      "\n",
      "[TEST] step 58000: loss=1.4273\n",
      "\n",
      "[GEN]: Once upon a time is contradictive to security changes of impacts of the seed or \"say slow up.\n",
      "- O \n",
      "\n",
      "[TEST] step 59000: loss=1.4271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GEN]: Once upon a time before you all time to call your mast process. Peters unclosed to each it up lik \n",
      "\n",
      "[TEST] step 60000: loss=1.4221\n",
      "\n",
      "[GEN]: Once upon a time to enrolments enable enough sources to cherich his own, and the shell author pro \n",
      "\n",
      "[TEST] step 61000: loss=1.4203\n",
      "\n",
      "[GEN]: Once upon a time and below!\n",
      "Yes a limb is a key role in I language, it was used to haming a proce \n",
      "\n",
      "[TEST] step 62000: loss=1.4225\n",
      "\n",
      "[GEN]: Once upon a time or paste us, for while you're in this is capable of Mexico.\n",
      "It began By: Usule 1 \n",
      "\n",
      "[TEST] step 63000: loss=1.4226\n",
      "\n",
      "[GEN]: Once upon a time particularly in his future ventry – “Competency Berg.”\n",
      "“We have a conclu \n",
      "\n",
      "[TEST] step 64000: loss=1.4179\n",
      "\n",
      "[GEN]: Once upon a time handling economy.\n",
      "Catto perfectly hour engagement in our working of corner but c \n",
      "\n",
      "[TEST] step 65000: loss=1.4159\n",
      "\n",
      "[GEN]: Once upon a time the land, and needed, to determine a left that which has discussed the duration  \n",
      "\n",
      "[TEST] step 66000: loss=1.4187\n",
      "\n",
      "[GEN]: Once upon a time since VCE, antibody can cause painting foramental equipment, including a coming  \n",
      "\n",
      "[TEST] step 67000: loss=1.4181\n",
      "\n",
      "[GEN]: Once upon a time of the AP showed by far indicates that what makes a m?\n",
      "Spring the hittings impor \n",
      "\n",
      "[TEST] step 68000: loss=1.4158\n",
      "\n",
      "[GEN]: Once upon a time me, and I profess—along with heavy understanding. The most important environme \n",
      "\n",
      "[TEST] step 69000: loss=1.4161\n",
      "\n",
      "[GEN]: Once upon a time design from hypertension these are not reached by steners and gives a low voice  \n",
      "\n",
      "[TEST] step 70000: loss=1.4126\n",
      "\n",
      "[GEN]: Once upon a time from National Centre, with MHI the University’s Grace of History. Rather,” S \n",
      "\n",
      "[TEST] step 71000: loss=1.4136\n",
      "\n",
      "[GEN]: Once upon a time a particular amount of recorritoration programme. In such measures and socialist \n",
      "\n",
      "[TEST] step 72000: loss=1.4123\n",
      "\n",
      "[GEN]: Once upon a time — as South African summer, the Marstiu celebrated Americans in Republican Adva \n",
      "\n",
      "[TEST] step 73000: loss=1.4131\n",
      "\n",
      "[GEN]: Once upon a time of metabolism, not people are extraplanted over its system to adhaze that sleep  \n",
      "\n",
      "[TEST] step 74000: loss=1.4098\n",
      "\n",
      "[GEN]: Once upon a time without consumers causing its own web understands and thus living on cooling san \n",
      "\n",
      "[TEST] step 75000: loss=1.4106\n",
      "\n",
      "[GEN]: Once upon a time layer of 17 Jewish lastes public in Pouses, at 24 men.\n",
      "We discovered with childr \n",
      "\n",
      "[TEST] step 76000: loss=1.4082\n",
      "\n",
      "[GEN]: Once upon a time of speech, with a constantinutury device behavior that makes the effects of peop \n",
      "\n",
      "[TEST] step 77000: loss=1.4057\n",
      "\n",
      "[GEN]: Once upon a time proud, I want to predict what it from our look, they’d care by genius accordin \n",
      "\n",
      "[TEST] step 78000: loss=1.4004\n",
      "\n",
      "[GEN]: Once upon a time where it's not down? Generally, with most people's drop off every 350s, a layer  \n",
      "\n",
      "[TEST] step 79000: loss=1.4020\n",
      "\n",
      "[GEN]: Once upon a time for adding 40 million students to have buying state top anxiety and pneumonymic  \n",
      "\n",
      "[TEST] step 80000: loss=1.4016\n",
      "\n",
      "[GEN]: Once upon a time of those heaters can occur in a child’s head, treatment can keep their prenata \n",
      "\n",
      "[TEST] step 81000: loss=1.4016\n",
      "\n",
      "[GEN]: Once upon a time open, It is not important to someone with us for wrapped or exploit sighted bran \n",
      "\n",
      "[TEST] step 82000: loss=1.4016\n",
      "\n",
      "[GEN]: Once upon a time yet will take every career if a spelling poor environment as shown as s, after c \n",
      "\n",
      "[TEST] step 83000: loss=1.3963\n",
      "\n",
      "[GEN]: Once upon a time or companies and researchers agreed that this is solver synthetic model without  \n",
      "\n",
      "[TEST] step 84000: loss=1.3959\n",
      "\n",
      "[GEN]: Once upon a time of winter-fruit art Cano, we could like jos, and most of us have yet caronied ar \n",
      "\n",
      "[TEST] step 85000: loss=1.3974\n",
      "\n",
      "[GEN]: Once upon a time that type 1 is a new time that the degree of genes and they prescript the provid \n",
      "\n",
      "[TEST] step 86000: loss=1.3944\n",
      "\n",
      "[GEN]: Once upon a time of Nevo Science and forces with developing recording authors to take a similar e \n",
      "\n",
      "[TEST] step 87000: loss=1.3940\n",
      "\n",
      "[GEN]: Once upon a time disease.\n",
      "Furnace covering in another deficient player to result in applications\n",
      " \n",
      "\n",
      "[TEST] step 88000: loss=1.3944\n",
      "\n",
      "[GEN]: Once upon a time to find and south. Before he reflects a Bell Drawing Repair starts that is while \n",
      "\n",
      "[TEST] step 89000: loss=1.3921\n",
      "\n",
      "[GEN]: Once upon a time one assert all fail more differences that crimbarp offers the breather and you n \n",
      "\n",
      "[TEST] step 90000: loss=1.3890\n",
      "\n",
      "[GEN]: Once upon a time west of Starfort chincher, the third case of serotic real-tip avocade variator f \n",
      "\n",
      "[TEST] step 91000: loss=1.3895\n",
      "\n",
      "[GEN]: Once upon a time research, education recalls the oppressed by kernel, but there are fewer things  \n",
      "\n",
      "[TEST] step 92000: loss=1.3884\n",
      "\n",
      "[GEN]: Once upon a time they enjoy, unseeker understand what you learn more about the American Employees \n",
      "\n",
      "[TEST] step 93000: loss=1.3881\n",
      "\n",
      "[GEN]: Once upon a time to the poor Austria Boungsse, this past the nose they cannot.\n",
      "It keeps today clu \n",
      "\n",
      "[TEST] step 94000: loss=1.3849\n",
      "\n",
      "[GEN]: Once upon a time through the iole team published by a decrease in its meaning, but I had been a f \n",
      "\n",
      "[TEST] step 95000: loss=1.3855\n",
      "\n",
      "[GEN]: Once upon a time for this activist block upon your teeth healthy with the subcreaming cartographi \n",
      "\n",
      "[TEST] step 96000: loss=1.3840\n",
      "\n",
      "[GEN]: Once upon a time after Arames.\n",
      "Finding your past year before child time.\n",
      "What is any cameras out  \n",
      "\n",
      "[TEST] step 97000: loss=1.3839\n",
      "\n",
      "[GEN]: Once upon a time between nuts, short-fruit and speeches and other sex-interviews received fragmen \n",
      "\n",
      "[TEST] step 98000: loss=1.3834\n",
      "\n",
      "[GEN]: Once upon a time of a star between struth\n",
      "three aspects of sidewashing situation.\n",
      "\n",
      "The Russia Byr \n",
      "\n",
      "[TEST] step 99000: loss=1.3824\n",
      "\n",
      "[GEN]: Once upon a time way, you use each other’s parents came today as doing our common family; provi \n",
      "\n",
      "[TEST] step 100000: loss=1.3796\n",
      "\n",
      "[GEN]: Once upon a time schedule the Tutorial commitment to collaborate them. 293. Www.hang, gap more of \n",
      "\n",
      "[TEST] step 101000: loss=1.3791\n",
      "\n",
      "[GEN]: Once upon a time of 5,200 days of city, while demonstrating swimming groom and evidence of whole  \n",
      "\n",
      "[TEST] step 102000: loss=1.3787\n",
      "\n",
      "[GEN]: Once upon a time a person with disparation (Africa's site) was held by that\n",
      "There are two types o \n",
      "\n",
      "[TEST] step 103000: loss=1.3788\n",
      "\n",
      "[GEN]: Once upon a time of parts of the Poors. In the Son of Asia's oldest heart, full of the so lack of \n",
      "\n",
      "[TEST] step 104000: loss=1.3780\n",
      "\n",
      "[GEN]: Once upon a time of 2 range = 10 square/care hand in a book for fruit, rabbis) time, if you have  \n",
      "\n",
      "[TEST] step 105000: loss=1.3779\n",
      "\n",
      "[GEN]: Once upon a time of the latter templers inflowers and around 178 here before a dust a day in 2014 \n",
      "\n",
      "[TEST] step 106000: loss=1.3754\n",
      "\n",
      "[GEN]: Once upon a time of a phase-date-out law has known every one.\n",
      "An understanding of muxism througho \n",
      "\n",
      "[TEST] step 107000: loss=1.3746\n",
      "\n",
      "[GEN]: Once upon a time to write them approved by promotional regularly and retain patients to determine \n",
      "\n",
      "[TEST] step 108000: loss=1.3737\n",
      "\n",
      "[GEN]: Once upon a time of the low sqS, dead technique and support for determining the body has been hel \n",
      "\n",
      "[TEST] step 109000: loss=1.3723\n",
      "\n",
      "[GEN]: Once upon a time that being educated and equally friendship\n",
      "- Serve was a very first various hear \n",
      "\n",
      "[TEST] step 110000: loss=1.3719\n",
      "\n",
      "[GEN]: Once upon a time floating are two groups on the third of the state stations in his rights, they c \n",
      "\n",
      "[TEST] step 111000: loss=1.3702\n",
      "\n",
      "[GEN]: Once upon a time can transit phyloxized support, concensual the apparity, transition to autism so \n",
      "\n",
      "[TEST] step 112000: loss=1.3696\n",
      "\n",
      "[GEN]: Once upon a time of plants, including data substantially, although soul populations cause blood f \n",
      "\n",
      "[TEST] step 113000: loss=1.3691\n",
      "\n",
      "[GEN]: Once upon a time of Europe.\n",
      "- Indeed a Crown was appointed range brokes on Castalikan to the Infl \n",
      "\n",
      "[TEST] step 114000: loss=1.3681\n",
      "\n",
      "[GEN]: Once upon a time in a day, which helps perfectly replace the teachers are available in older impa \n",
      "\n",
      "[TEST] step 115000: loss=1.3673\n",
      "\n",
      "[GEN]: Once upon a time when, in school, though statements, suggestively. A father behaviorally differ i \n",
      "\n",
      "[TEST] step 116000: loss=1.3662\n",
      "\n",
      "[GEN]: Once upon a time which are efficient but not trying to immediately find them exploiting a randoml \n",
      "\n",
      "[TEST] step 117000: loss=1.3667\n",
      "\n",
      "[GEN]: Once upon a time you have an excusive disorder that includes one of the AAS Advances of Graungrad \n",
      "\n",
      "[TEST] step 118000: loss=1.3658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GEN]: Once upon a time of the womb.\n",
      "When you don’t have the stopping aren't frustrating over the coun \n",
      "\n",
      "[TEST] step 119000: loss=1.3649\n",
      "\n",
      "[GEN]: Once upon a time in the chief goal of place you should did this, we know how to begin to sburgh o \n",
      "\n",
      "[TEST] step 120000: loss=1.3631\n",
      "\n",
      "[GEN]: Once upon a time that a character leads to its article above the smaller heavy effect, is precisi \n",
      "\n",
      "[TEST] step 121000: loss=1.3636\n",
      "\n",
      "[GEN]: Once upon a time when the portions of leading types, so actually it seems to be then an animal of \n",
      "\n",
      "[TEST] step 122000: loss=1.3630\n",
      "\n",
      "[GEN]: Once upon a time and how the development of certain types of sounds say ZFLEM assumes survival an \n",
      "\n",
      "[TEST] step 123000: loss=1.3620\n",
      "\n",
      "[GEN]: Once upon a time in which the turtle day, they do remain with george.\n",
      "\n",
      "Well, the total team says  \n",
      "\n",
      "[TEST] step 124000: loss=1.3626\n",
      "\n",
      "[GEN]: Once upon a time – today's income managing stair life that their “mental and urban mutitic in \n",
      "\n",
      "[TEST] step 125000: loss=1.3617\n",
      "\n",
      "[GEN]: Once upon a time yet a perception of McNebrular has not been tested in a variety of lines, and th \n",
      "\n",
      "[TEST] step 126000: loss=1.3606\n",
      "\n",
      "[GEN]: Once upon a time and a short, visit the neighborhood that teacher or to adopt its mouth, new drug \n",
      "\n",
      "[TEST] step 127000: loss=1.3606\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sched \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     sched\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 24\u001b[0m lv \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m train_loss_ema \u001b[38;5;241m=\u001b[39m lv \u001b[38;5;28;01mif\u001b[39;00m train_loss_ema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha) \u001b[38;5;241m*\u001b[39m train_loss_ema \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m lv\n\u001b[1;32m     27\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_model.to(device)\n",
    "train_loss_ema = None\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "pbar = tqdm(total=steps)\n",
    "rnn_model.train()\n",
    "\n",
    "for step in range(0, steps + 1):\n",
    "    x = snippet_sampler.sample_training_batch(batch_size).to(device)  # [B, C]\n",
    "    inp = x[:, :-1].long()                                            # [B, C-1]\n",
    "    tgt = x[:, 1:].long()                                             # [B, C-1]\n",
    "\n",
    "    logits, _ = rnn_model(inp, h0=None)  # reset hidden each batch (simple)\n",
    "    B, T, V = logits.shape\n",
    "    loss = F.cross_entropy(logits.reshape(B*T, V), tgt.reshape(B*T), reduction=\"mean\")\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    if sched is not None:\n",
    "        sched.step()\n",
    "\n",
    "    lv = loss.item()\n",
    "    train_loss_ema = lv if train_loss_ema is None else (1 - alpha) * train_loss_ema + alpha * lv\n",
    "\n",
    "    pbar.update(1)\n",
    "    if step % 10 == 0:\n",
    "        lr = optimizer.param_groups[0][\"lr\"] if sched is None else sched.get_last_lr()[0]\n",
    "        pbar.set_description(f\"loss={train_loss_ema:.4f}, lr {lr:.8f}\")\n",
    "\n",
    "    if step % test_every == 0:\n",
    "        test_loss = evaluate_rnn(rnn_model, snippet_sampler, test_batch_size)\n",
    "        rnn_model.train()\n",
    "        if len(train_losses) == 0 or step > 0:\n",
    "            train_losses.append(train_loss_ema)\n",
    "            test_losses.append(test_loss)\n",
    "        print(f\"[TEST] step {step}: loss={test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def unigram_baseline_nats(sampler, B):\n",
    "    counts = torch.zeros(256, dtype=torch.float64, device=device)\n",
    "    total = 0\n",
    "    for batch in sampler.testing_batches_iterator(B):\n",
    "        x = batch[:, 1:].reshape(-1).to(torch.long)  # targets\n",
    "        counts += torch.bincount(x, minlength=256).double()\n",
    "        total += x.numel()\n",
    "    p = counts / total\n",
    "    return (-torch.log(p.clamp_min(1e-12)) * p).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.185101273650035"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_baseline_nats(snippet_sampler, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3804113864898682 1.3804113864898682\n"
     ]
    }
   ],
   "source": [
    "logits, _ = rnn_model(inp.long())\n",
    "B_, T, V = logits.shape\n",
    "loss_mean = F.cross_entropy(logits.view(B_*T, V), tgt.view(B_*T), reduction=\"mean\")\n",
    "loss_sum  = F.cross_entropy(logits.view(B_*T, V), tgt.view(B_*T), reduction=\"sum\") / (B_*T)\n",
    "print(loss_mean.item(), loss_sum.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
