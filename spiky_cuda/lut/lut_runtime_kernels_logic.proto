// ======================= HELPER FUNCTIONS =============================== //

find_forward_synapse_group(
    NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info,
    uint32_t output_block_index,
    uint32_t n_outputs_per_block,
    uint8_t* lut_data,
    NeuronDataId_t current_group_id
) {
    if(lookup_neuron_synapses_info.n_synapse_metas == 1) {
        current_group_id = lookup_neuron_synapses_info.first_group_id;
        current_group_id += output_block_index * SizeOfForwardSynapseGroup(n_outputs_per_block, true);
    } else {
        DelayInfo* delays_info = DelayInfos(lookup_neuron_synapses_info.delays_info_id, lut_data);
        uint32_t current_group = 0;
        for(uint32_t j=0;j < lookup_neuron_synapses_info.n_synapse_metas;j++) {
            DelayInfo delay_info = delays_info[j];
            if(delay_info != 0) {
                uint32_t n_groups = DELAY_INFO_N_GROUPS(delay_info);
                if(current_group + n_groups <= output_block_index) {
                    current_group += n_groups;
                    continue;
                }
                current_group_id = lookup_neuron_synapses_info.first_group_id + DELAY_INFO_BYTE_SHIFT_FROM_FIRST_GROUP(delay_info);
                current_group_id += (output_block_index - current_group) * SizeOfForwardSynapseGroup(n_outputs_per_block, true);
                break;
            }
        }
    }
}

calculate_product_lookup_index(
    bool sliced_mode,
    AnchorsPair* detector_anchors,
    uint32_t n_anchors_per_detector,
    uint32_t n_inputs_1,
    uint32_t n_inputs_2,
    uint32_t positional_dim,
    EXTERNAL_REAL_DT* input_1_ptr,
    EXTERNAL_REAL_DT* input_2_ptr,
    EXTERNAL_REAL_DT* pos_emb_ptr,
    int32_t lookup_index
) {
    lookup_index = 0;

    EXTERNAL_REAL_DT* sources[3];
    sources[0] = input_1_ptr;
    sources[1] = input_2_ptr;
    sources[2] = pos_emb_ptr;

    if(sliced_mode) {
        // Sliced mode: [i11, i21, p1, i12, i22, p2, ...]
        for(uint32_t anchor_idx = 0; anchor_idx < n_anchors_per_detector; anchor_idx++) {
            AnchorsPair ap = detector_anchors[anchor_idx];

            if(ap.anchor1_id == std::numeric_limits<NeuronIndex_t>::max() ||
               ap.anchor2_id == std::numeric_limits<NeuronIndex_t>::max()) {
                continue;
            }

            // Map anchor_id to the appropriate input slice
            uint32_t slice_idx_1 = ap.anchor1_id / 3;
            uint32_t component_1 = ap.anchor1_id % 3;
            uint32_t slice_idx_2 = ap.anchor2_id / 3;
            uint32_t component_2 = ap.anchor2_id % 3;

            EXTERNAL_REAL_DT val1 = sources[component_1][slice_idx_1];
            EXTERNAL_REAL_DT val2 = sources[component_2][slice_idx_2];

            lookup_index |= ROUNDED_COMPARISON_GREATER(val1, val2) ? (1 << anchor_idx) : 0;
        }
    } else {
        int shifts[3];
        shifts[0] = 0;
        shifts[1] = n_inputs_1;
        shifts[2] = n_inputs_1 + n_inputs_2;
        // Sequential mode: [inp1, inp2, pos_embedding]
        for(uint32_t anchor_idx = 0; anchor_idx < n_anchors_per_detector; anchor_idx++) {
            AnchorsPair ap = detector_anchors[anchor_idx];

            if(ap.anchor1_id == std::numeric_limits<NeuronIndex_t>::max() ||
               ap.anchor2_id == std::numeric_limits<NeuronIndex_t>::max()) {
                continue;
            }

            uint32_t component_1 = (ap.anchor1_id < shifts[1]) ? 0 : (ap.anchor1_id < shifts[2]) ? 1 : 2;
            uint32_t component_2 = (ap.anchor2_id < shifts[1]) ? 0 : (ap.anchor2_id < shifts[2]) ? 1 : 2;

            EXTERNAL_REAL_DT val1 = sources[component_1][ap.anchor1_id - shifts[component_1]];
            EXTERNAL_REAL_DT val2 = sources[component_2][ap.anchor2_id - shifts[component_2]];

            lookup_index |= ROUNDED_COMPARISON_GREATER(val1, val2) ? (1 << anchor_idx) : 0;
        }
    }
}

calculate_product_lookup_index_backprop(
    bool sliced_mode,
    AnchorsPair* detector_anchors,
    uint32_t n_anchors_per_detector,
    uint32_t n_inputs_1,
    uint32_t n_inputs_2,
    uint32_t positional_dim,
    EXTERNAL_REAL_DT* input_1_ptr,
    EXTERNAL_REAL_DT* input_2_ptr,
    EXTERNAL_REAL_DT* pos_emb_ptr,
    int32_t lookup_index,
    int32_t alt_lookup_index,
    uint32_t first_flipping_index,
    uint32_t second_flipping_index,
    EXTERNAL_REAL_DT min_delta,
    uint32_t first_anchor_source,
    uint32_t second_anchor_source
) {
    lookup_index = 0;
    alt_lookup_index = 0;
    first_flipping_index = std::numeric_limits<uint32_t>::max();
    second_flipping_index = std::numeric_limits<uint32_t>::max();
    min_delta = std::numeric_limits<EXTERNAL_REAL_DT>::max();
    first_anchor_source = std::numeric_limits<uint32_t>::max();
    second_anchor_source = std::numeric_limits<uint32_t>::max();
    uint32_t flipping_anchor_idx = std::numeric_limits<uint32_t>::max();

    EXTERNAL_REAL_DT* sources[3];
    sources[0] = input_1_ptr;
    sources[1] = input_2_ptr;
    sources[2] = pos_emb_ptr;

    if(sliced_mode) {
        // Sliced mode: [i11, i21, p1, i12, i22, p2, ...]
        for(uint32_t anchor_idx = 0; anchor_idx < n_anchors_per_detector; anchor_idx++) {
            AnchorsPair ap = detector_anchors[anchor_idx];

            if(ap.anchor1_id == std::numeric_limits<NeuronIndex_t>::max() ||
               ap.anchor2_id == std::numeric_limits<NeuronIndex_t>::max()) {
                continue;
            }

            // Map anchor_id to the appropriate input slice
            uint32_t slice_idx_1 = ap.anchor1_id / 3;
            uint32_t component_1 = ap.anchor1_id % 3;
            uint32_t slice_idx_2 = ap.anchor2_id / 3;
            uint32_t component_2 = ap.anchor2_id % 3;

            EXTERNAL_REAL_DT val1 = sources[component_1][slice_idx_1];
            EXTERNAL_REAL_DT val2 = sources[component_2][slice_idx_2];

            EXTERNAL_REAL_DT delta = val1 - val2;
            lookup_index |= ROUNDED_COMPARISON_GREATER(delta, 0.0) ? (1 << anchor_idx) : 0;

            bool m = fabs(delta) < fabs(min_delta);
            min_delta = m ? delta : min_delta;
            flipping_anchor_idx = m ? anchor_idx : flipping_anchor_idx;
            first_anchor_source = m ? component_1 : first_anchor_source;
            second_anchor_source = m ? component_2 : second_anchor_source;
            first_flipping_index = m ? slice_idx_1 : first_flipping_index;
            second_flipping_index = m ? slice_idx_2 : second_flipping_index;
        }
    } else {
        int shifts[3];
        shifts[0] = 0;
        shifts[1] = n_inputs_1;
        shifts[2] = n_inputs_1 + n_inputs_2;
        // Sequential mode: [inp1, inp2, pos_embedding]
        for(uint32_t anchor_idx = 0; anchor_idx < n_anchors_per_detector; anchor_idx++) {
            AnchorsPair ap = detector_anchors[anchor_idx];

            if(ap.anchor1_id == std::numeric_limits<NeuronIndex_t>::max() ||
               ap.anchor2_id == std::numeric_limits<NeuronIndex_t>::max()) {
                continue;
            }

            uint32_t component_1 = (ap.anchor1_id < shifts[1]) ? 0 : (ap.anchor1_id < shifts[2]) ? 1 : 2;
            uint32_t component_2 = (ap.anchor2_id < shifts[1]) ? 0 : (ap.anchor2_id < shifts[2]) ? 1 : 2;

            EXTERNAL_REAL_DT val1 = sources[component_1][ap.anchor1_id - shifts[component_1]];
            EXTERNAL_REAL_DT val2 = sources[component_2][ap.anchor2_id - shifts[component_2]];

            EXTERNAL_REAL_DT delta = val1 - val2;
            lookup_index |= ROUNDED_COMPARISON_GREATER(delta, 0.0) ? (1 << anchor_idx) : 0;

            bool m = fabs(delta) < fabs(min_delta);
            min_delta = m ? delta : min_delta;
            flipping_anchor_idx = m ? anchor_idx : flipping_anchor_idx;
            first_anchor_source = m ? component_1 : first_anchor_source;
            second_anchor_source = m ? component_2 : second_anchor_source;
            first_flipping_index = m ? (ap.anchor1_id - shifts[component_1]) : first_flipping_index;
            second_flipping_index = m ? (ap.anchor2_id - shifts[component_2]) : second_flipping_index;
        }
    }
    alt_lookup_index = lookup_index ^ (1 << flipping_anchor_idx);
}

convert_integers_to_floats_logic(
    EXTERNAL_REAL_DT* buffer,
    uint64_t n,
    double int_rescaler
) {
    uint64_t i = static_cast<uint64_t>(blockIdx.x) * blockDim.x + threadIdx.x;
    if(i < n) {
        #ifdef INTEGERS_INSTEAD_OF_FLOATS
        buffer += n * blockIdx.y;
        double y = static_cast<double>(*reinterpret_cast<SUMMATION32_DT *>(buffer + i)) * static_cast<double>(DENOMINATOR32_RECIPROC);
        buffer[i] = static_cast<EXTERNAL_REAL_DT>(y / int_rescaler);
        #endif
    }
}

fire_single_synapse_group(
    NeuronDataId_t group_id,
    uint8_t* lut_data,
    NeuronDataId_t first_synapse_id,
    EXTERNAL_REAL_DT* r_weights,
    EXTERNAL_REAL_DT* w_output,
    uint32_t n_lookup_neurons,
    double int_rescaler
) {
    ForwardSynapseGroup* fws_group_ptr = GetForwardSynapseGroup(group_id, lut_data);
    int32_t current_group_size = static_cast<int32_t>(SynapseGroupSize(fws_group_ptr->meta_info));
    NeuronIndex_t* current_target_ptr = TargetNeuronsInForwardGroup(group_id, lut_data);
    long weight_shift = (reinterpret_cast<uint8_t *>(current_target_ptr) - (lut_data + first_synapse_id)) >> 2;
    EXTERNAL_REAL_DT* weight_ptr = r_weights + weight_shift;
    uint4 targets_quad = make_uint4(0, 0, 0, 0);
    float4 weights_quad = make_float4(0.0, 0.0, 0.0, 0.0);

    NeuronIndex_t target_id;
    double weight;
    SUMMATION32_DT payload;

    int32_t n_t_buffered = 0;
    int32_t n_w_buffered = 0;
    for(int32_t cursor=0;cursor < current_group_size;cursor++, current_target_ptr++, weight_ptr++) {
        if(((reinterpret_cast<uintptr_t>(current_target_ptr) & 15) == 0) && (cursor < current_group_size - 3)) {
            #ifdef ATOMIC
            asm volatile(
                "ld.global.v4.s32 {%0,%1,%2,%3}, [%4];"
                : "=r"(targets_quad.x), "=r"(targets_quad.y), "=r"(targets_quad.z), "=r"(targets_quad.w)
                : "l"(current_target_ptr)
            );
            #else
            targets_quad = *reinterpret_cast<uint4 *>(current_target_ptr);
            #endif
            target_id = targets_quad.x;
            n_t_buffered = 3;
        } else if(n_t_buffered > 0) {
            if(n_t_buffered == 3) {
                target_id = targets_quad.y;
            } else if(n_t_buffered == 2) {
                target_id = targets_quad.z;
            } else {
                target_id = targets_quad.w;
            }
            n_t_buffered--;
        } else {
            target_id = *current_target_ptr;
        }

        if(((reinterpret_cast<uintptr_t>(weight_ptr) & 15) == 0) && (cursor < current_group_size - 3)) {
            #ifdef ATOMIC
            asm volatile(
                "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                : "=f"(weights_quad.x), "=f"(weights_quad.y), "=f"(weights_quad.z), "=f"(weights_quad.w)
                : "l"(weight_ptr)
            );
            #else
            weights_quad = *reinterpret_cast<float4 *>(weight_ptr);
            #endif
            weight = static_cast<double>(weights_quad.x);
            n_w_buffered = 3;
        } else if(n_w_buffered > 0) {
            if(n_w_buffered == 3) {
                weight = static_cast<double>(weights_quad.y);
            } else if(n_w_buffered == 2) {
                weight = static_cast<double>(weights_quad.z);
            } else {
                weight = static_cast<double>(weights_quad.w);
            }
            n_w_buffered--;
        } else {
            weight = static_cast<double>(*weight_ptr);
        }

        target_id -= n_lookup_neurons;

        #ifdef INTEGERS_INSTEAD_OF_FLOATS
            payload = static_cast<SUMMATION32_DT>(weight * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(
                reinterpret_cast<SUMMATION32_DT *>(w_output + target_id),
                payload
            );
            #else
            *reinterpret_cast<SUMMATION32_DT *>(w_output + target_id) += payload;
            #endif
        #else
            payload = static_cast<SUMMATION32_DT>(weight);
            #ifdef ATOMIC
            atomicAdd(
                w_output + target_id,
                payload
            );
            #else
            w_output[target_id] += payload;
            #endif
        #endif
    }
}

accumulate_grad_sum_sparse(
    NeuronDataId_t current_group_id,
    uint8_t* lut_data,
    NeuronDataId_t first_synapse_id,
    EXTERNAL_REAL_DT* r_output_gradients,
    EXTERNAL_REAL_DT* r_weights,
    uint32_t n_lookup_neurons,
    uint32_t n_outputs,
    double grad_sum
) {
    ForwardSynapseGroup fws_group = *GetForwardSynapseGroup(current_group_id, lut_data);
    int32_t n_outputs_in_current_block = SynapseGroupSize(fws_group.meta_info);
    NeuronIndex_t* current_target_ptr = TargetNeuronsInForwardGroup(current_group_id, lut_data);
    long weight_shift = (reinterpret_cast<uint8_t *>(current_target_ptr) - (lut_data + first_synapse_id)) >> 2;
    double weight;
    double output_grad;

    #ifdef ATOMIC
    uint4 targets_quad = make_uint4(0, 0, 0, 0);
    float4 weights_quad = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
    int32_t n_t_buffered = 0;
    int32_t n_w_buffered = 0;
    NeuronIndex_t target_id;
    EXTERNAL_REAL_DT* weight_ptr = r_weights + weight_shift;

    for(int32_t cursor = 0; cursor < n_outputs_in_current_block; cursor++, current_target_ptr++, weight_ptr++) {
        if(((reinterpret_cast<uintptr_t>(current_target_ptr) & 15) == 0) && (cursor < n_outputs_in_current_block - 3)) {
            asm volatile(
                "ld.global.v4.s32 {%0,%1,%2,%3}, [%4];"
                : "=r"(targets_quad.x), "=r"(targets_quad.y), "=r"(targets_quad.z), "=r"(targets_quad.w)
                : "l"(current_target_ptr)
            );
            target_id = targets_quad.x;
            n_t_buffered = 3;
        } else if(n_t_buffered > 0) {
            if(n_t_buffered == 3) {
                target_id = targets_quad.y;
            } else if(n_t_buffered == 2) {
                target_id = targets_quad.z;
            } else {
                target_id = targets_quad.w;
            }
            n_t_buffered--;
        } else {
            target_id = *current_target_ptr;
        }
        target_id -= n_lookup_neurons;

        // Vectorized read of weights
        if(((reinterpret_cast<uintptr_t>(weight_ptr) & 15) == 0) && (cursor < n_outputs_in_current_block - 3)) {
            asm volatile(
                "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                : "=f"(weights_quad.x), "=f"(weights_quad.y), "=f"(weights_quad.z), "=f"(weights_quad.w)
                : "l"(weight_ptr)
            );
            weight = static_cast<double>(weights_quad.x);
            n_w_buffered = 3;
        } else if(n_w_buffered > 0) {
            if(n_w_buffered == 3) {
                weight = static_cast<double>(weights_quad.y);
            } else if(n_w_buffered == 2) {
                weight = static_cast<double>(weights_quad.z);
            } else {
                weight = static_cast<double>(weights_quad.w);
            }
            n_w_buffered--;
        } else {
            weight = static_cast<double>(*weight_ptr);
        }
        output_grad = static_cast<double>(r_output_gradients[target_id]);
        grad_sum += weight * output_grad;
    }
    #else
    for(int32_t cursor = 0; cursor < n_outputs_in_current_block; cursor++, weight_shift++) {
        NeuronIndex_t target_id = current_target_ptr[cursor] - n_lookup_neurons;
        output_grad = static_cast<double>(r_output_gradients[target_id]);
        weight = static_cast<double>(r_weights[weight_shift]);
        grad_sum += weight * output_grad;
    }
    #endif
}

accumulate_grad_sum_fc(
    EXTERNAL_REAL_DT* r_output_gradients,
    EXTERNAL_REAL_DT* r_weights,
    uint32_t neuron_id,
    uint32_t n_outputs,
    int32_t n_outputs_per_block,
    double grad_sum
) {
    uint64_t weights_shift = static_cast<uint64_t>(neuron_id) * n_outputs;

    #ifdef ATOMIC
    float4 output_grad_quad = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
    float4 weights_quad = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
    int32_t n_output_grad_buffered = 0;
    int32_t n_w_buffered = 0;
    EXTERNAL_REAL_DT* r_output_grad_ptr = r_output_gradients;
    EXTERNAL_REAL_DT* r_weights_ptr = r_weights + weights_shift;
    double output_grad;
    double weight;

    for(int32_t cursor = 0; cursor < n_outputs_per_block; cursor++) {
        // Vectorized read of output gradients
        if(((reinterpret_cast<uintptr_t>(r_output_grad_ptr) & 15) == 0) && (cursor < n_outputs_per_block - 3)) {
            asm volatile(
                "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                : "=f"(output_grad_quad.x), "=f"(output_grad_quad.y), "=f"(output_grad_quad.z), "=f"(output_grad_quad.w)
                : "l"(r_output_grad_ptr)
            );
            output_grad = static_cast<double>(output_grad_quad.x);
            n_output_grad_buffered = 3;
        } else if(n_output_grad_buffered > 0) {
            if(n_output_grad_buffered == 3) {
                output_grad = static_cast<double>(output_grad_quad.y);
            } else if(n_output_grad_buffered == 2) {
                output_grad = static_cast<double>(output_grad_quad.z);
            } else {
                output_grad = static_cast<double>(output_grad_quad.w);
            }
            n_output_grad_buffered--;
        } else {
            output_grad = static_cast<double>(*r_output_grad_ptr);
        }

        // Vectorized read of weights
        if(((reinterpret_cast<uintptr_t>(r_weights_ptr) & 15) == 0) && (cursor < n_outputs_per_block - 3)) {
            asm volatile(
                "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                : "=f"(weights_quad.x), "=f"(weights_quad.y), "=f"(weights_quad.z), "=f"(weights_quad.w)
                : "l"(r_weights_ptr)
            );
            weight = static_cast<double>(weights_quad.x);
            n_w_buffered = 3;
        } else if(n_w_buffered > 0) {
            if(n_w_buffered == 3) {
                weight = static_cast<double>(weights_quad.y);
            } else if(n_w_buffered == 2) {
                weight = static_cast<double>(weights_quad.z);
            } else {
                weight = static_cast<double>(weights_quad.w);
            }
            n_w_buffered--;
        } else {
            weight = static_cast<double>(*r_weights_ptr);
        }

        grad_sum += weight * output_grad;

        r_output_grad_ptr++;
        r_weights_ptr++;
    }
    #else
    for(int32_t cursor = 0; cursor < n_outputs_per_block; cursor++, weights_shift++) {
        grad_sum += static_cast<double>(r_weights[weights_shift]) * static_cast<double>(r_output_gradients[cursor]);
    }
    #endif
}

accumulate_pair_of_gradient_sums_fc(
    EXTERNAL_REAL_DT* r_output_gradients,
    EXTERNAL_REAL_DT* r_weights,
    NeuronIndex_t main_neuron_id,
    NeuronIndex_t alt_neuron_id,
    uint32_t n_outputs,
    int32_t n_outputs_in_current_block,
    double main_grad_sum,
    double alt_grad_sum
) {
    uint64_t weights_main_shift = static_cast<uint64_t>(main_neuron_id) * n_outputs;
    uint64_t weights_alt_shift = static_cast<uint64_t>(alt_neuron_id) * n_outputs;

    #ifdef ATOMIC
    float4 output_grad_quad = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
    float4 main_weights_quad = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
    float4 alt_weights_quad = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
    int32_t n_output_grad_buffered = 0;
    int32_t n_main_w_buffered = 0;
    int32_t n_alt_w_buffered = 0;
    EXTERNAL_REAL_DT* r_output_grad_ptr = r_output_gradients;
    EXTERNAL_REAL_DT* r_main_weights_ptr = r_weights + weights_main_shift;
    EXTERNAL_REAL_DT* r_alt_weights_ptr = r_weights + weights_alt_shift;
    double output_grad;
    double weight;

    for(int32_t cursor = 0; cursor < n_outputs_in_current_block; cursor++) {
        // Vectorized read of output gradients
        if(((reinterpret_cast<uintptr_t>(r_output_grad_ptr) & 15) == 0) && (cursor < n_outputs_in_current_block - 3)) {
            asm volatile(
                "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                : "=f"(output_grad_quad.x), "=f"(output_grad_quad.y), "=f"(output_grad_quad.z), "=f"(output_grad_quad.w)
                : "l"(r_output_grad_ptr)
            );
            output_grad = static_cast<double>(output_grad_quad.x);
            n_output_grad_buffered = 3;
        } else if(n_output_grad_buffered > 0) {
            if(n_output_grad_buffered == 3) {
                output_grad = static_cast<double>(output_grad_quad.y);
            } else if(n_output_grad_buffered == 2) {
                output_grad = static_cast<double>(output_grad_quad.z);
            } else {
                output_grad = static_cast<double>(output_grad_quad.w);
            }
            n_output_grad_buffered--;
        } else {
            output_grad = static_cast<double>(*r_output_grad_ptr);
        }

        // Vectorized read of main weights
        if(((reinterpret_cast<uintptr_t>(r_main_weights_ptr) & 15) == 0) && (cursor < n_outputs_in_current_block - 3)) {
            asm volatile(
                "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                : "=f"(main_weights_quad.x), "=f"(main_weights_quad.y), "=f"(main_weights_quad.z), "=f"(main_weights_quad.w)
                : "l"(r_main_weights_ptr)
            );
            weight = static_cast<double>(main_weights_quad.x);
            n_main_w_buffered = 3;
        } else if(n_main_w_buffered > 0) {
            if(n_main_w_buffered == 3) {
                weight = static_cast<double>(main_weights_quad.y);
            } else if(n_main_w_buffered == 2) {
                weight = static_cast<double>(main_weights_quad.z);
            } else {
                weight = static_cast<double>(main_weights_quad.w);
            }
            n_main_w_buffered--;
        } else {
            weight = static_cast<double>(*r_main_weights_ptr);
        }

        main_grad_sum += weight * output_grad;

        // Vectorized read of alt weights
        if(((reinterpret_cast<uintptr_t>(r_alt_weights_ptr) & 15) == 0) && (cursor < n_outputs_in_current_block - 3)) {
            asm volatile(
                "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                : "=f"(alt_weights_quad.x), "=f"(alt_weights_quad.y), "=f"(alt_weights_quad.z), "=f"(alt_weights_quad.w)
                : "l"(r_alt_weights_ptr)
            );
            weight = static_cast<double>(alt_weights_quad.x);
            n_alt_w_buffered = 3;
        } else if(n_alt_w_buffered > 0) {
            if(n_alt_w_buffered == 3) {
                weight = static_cast<double>(alt_weights_quad.y);
            } else if(n_alt_w_buffered == 2) {
                weight = static_cast<double>(alt_weights_quad.z);
            } else {
                weight = static_cast<double>(alt_weights_quad.w);
            }
            n_alt_w_buffered--;
        } else {
            weight = static_cast<double>(*r_alt_weights_ptr);
        }

        alt_grad_sum += weight * output_grad;

        r_output_grad_ptr++;
        r_main_weights_ptr++;
        r_alt_weights_ptr++;
    }
    #else
    for(int32_t cursor = 0; cursor < n_outputs_in_current_block; cursor++, weights_main_shift++, weights_alt_shift++) {
        double output_grad = static_cast<double>(r_output_gradients[cursor]);
        main_grad_sum += static_cast<double>(r_weights[weights_main_shift]) * output_grad;
        alt_grad_sum += static_cast<double>(r_weights[weights_alt_shift]) * output_grad;
    }
    #endif
}

gather_weight_gradients_sparse(
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    uint32_t neuron_id,
    uint32_t output_block_index,
    uint32_t n_outputs_per_block,
    uint8_t* lut_data,
    NeuronDataId_t first_synapse_id,
    EXTERNAL_REAL_DT* r_output_gradients,
    BaseSynapseMeta* r_synapse_metas,
    double first_synapse_meta_lr,
    double external_lr,
    uint32_t n_lookup_neurons,
    EXTERNAL_REAL_DT* w_weights_gradients,
    double int_rescaler
) {
    NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + neuron_id);

    if(output_block_index < lookup_neuron_synapses_info.n_groups) {
        NeuronDataId_t current_group_id;
        PFX(find_forward_synapse_group)(
            lookup_neuron_synapses_info,
            output_block_index,
            n_outputs_per_block,
            lut_data,
            current_group_id
        );

        ForwardSynapseGroup fws_group = *GetForwardSynapseGroup(current_group_id, lut_data);
        uint32_t sm_index = SynapseGroupSynapseMetaIndex(fws_group.meta_info);
        double lr;
        if(sm_index == 0) {
            lr = first_synapse_meta_lr;
        } else {
            lr = static_cast<double>((r_synapse_metas + sm_index)->lr);
        }
        if(external_lr >= 0) {
            lr *= -external_lr;
        }

        uint32_t current_group_size = SynapseGroupSize(fws_group.meta_info);
        NeuronIndex_t* current_target_ptr = TargetNeuronsInForwardGroup(current_group_id, lut_data);
        long weight_shift = (reinterpret_cast<uint8_t *>(current_target_ptr) - (lut_data + first_synapse_id)) >> 2;

        double output_grad;
        SUMMATION32_DT sum32_delta;

        #ifdef ATOMIC
        uint4 targets_quad = make_uint4(0, 0, 0, 0);
        int32_t n_t_buffered = 0;
        NeuronIndex_t target_id;

        for(uint32_t cursor = 0; cursor < current_group_size; cursor++, current_target_ptr++, weight_shift++) {
            // Vectorized read of target IDs
            if(((reinterpret_cast<uintptr_t>(current_target_ptr) & 15) == 0) && (cursor < current_group_size - 3)) {
                asm volatile(
                    "ld.global.v4.s32 {%0,%1,%2,%3}, [%4];"
                    : "=r"(targets_quad.x), "=r"(targets_quad.y), "=r"(targets_quad.z), "=r"(targets_quad.w)
                    : "l"(current_target_ptr)
                );
                target_id = targets_quad.x;
                n_t_buffered = 3;
            } else if(n_t_buffered > 0) {
                if(n_t_buffered == 3) {
                    target_id = targets_quad.y;
                } else if(n_t_buffered == 2) {
                    target_id = targets_quad.z;
                } else {
                    target_id = targets_quad.w;
                }
                n_t_buffered--;
            } else {
                target_id = *current_target_ptr;
            }
            target_id -= n_lookup_neurons;
            output_grad = static_cast<double>(r_output_gradients[target_id]);

            if(fabs(output_grad) > EPS) {
                #ifdef INTEGERS_INSTEAD_OF_FLOATS
                    sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
                    #ifdef ATOMIC
                    atomicAdd(
                        reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients) + weight_shift,
                        sum32_delta
                    );
                    #else
                    *reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients + weight_shift) += sum32_delta;
                    #endif
                #else
                    sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr);
                    #ifdef ATOMIC
                    atomicAdd(
                        w_weights_gradients + weight_shift,
                        sum32_delta
                    );
                    #else
                    w_weights_gradients[weight_shift] += sum32_delta;
                    #endif
                #endif
            }
        }
        #else
        for(uint32_t cursor = 0; cursor < current_group_size; cursor++, weight_shift++) {
            NeuronIndex_t target_id = current_target_ptr[cursor] - n_lookup_neurons;
            output_grad = static_cast<double>(r_output_gradients[target_id]);
            if(fabs(output_grad) > EPS) {
                #ifdef INTEGERS_INSTEAD_OF_FLOATS
                    sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
                    #ifdef ATOMIC
                    atomicAdd(
                        reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients) + weight_shift,
                        sum32_delta
                    );
                    #else
                    *reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients + weight_shift) += sum32_delta;
                    #endif
                #else
                    sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr);
                    #ifdef ATOMIC
                    atomicAdd(
                        w_weights_gradients + weight_shift,
                        sum32_delta
                    );
                    #else
                    w_weights_gradients[weight_shift] += sum32_delta;
                    #endif
                #endif
            }
        }
        #endif
    }
}

gather_weight_gradients_fc(
    EXTERNAL_REAL_DT* r_output_gradients,
    uint32_t neuron_id,
    uint32_t n_outputs,
    int32_t n_outputs_in_current_block,
    double first_synapse_meta_lr,
    double external_lr,
    EXTERNAL_REAL_DT* w_weights_gradients,
    double int_rescaler
) {
    double output_grad;
    SUMMATION32_DT sum32_delta;
    double lr;

    // Fully connected: direct indexing
    lr = first_synapse_meta_lr;
    if(external_lr >= 0) {
        lr *= -external_lr;
    }
    uint64_t weights_shift = static_cast<uint64_t>(neuron_id) * n_outputs;

    #ifdef ATOMIC
    float4 output_grad_quad = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
    int32_t n_output_grad_buffered = 0;
    EXTERNAL_REAL_DT* r_output_grad_ptr = r_output_gradients;

    for(int32_t cursor = 0; cursor < n_outputs_in_current_block; cursor++, weights_shift++) {
        // Vectorized read of output gradients
        if(((reinterpret_cast<uintptr_t>(r_output_grad_ptr) & 15) == 0) && (cursor < n_outputs_in_current_block - 3)) {
            asm volatile(
                "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                : "=f"(output_grad_quad.x), "=f"(output_grad_quad.y), "=f"(output_grad_quad.z), "=f"(output_grad_quad.w)
                : "l"(r_output_grad_ptr)
            );
            output_grad = static_cast<double>(output_grad_quad.x);
            n_output_grad_buffered = 3;
        } else if(n_output_grad_buffered > 0) {
            if(n_output_grad_buffered == 3) {
                output_grad = static_cast<double>(output_grad_quad.y);
            } else if(n_output_grad_buffered == 2) {
                output_grad = static_cast<double>(output_grad_quad.z);
            } else {
                output_grad = static_cast<double>(output_grad_quad.w);
            }
            n_output_grad_buffered--;
        } else {
            output_grad = static_cast<double>(*r_output_grad_ptr);
        }

        if(fabs(output_grad) > EPS) {
            #ifdef INTEGERS_INSTEAD_OF_FLOATS
                sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
                #ifdef ATOMIC
                atomicAdd(
                    reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients) + weights_shift,
                    sum32_delta
                );
                #else
                *reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients + weights_shift) += sum32_delta;
                #endif
            #else
                sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr);
                #ifdef ATOMIC
                atomicAdd(
                    w_weights_gradients + weights_shift,
                    sum32_delta
                );
                #else
                w_weights_gradients[weights_shift] += sum32_delta;
                #endif
            #endif
        }

        r_output_grad_ptr++;
    }
    #else
    for(int32_t cursor = 0; cursor < n_outputs_in_current_block; cursor++, weights_shift++) {
        output_grad = static_cast<double>(r_output_gradients[cursor]);
        if(fabs(output_grad) > EPS) {
            #ifdef INTEGERS_INSTEAD_OF_FLOATS
                sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
                #ifdef ATOMIC
                atomicAdd(
                    reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients) + weights_shift,
                    sum32_delta
                );
                #else
                *reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients + weights_shift) += sum32_delta;
                #endif
            #else
                sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr);
                #ifdef ATOMIC
                atomicAdd(
                    w_weights_gradients + weights_shift,
                    sum32_delta
                );
                #else
                w_weights_gradients[weights_shift] += sum32_delta;
                #endif
            #endif
        }
    }
    #endif
}

propagate_through_detector(
    double du,
    AnchorsPair* r_detectors,
    uint32_t detector_index,
    uint32_t n_anchors_per_detector,
    uint32_t flipping_anchor_index,
    EXTERNAL_REAL_DT* w_input_gradients,
    uint32_t i_or_j,
    uint32_t n_inputs,
    double int_rescaler
) {
    if(fabs(du) > EPS) {
        AnchorsPair ap = r_detectors[detector_index * n_anchors_per_detector + flipping_anchor_index];
        EXTERNAL_REAL_DT* w_input_grad = w_input_gradients + i_or_j * n_inputs;

        #ifdef INTEGERS_INSTEAD_OF_FLOATS
            SUMMATION32_DT sum32_delta = static_cast<SUMMATION32_DT>(du * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(
                reinterpret_cast<SUMMATION32_DT *>(w_input_grad) + ap.anchor1_id,
                sum32_delta
            );
            atomicAdd(
                reinterpret_cast<SUMMATION32_DT *>(w_input_grad) + ap.anchor2_id,
                -sum32_delta
            );
            #else
            *reinterpret_cast<SUMMATION32_DT *>(w_input_grad + ap.anchor1_id) += sum32_delta;
            *reinterpret_cast<SUMMATION32_DT *>(w_input_grad + ap.anchor2_id) -= sum32_delta;
            #endif
        #else
            #ifdef ATOMIC
            atomicAdd(
                w_input_grad + ap.anchor1_id,
                du
            );
            atomicAdd(
                w_input_grad + ap.anchor2_id,
                -du
            );
            #else
            w_input_grad[ap.anchor1_id] += du;
            w_input_grad[ap.anchor2_id] -= du;
            #endif
        #endif
    }
}

propagate_through_detector_product(
    EXTERNAL_REAL_DT min_delta,
    double main_grad_sum,
    double alt_grad_sum,
    uint32_t first_flipping_index,
    uint32_t first_anchor_source,
    uint32_t second_flipping_index,
    uint32_t second_anchor_source,
    EXTERNAL_REAL_DT* w_input_gradients_1, // [batch_size * sequence_length * n_inputs_1]
    EXTERNAL_REAL_DT* w_input_gradients_2, // [batch_size * sequence_length * n_inputs_2]
    uint32_t batch_index,
    uint32_t sequence_length,
    uint32_t i,
    uint32_t j,
    uint32_t n_inputs_1,
    uint32_t n_inputs_2,
    uint32_t positional_dim,
    EXTERNAL_REAL_DT* w_positional_embeddings_gradients, // can be nullptr when positional_dim == 0
    double int_rescaler
) {
    // Compute uncertainty function gradient
    double du = static_cast<double>(min_delta);

    // Compute uncertainty gradient using min_delta
    if(du > 0) {
        du = 1.0 / (1.0 + fabs(du));
        du *= 0.5 * du;
    } else {
        du = 1.0 / (1.0 + fabs(du));
        du *= -0.5 * du;
    }
    du *= main_grad_sum - alt_grad_sum;

    if(fabs(du) > EPS) {
        // Propagate to the appropriate input component based on first_anchor_source
        if(first_anchor_source == 0) {
            // Propagate to input_1 gradients (from position i)
            EXTERNAL_REAL_DT* w_input_1_grad = w_input_gradients_1 + (batch_index * sequence_length + i) * n_inputs_1;
            #ifdef INTEGERS_INSTEAD_OF_FLOATS
            SUMMATION32_DT sum32_delta = static_cast<SUMMATION32_DT>(du * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(reinterpret_cast<SUMMATION32_DT *>(w_input_1_grad) + first_flipping_index, sum32_delta);
            #else
            *reinterpret_cast<SUMMATION32_DT *>(w_input_1_grad + first_flipping_index) += sum32_delta;
            #endif
            #else
            #ifdef ATOMIC
            atomicAdd(w_input_1_grad + first_flipping_index, static_cast<EXTERNAL_REAL_DT>(du));
            #else
            w_input_1_grad[first_flipping_index] += static_cast<EXTERNAL_REAL_DT>(du);
            #endif
            #endif
        } else if(first_anchor_source == 1) {
            // Propagate to input_2 gradients (from position j)
            EXTERNAL_REAL_DT* w_input_2_grad = w_input_gradients_2 + (batch_index * sequence_length + j) * n_inputs_2;
            #ifdef INTEGERS_INSTEAD_OF_FLOATS
            SUMMATION32_DT sum32_delta = static_cast<SUMMATION32_DT>(du * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(reinterpret_cast<SUMMATION32_DT *>(w_input_2_grad) + first_flipping_index, sum32_delta);
            #else
            *reinterpret_cast<SUMMATION32_DT *>(w_input_2_grad + first_flipping_index) += sum32_delta;
            #endif
            #else
            #ifdef ATOMIC
            atomicAdd(w_input_2_grad + first_flipping_index, static_cast<EXTERNAL_REAL_DT>(du));
            #else
            w_input_2_grad[first_flipping_index] += static_cast<EXTERNAL_REAL_DT>(du);
            #endif
            #endif
        } else if(first_anchor_source == 2) {
            // Propagate to positional embedding gradients
            uint32_t pe_offset = (j - i - 1) * positional_dim;
            EXTERNAL_REAL_DT* w_pe_grad = w_positional_embeddings_gradients + pe_offset;
            #ifdef INTEGERS_INSTEAD_OF_FLOATS
            SUMMATION32_DT sum32_delta = static_cast<SUMMATION32_DT>(du * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(reinterpret_cast<SUMMATION32_DT *>(w_pe_grad) + first_flipping_index, sum32_delta);
            #else
            *reinterpret_cast<SUMMATION32_DT *>(w_pe_grad + first_flipping_index) += sum32_delta;
            #endif
            #else
            #ifdef ATOMIC
            atomicAdd(w_pe_grad + first_flipping_index, static_cast<EXTERNAL_REAL_DT>(du));
            #else
            w_pe_grad[first_flipping_index] += static_cast<EXTERNAL_REAL_DT>(du);
            #endif
            #endif
        }

        du = -du;
        if(second_anchor_source == 0) {
            // Propagate to input_1 gradients (from position i)
            EXTERNAL_REAL_DT* w_input_1_grad = w_input_gradients_1 + (batch_index * sequence_length + i) * n_inputs_1;
            #ifdef INTEGERS_INSTEAD_OF_FLOATS
            SUMMATION32_DT sum32_delta = static_cast<SUMMATION32_DT>(du * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(reinterpret_cast<SUMMATION32_DT *>(w_input_1_grad) + second_flipping_index, sum32_delta);
            #else
            *reinterpret_cast<SUMMATION32_DT *>(w_input_1_grad + second_flipping_index) += sum32_delta;
            #endif
            #else
            #ifdef ATOMIC
            atomicAdd(w_input_1_grad + second_flipping_index, static_cast<EXTERNAL_REAL_DT>(du));
            #else
            w_input_1_grad[second_flipping_index] += static_cast<EXTERNAL_REAL_DT>(du);
            #endif
            #endif
        } else if(second_anchor_source == 1) {
            // Propagate to input_2 gradients (from position j)
            EXTERNAL_REAL_DT* w_input_2_grad = w_input_gradients_2 + (batch_index * sequence_length + j) * n_inputs_2;
            #ifdef INTEGERS_INSTEAD_OF_FLOATS
            SUMMATION32_DT sum32_delta = static_cast<SUMMATION32_DT>(du * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(reinterpret_cast<SUMMATION32_DT *>(w_input_2_grad) + second_flipping_index, sum32_delta);
            #else
            *reinterpret_cast<SUMMATION32_DT *>(w_input_2_grad + second_flipping_index) += sum32_delta;
            #endif
            #else
            #ifdef ATOMIC
            atomicAdd(w_input_2_grad + second_flipping_index, static_cast<EXTERNAL_REAL_DT>(du));
            #else
            w_input_2_grad[second_flipping_index] += static_cast<EXTERNAL_REAL_DT>(du);
            #endif
            #endif
        } else if(second_anchor_source == 2) {
            // Propagate to positional embedding gradients
            uint32_t pe_offset = (j - i - 1) * positional_dim;
            EXTERNAL_REAL_DT* w_pe_grad = w_positional_embeddings_gradients + pe_offset;
            #ifdef INTEGERS_INSTEAD_OF_FLOATS
            SUMMATION32_DT sum32_delta = static_cast<SUMMATION32_DT>(du * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(reinterpret_cast<SUMMATION32_DT *>(w_pe_grad) + second_flipping_index, sum32_delta);
            #else
            *reinterpret_cast<SUMMATION32_DT *>(w_pe_grad + second_flipping_index) += sum32_delta;
            #endif
            #else
            #ifdef ATOMIC
            atomicAdd(w_pe_grad + second_flipping_index, static_cast<EXTERNAL_REAL_DT>(du));
            #else
            w_pe_grad[second_flipping_index] += static_cast<EXTERNAL_REAL_DT>(du);
            #endif
            #endif
        }
    }
}

update_positional_gradients(
    double du,
    EXTERNAL_REAL_DT* w_positional_embeddings_gradients,
    uint32_t pe_offset,
    uint32_t positional_dim,
    int32_t pe_min_delta_index,
    double int_rescaler
) {
    w_positional_embeddings_gradients += pe_offset * positional_dim;

    #ifdef INTEGERS_INSTEAD_OF_FLOATS
        SUMMATION32_DT sum32_delta = static_cast<SUMMATION32_DT>(du * static_cast<double>(DENOMINATOR32) * int_rescaler);
        #ifdef ATOMIC
        atomicAdd(
            reinterpret_cast<SUMMATION32_DT *>(w_positional_embeddings_gradients) + pe_min_delta_index,
            sum32_delta
        );
        #else
        *reinterpret_cast<SUMMATION32_DT *>(w_positional_embeddings_gradients + pe_min_delta_index) += sum32_delta;
        #endif
    #else
        #ifdef ATOMIC
        atomicAdd(
            w_positional_embeddings_gradients + pe_min_delta_index,
            du
        );
        #else
        w_positional_embeddings_gradients[pe_min_delta_index] += du;
        #endif
    #endif
}

// ======================= NON SEQUENCE, FORWARD =============================== //

check_detectors_eval_logic(
    EXTERNAL_REAL_DT* r_input,
    uint32_t n_inputs,
    AnchorsPair* r_detectors,
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    int32_t* w_lookup_indices
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;

    if(i < n_detectors) {
        // Offset input for current batch (sequence_length == 1)
        r_input += blockIdx.y * n_inputs;
        w_lookup_indices += blockIdx.y * n_detectors + i;

        // Get detector's anchor pairs
        AnchorsPair* detector_anchors = r_detectors + i * n_anchors_per_detector;

        // Calculate differences between anchor pairs and form bit representation
        int32_t lookup_index = 0;
        AnchorsPair ap;

        for(uint32_t anchor_idx = 0; anchor_idx < n_anchors_per_detector; anchor_idx++) {
            ap = detector_anchors[anchor_idx];

            // Skip if anchor IDs are invalid
            if(ap.anchor1_id == std::numeric_limits<NeuronIndex_t>::max() ||
               ap.anchor2_id == std::numeric_limits<NeuronIndex_t>::max()) {
                continue;
            }

            EXTERNAL_REAL_DT delta = r_input[ap.anchor1_id] - r_input[ap.anchor2_id];

            // Form bit representation: 1 if delta > 0, 0 if delta <= 0
            if(ROUNDED_COMPARISON_GREATER(delta, 0.0)) {
                lookup_index |= (1 << anchor_idx);
            }
        }

        *w_lookup_indices = lookup_index;
    }
}

check_detectors_logic(
    EXTERNAL_REAL_DT* r_input,
    uint32_t n_inputs,
    AnchorsPair* r_detectors,
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    int32_t* w_lookup_indices,
    EXTERNAL_REAL_DT* w_min_anchor_deltas,
    int32_t* w_min_anchor_delta_indices
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;

    if(i < n_detectors) {
        // Offset input for current batch (sequence_length == 1)
        r_input += blockIdx.y * n_inputs;
        w_lookup_indices += blockIdx.y * n_detectors + i;
        w_min_anchor_deltas += blockIdx.y * n_detectors + i;
        w_min_anchor_delta_indices += blockIdx.y * n_detectors + i;

        // Get detector's anchor pairs
        AnchorsPair* detector_anchors = r_detectors + i * n_anchors_per_detector;

        // Calculate differences between anchor pairs and form bit representation
        int32_t lookup_index = 0;
        EXTERNAL_REAL_DT min_delta = 0.0;
        int32_t min_delta_anchor_idx = -1;
        AnchorsPair ap;

        for(uint32_t anchor_idx = 0; anchor_idx < n_anchors_per_detector; anchor_idx++) {
            ap = detector_anchors[anchor_idx];

            // Skip if anchor IDs are invalid
            if(ap.anchor1_id == std::numeric_limits<NeuronIndex_t>::max() ||
               ap.anchor2_id == std::numeric_limits<NeuronIndex_t>::max()) {
                continue;
            }

            EXTERNAL_REAL_DT delta = r_input[ap.anchor1_id] - r_input[ap.anchor2_id];

            // Form bit representation: 1 if delta > 0, 0 if delta <= 0
            if(ROUNDED_COMPARISON_GREATER(delta, 0.0)) {
                lookup_index |= (1 << anchor_idx);
            }

            // Track anchor pair with minimum absolute delta
            if((min_delta_anchor_idx == -1) || (fabs(delta) < fabs(min_delta))) {
                min_delta = delta;
                min_delta_anchor_idx = anchor_idx;
            }
        }

        // Store lookup index and related data
        *w_lookup_indices = lookup_index;
        *w_min_anchor_deltas = min_delta;
        *w_min_anchor_delta_indices = min_delta_anchor_idx;

        __SUPER_DETAILED_TRACE__("check_detectors_logic: detector_index %d, lookup_index %d, min_delta %f, min_delta_anchor_idx %d\n", i, lookup_index, min_delta, min_delta_anchor_idx);
    }
}

fill_outputs_non_seq_sparse_logic(
    EXTERNAL_REAL_DT* r_weights,
    int32_t* r_lookup_indices,
    uint32_t n_detectors,
    uint32_t n_lookup_neurons_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t n_outputs_per_block,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    NeuronDataId_t first_synapse_id,
    uint8_t* lut_data,
    EXTERNAL_REAL_DT* w_output,
    double int_rescaler
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    uint32_t n_items = n_detectors * n_output_blocks;

    if(i < n_items) {
        uint32_t detector_index = i / n_output_blocks;
        uint32_t output_block_index = i % n_output_blocks;

        r_lookup_indices += blockIdx.y * n_detectors;
        w_output += blockIdx.y * n_outputs;

        NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info;
        NeuronIndex_t main_neuron_id = detector_index * n_lookup_neurons_per_detector + r_lookup_indices[detector_index];
        uint32_t n_lookup_neurons = n_lookup_neurons_per_detector * n_detectors;

        // Sparse connectivity: gather output gradients using ForwardSynapseGroups
        lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + main_neuron_id);
        NeuronDataId_t current_group_id;

        if(output_block_index < lookup_neuron_synapses_info.n_groups) {
            PFX(find_forward_synapse_group)(
                lookup_neuron_synapses_info,
                output_block_index,
                n_outputs_per_block,
                lut_data,
                current_group_id
            );

            ATOMIC_PFX(fire_single_synapse_group)(
                current_group_id,
                lut_data,
                first_synapse_id,
                r_weights,
                w_output,
                n_lookup_neurons,
                int_rescaler
            );
        }
    }
}

fill_outputs_non_seq_fc_logic(
    EXTERNAL_REAL_DT* r_weights,
    int32_t* r_lookup_indices,
    EXTERNAL_REAL_DT* w_output,
    uint32_t n_outputs,
    uint32_t n_detectors,
    uint32_t n_detector_blocks,
    uint32_t n_lookup_neurons_per_detector,
    int32_t n_detectors_per_block,
    double int_rescaler
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    if(i < n_detector_blocks * n_outputs) {
        uint32_t detector_block_index = i / n_outputs;
        uint32_t output_index = i % n_outputs;
        __SUPER_DETAILED_TRACE__("fill_outputs_non_seq_fc_logic: batch_index %d, output_index %d, detector_block_index %d\n", blockIdx.y, output_index, detector_block_index);
        r_lookup_indices += blockIdx.y * n_detectors;
        r_lookup_indices += detector_block_index * n_detectors_per_block;
        w_output += blockIdx.y * n_outputs;
        uint64_t weights_per_detector = static_cast<uint64_t>(n_lookup_neurons_per_detector) * n_outputs;
        __SUPER_DETAILED_TRACE__("fill_outputs_non_seq_fc_logic: weights_per_detector %lu\n", weights_per_detector);
        r_weights += weights_per_detector * detector_block_index * n_detectors_per_block;
        int4 li_quad = make_int4(0, 0, 0, 0);
        int32_t current_lookup_index;
        int32_t n_buffered = 0;
        double weight_sum = 0.0;
        if((detector_block_index == n_detector_blocks - 1) && (n_detectors % n_detectors_per_block)) {
            n_detectors_per_block = n_detectors % n_detectors_per_block;
        }
        __SUPER_DETAILED_TRACE__("fill_outputs_non_seq_fc_logic: n_detectors_per_block %d\n", n_detectors_per_block);
        for(int32_t cursor=0;cursor < n_detectors_per_block;cursor++, r_lookup_indices++, r_weights+=weights_per_detector) {
            if(((reinterpret_cast<uintptr_t>(r_lookup_indices) & 15) == 0) && (cursor < n_detectors_per_block - 3)) {
                #ifdef ATOMIC
                asm volatile(
                    "ld.global.v4.s32 {%0,%1,%2,%3}, [%4];"
                    : "=r"(li_quad.x), "=r"(li_quad.y), "=r"(li_quad.z), "=r"(li_quad.w)
                    : "l"(r_lookup_indices)
                );
                #else
                li_quad = *reinterpret_cast<int4 *>(r_lookup_indices);
                #endif
                current_lookup_index = li_quad.x;
                n_buffered = 3;
            } else if(n_buffered > 0) {
                if(n_buffered == 3) {
                    current_lookup_index = li_quad.y;
                } else if(n_buffered == 2) {
                    current_lookup_index = li_quad.z;
                } else {
                    current_lookup_index = li_quad.w;
                }
                n_buffered--;
            } else {
                current_lookup_index = *r_lookup_indices;
            }
            __SUPER_DETAILED_TRACE__("fill_outputs_non_seq_fc_logic: current_lookup_index %d, output_index %d, weight %f\n", current_lookup_index, output_index, r_weights[current_lookup_index * n_outputs + output_index]);
            weight_sum += static_cast<double>(r_weights[current_lookup_index * n_outputs + output_index]);
        }

        __SUPER_DETAILED_TRACE__("fill_outputs_non_seq_fc_logic: weight_sum %f\n", weight_sum);

        SUMMATION32_DT payload;
        #ifdef INTEGERS_INSTEAD_OF_FLOATS
            payload = static_cast<SUMMATION32_DT>(weight_sum * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(
                reinterpret_cast<SUMMATION32_DT *>(w_output + output_index),
                payload
            );
            #else
            *reinterpret_cast<SUMMATION32_DT *>(w_output + output_index) += payload;
            #endif
        #else
            payload = static_cast<SUMMATION32_DT>(weight_sum);
            #ifdef ATOMIC
            atomicAdd(
                w_output + output_index,
                payload
            );
            #else
            __SUPER_DETAILED_TRACE__("fill_outputs_non_seq_fc_logic: payload %f\n", payload);
            w_output[output_index] += payload;
            #endif
        #endif
    }
}

// ======================= NON SEQUENCE, BACKWARD =============================== //

propagate_through_detectors_non_seq_sparse_logic(
    EXTERNAL_REAL_DT* r_weights,
    EXTERNAL_REAL_DT* r_output_gradients,
    int32_t* r_lookup_indices,
    EXTERNAL_REAL_DT* r_min_anchor_deltas,
    int32_t* r_min_anchor_delta_indices,
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    AnchorsPair* r_detectors,
    uint32_t n_lookup_neurons_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t n_outputs_per_block,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    BaseSynapseMeta* r_synapse_metas,
    NeuronDataId_t first_synapse_id,
    uint8_t* lut_data,
    EXTERNAL_REAL_DT* w_input_gradients,
    uint32_t n_inputs,
    double int_rescaler
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    uint32_t n_items = n_detectors * n_output_blocks;

    if(i < n_items) {
        uint32_t detector_index = i / n_output_blocks;
        uint32_t output_block_index = i % n_output_blocks;

        uint64_t shift = blockIdx.y * n_detectors;
        r_lookup_indices += shift;
        r_min_anchor_deltas += shift;
        r_min_anchor_delta_indices += shift;
        w_input_gradients += blockIdx.y * n_inputs;
        r_output_gradients += blockIdx.y * n_outputs;

        double du = static_cast<double>(r_min_anchor_deltas[detector_index]);
        if(du > 0) {
            du = 1.0 / (1.0 + fabs(du));
            du *= 0.5 * du;
        } else {
            du = 1.0 / (1.0 + fabs(du));
            du *= -0.5 * du;
        }

        double main_grad_sum = 0.0;
        double alt_grad_sum = 0.0;
        NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info;

        int32_t lookup_index = r_lookup_indices[detector_index];
        NeuronIndex_t main_neuron_id = detector_index * n_lookup_neurons_per_detector + lookup_index;
        NeuronIndex_t alt_neuron_id = detector_index * n_lookup_neurons_per_detector + (lookup_index ^ (1 << r_min_anchor_delta_indices[detector_index]));
        uint32_t n_lookup_neurons = n_lookup_neurons_per_detector * n_detectors;

        // Sparse connectivity: gather output gradients using ForwardSynapseGroups
        lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + main_neuron_id);
        NeuronDataId_t current_group_id;

        if(output_block_index < lookup_neuron_synapses_info.n_groups) {
            PFX(find_forward_synapse_group)(
                lookup_neuron_synapses_info,
                output_block_index,
                n_outputs_per_block,
                lut_data,
                current_group_id
            );

            ATOMIC_PFX(accumulate_grad_sum_sparse)(
                current_group_id,
                lut_data,
                first_synapse_id,
                r_output_gradients,
                r_weights,
                n_lookup_neurons,
                n_outputs,
                main_grad_sum
            );
        }

        lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + alt_neuron_id);

        // Process alt concatenated_index_alt
        if(output_block_index < lookup_neuron_synapses_info.n_groups) {
            PFX(find_forward_synapse_group)(
                lookup_neuron_synapses_info,
                output_block_index,
                n_outputs_per_block,
                lut_data,
                current_group_id
            );

            ATOMIC_PFX(accumulate_grad_sum_sparse)(
                current_group_id,
                lut_data,
                first_synapse_id,
                r_output_gradients,
                r_weights,
                n_lookup_neurons,
                n_outputs,
                alt_grad_sum
            );
        }
        du *= main_grad_sum - alt_grad_sum;

        uint32_t _s = 0;
        uint32_t flipping_anchor_index = r_min_anchor_delta_indices[detector_index];
        ATOMIC_PFX(propagate_through_detector)(
            du,
            r_detectors,
            detector_index,
            n_anchors_per_detector,
            flipping_anchor_index,
            w_input_gradients,
            _s, _s,
            int_rescaler
        );
    }
}

propagate_through_detectors_non_seq_fc_logic(
    EXTERNAL_REAL_DT* r_weights,
    EXTERNAL_REAL_DT* r_output_gradients,
    int32_t* r_lookup_indices,
    EXTERNAL_REAL_DT* r_min_anchor_deltas,
    int32_t* r_min_anchor_delta_indices,
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    AnchorsPair* r_detectors,
    uint32_t n_lookup_neurons_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    int32_t n_outputs_per_block,
    EXTERNAL_REAL_DT* w_input_gradients,
    uint32_t n_inputs,
    double int_rescaler
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    uint32_t n_items = n_detectors * n_output_blocks;

    if(i < n_items) {
        uint32_t detector_index = i / n_output_blocks;
        uint32_t output_block_index = i % n_output_blocks;

        uint64_t shift = blockIdx.y * n_detectors;
        r_lookup_indices += shift;
        r_min_anchor_deltas += shift;
        r_min_anchor_delta_indices += shift;
        w_input_gradients += blockIdx.y * n_inputs;
        r_output_gradients += blockIdx.y * n_outputs;

        r_weights += output_block_index * n_outputs_per_block;
        r_output_gradients += output_block_index * n_outputs_per_block;
        if((output_block_index == n_output_blocks - 1) && (n_outputs % n_outputs_per_block)) {
            n_outputs_per_block = n_outputs % n_outputs_per_block;
        }

        double du = static_cast<double>(r_min_anchor_deltas[detector_index]);
        if(du > 0) {
            du = 1.0 / (1.0 + fabs(du));
            du *= 0.5 * du;
        } else {
            du = 1.0 / (1.0 + fabs(du));
            du *= -0.5 * du;
        }

        double main_grad_sum = 0.0;
        double alt_grad_sum = 0.0;

        int32_t lookup_index = r_lookup_indices[detector_index];
        NeuronIndex_t main_neuron_id = detector_index * n_lookup_neurons_per_detector + lookup_index;
        NeuronIndex_t alt_neuron_id = detector_index * n_lookup_neurons_per_detector + (lookup_index ^ (1 << r_min_anchor_delta_indices[detector_index]));

        ATOMIC_PFX(accumulate_pair_of_gradient_sums_fc)(
            r_output_gradients,
            r_weights,
            main_neuron_id,
            alt_neuron_id,
            n_outputs,
            n_outputs_per_block,
            main_grad_sum,
            alt_grad_sum
        );

        du *= main_grad_sum - alt_grad_sum;
        uint32_t _s = 0;
        uint32_t flipping_anchor_index = r_min_anchor_delta_indices[detector_index];
        ATOMIC_PFX(propagate_through_detector)(
            du,
            r_detectors,
            detector_index,
            n_anchors_per_detector,
            flipping_anchor_index,
            w_input_gradients,
            _s, _s,
            int_rescaler
        );
    }
}

gather_w_gradients_non_seq_sparse_logic(
    EXTERNAL_REAL_DT* r_output_gradients,
    int32_t* r_lookup_indices,
    EXTERNAL_REAL_DT* w_weights_gradients,
    uint32_t n_outputs,
    uint32_t n_detectors,
    uint32_t n_output_blocks,
    uint32_t n_outputs_per_block,
    uint32_t n_lookup_neurons_per_detector,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    BaseSynapseMeta* r_synapse_metas,
    NeuronDataId_t first_synapse_id,
    uint8_t* lut_data,
    double external_lr,
    double first_synapse_meta_lr,
    double int_rescaler
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    if(i < n_detectors * n_output_blocks) {
        uint32_t detector_index = i / n_output_blocks;
        uint32_t output_block_index = i % n_output_blocks;
        r_lookup_indices += blockIdx.y * n_detectors;
        r_output_gradients += blockIdx.y * n_outputs;

        NeuronIndex_t main_neuron_id = detector_index * n_lookup_neurons_per_detector + r_lookup_indices[detector_index];
        uint32_t n_lookup_neurons = n_lookup_neurons_per_detector * n_detectors;

        // Sparse connectivity: gather weight gradients using ForwardSynapseGroups
        ATOMIC_PFX(gather_weight_gradients_sparse)(
            r_lookup_neuron_synapses_infos,
            main_neuron_id,
            output_block_index,
            n_outputs_per_block,
            lut_data,
            first_synapse_id,
            r_output_gradients,
            r_synapse_metas,
            first_synapse_meta_lr,
            external_lr,
            n_lookup_neurons,
            w_weights_gradients,
            int_rescaler
        );
    }
}

gather_w_gradients_non_seq_fc_logic(
    EXTERNAL_REAL_DT* r_output_gradients,
    int32_t* r_lookup_indices,
    EXTERNAL_REAL_DT* w_weights_gradients,
    uint32_t n_outputs,
    uint32_t n_detectors,
    uint32_t n_output_blocks,
    int32_t n_outputs_per_block,
    uint32_t n_lookup_neurons_per_detector,
    double external_lr,
    double first_synapse_meta_lr,
    double int_rescaler
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    if(i < n_detectors * n_output_blocks) {
        uint32_t detector_index = i / n_output_blocks;
        uint32_t output_block_index = i % n_output_blocks;
        r_lookup_indices += blockIdx.y * n_detectors;
        w_weights_gradients += output_block_index * n_outputs_per_block;
        r_output_gradients += blockIdx.y * n_outputs + output_block_index * n_outputs_per_block;
        if((output_block_index == n_output_blocks - 1) && (n_outputs % n_outputs_per_block)) {
            n_outputs_per_block = n_outputs % n_outputs_per_block;
        }

        NeuronIndex_t main_neuron_id = detector_index * n_lookup_neurons_per_detector + r_lookup_indices[detector_index];

        // Fully connected: direct indexing
        ATOMIC_PFX(gather_weight_gradients_fc)(
            r_output_gradients,
            main_neuron_id,
            n_outputs,
            n_outputs_per_block,
            first_synapse_meta_lr,
            external_lr,
            w_weights_gradients,
            int_rescaler
        );
    }
}

// ======================= SEQUENCE, FORWARD =============================== //

check_detectors_seq_eval_logic(
    EXTERNAL_REAL_DT* r_input,
    uint32_t n_inputs,
    uint32_t sequence_length,
    AnchorsPair* r_detectors,
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    int32_t* w_lookup_indices
) {
    // i represents n_detectors * timestep + detector_index
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    uint32_t n_detector_items = sequence_length * n_detectors;

    if(i < n_detector_items) {
        uint32_t timestep = i / n_detectors;
        uint32_t detector_index = i % n_detectors;

        // Offset input for current batch and timestep
        // Input layout: [batch0_t0, batch0_t1, ..., batch0_tN, batch1_t0, ...]
        r_input += blockIdx.y * n_inputs * sequence_length + timestep * n_inputs;
        w_lookup_indices += blockIdx.y * n_detector_items + i;

        // Get detector's anchor pairs
        AnchorsPair* detector_anchors = r_detectors + detector_index * n_anchors_per_detector;

        // Calculate differences between anchor pairs and form bit representation
        int32_t lookup_index = 0;
        AnchorsPair ap;

        for(uint32_t anchor_idx = 0; anchor_idx < n_anchors_per_detector; anchor_idx++) {
            ap = detector_anchors[anchor_idx];

            // Skip if anchor IDs are invalid
            if(ap.anchor1_id == std::numeric_limits<NeuronIndex_t>::max() ||
               ap.anchor2_id == std::numeric_limits<NeuronIndex_t>::max()) {
                continue;
            }

            EXTERNAL_REAL_DT delta = r_input[ap.anchor1_id] - r_input[ap.anchor2_id];

            // Form bit representation: 1 if delta > 0, 0 if delta <= 0
            if(ROUNDED_COMPARISON_GREATER(delta, 0.0)) {
                lookup_index |= (1 << anchor_idx);
            }
        }

        // Store lookup index and related data
        *w_lookup_indices = lookup_index;
    }
}

check_detectors_seq_logic(
    EXTERNAL_REAL_DT* r_input,
    uint32_t n_inputs,
    uint32_t sequence_length,
    AnchorsPair* r_detectors,
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    int32_t* w_lookup_indices,
    EXTERNAL_REAL_DT* w_min_anchor_deltas,
    int32_t* w_min_anchor_delta_indices
) {
    // i represents n_detectors * timestep + detector_index
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    uint32_t n_detector_items = sequence_length * n_detectors;
    
    if(i < n_detector_items) {
        uint32_t timestep = i / n_detectors;
        uint32_t detector_index = i % n_detectors;

        // Offset input for current batch and timestep
        // Input layout: [batch0_t0, batch0_t1, ..., batch0_tN, batch1_t0, ...]
        r_input += blockIdx.y * n_inputs * sequence_length + timestep * n_inputs;
        w_lookup_indices += blockIdx.y * n_detector_items + i;
        w_min_anchor_deltas += blockIdx.y * n_detector_items + i;
        w_min_anchor_delta_indices += blockIdx.y * n_detector_items + i;
        
        // Get detector's anchor pairs
        AnchorsPair* detector_anchors = r_detectors + detector_index * n_anchors_per_detector;
        
        // Calculate differences between anchor pairs and form bit representation
        int32_t lookup_index = 0;
        EXTERNAL_REAL_DT min_delta = 0.0;
        int32_t min_delta_anchor_idx = -1;
        AnchorsPair ap;
        
        for(uint32_t anchor_idx = 0; anchor_idx < n_anchors_per_detector; anchor_idx++) {
            ap = detector_anchors[anchor_idx];
            
            // Skip if anchor IDs are invalid
            if(ap.anchor1_id == std::numeric_limits<NeuronIndex_t>::max() ||
               ap.anchor2_id == std::numeric_limits<NeuronIndex_t>::max()) {
                continue;
            }
            
            EXTERNAL_REAL_DT delta = r_input[ap.anchor1_id] - r_input[ap.anchor2_id];
            
            // Form bit representation: 1 if delta > 0, 0 if delta <= 0
            if(ROUNDED_COMPARISON_GREATER(delta, 0.0)) {
                lookup_index |= (1 << anchor_idx);
            }
            
            // Track anchor pair with minimum absolute delta
            if((min_delta_anchor_idx == -1) || (fabs(delta) < fabs(min_delta))) {
                min_delta = delta;
                min_delta_anchor_idx = anchor_idx;
            }
        }
        
        // Store lookup index and related data
        *w_lookup_indices = lookup_index;
        *w_min_anchor_deltas = min_delta;
        *w_min_anchor_delta_indices = min_delta_anchor_idx;
        
        __SUPER_DETAILED_TRACE__("check_detectors_seq_logic: batch_index %d, detector_index %d, timestep %d, lookup_index %d, min_delta %f, min_delta_anchor_idx %d\n", blockIdx.y, detector_index, timestep, lookup_index, min_delta, min_delta_anchor_idx);
    }
}

check_positional_embeddings_eval_logic(
    uint32_t sequence_length,
    EXTERNAL_REAL_DT* r_positional_embeddings,
    uint32_t n_detectors,
    uint32_t positional_dim,
    int32_t* w_positional_lookup_indices
) {
    // i represents n_detectors * timestep + detector_index
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    uint32_t n_detector_items = (sequence_length - 1) * n_detectors;

    if(i < n_detector_items) {
        uint32_t timestep = i / n_detectors;
        uint32_t detector_index = i % n_detectors;

        // Calculate offset for this batch, detector, and timestep
        // positional embeddings are not batched
        w_positional_lookup_indices += i;

        // Get positional embedding for this detector and timestep
        // Positional embeddings are stored as [(sequence_length - 1) * n_detectors * positional_dim]
        r_positional_embeddings += timestep * n_detectors * positional_dim;
        r_positional_embeddings += detector_index * positional_dim;

        int32_t lookup_index = 0;
        EXTERNAL_REAL_DT embedding_value;

        for(uint32_t dim = 0; dim < positional_dim; dim++) {
            embedding_value = r_positional_embeddings[dim];

            if(ROUNDED_COMPARISON_GREATER(embedding_value, 0.0)) {
                lookup_index |= (1 << dim);
            }
        }

        *w_positional_lookup_indices = lookup_index;
    }
}

check_positional_embeddings_logic(
    uint32_t sequence_length,
    EXTERNAL_REAL_DT* r_positional_embeddings,
    uint32_t n_detectors,
    uint32_t positional_dim,
    int32_t* w_positional_lookup_indices,
    EXTERNAL_REAL_DT* w_positional_min_deltas,
    int32_t* w_positional_min_delta_indices
) {
    // i represents n_detectors * timestep + detector_index
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    uint32_t n_detector_items = (sequence_length - 1) * n_detectors;
    
    if(i < n_detector_items) {
        uint32_t timestep = i / n_detectors;
        uint32_t detector_index = i % n_detectors;
        
        // Calculate offset for this batch, detector, and timestep
        // positional embeddings are not batched
        w_positional_lookup_indices += i;
        w_positional_min_deltas += i;
        w_positional_min_delta_indices += i;
        
        // Get positional embedding for this detector and timestep
        // Positional embeddings are stored as [(sequence_length - 1) * n_detectors * positional_dim]
        r_positional_embeddings += timestep * n_detectors * positional_dim;
        r_positional_embeddings += detector_index * positional_dim;
        
        int32_t lookup_index = 0;
        int32_t min_dim_index = -1;
        EXTERNAL_REAL_DT min_delta;
        EXTERNAL_REAL_DT embedding_value;
        
        for(uint32_t dim = 0; dim < positional_dim; dim++) {
            embedding_value = r_positional_embeddings[dim];
            
            if(ROUNDED_COMPARISON_GREATER(embedding_value, 0.0)) {
                lookup_index |= (1 << dim);
            }
            
            if((min_dim_index == -1) || (fabs(embedding_value) < fabs(min_delta))) {
                min_delta = embedding_value;
                min_dim_index = dim;
            }
        }
        
        *w_positional_lookup_indices = lookup_index;
        *w_positional_min_deltas = min_delta;
        *w_positional_min_delta_indices = min_dim_index;
        
        __SUPER_DETAILED_TRACE__(
            "check_positional_embeddings_logic: batch_index %d, detector_index %d, timestep %d, lookup_index %d, min_dim_index %d, min_delta %f\n",
            blockIdx.y, detector_index, timestep, lookup_index, min_dim_index, min_delta
        );
    }
}

fill_outputs_fully_connected_seq_logic(
    EXTERNAL_REAL_DT* r_weights,
    int32_t* r_lookup_indices,
    int32_t* r_positional_lookup_indices,
    EXTERNAL_REAL_DT* w_output,
    uint32_t n_outputs,
    uint32_t n_detectors,
    uint32_t sequence_length,
    uint32_t n_anchors_per_detector,
    uint32_t positional_dim,
    uint32_t n_lookup_neurons_per_detector,
    double int_rescaler
) {
    uint32_t linear_idx = blockIdx.x * blockDim.x + threadIdx.x;

    if(linear_idx >= (sequence_length - 1) * n_detectors * n_outputs) {
        return;
    }

    uint32_t j = (linear_idx / (n_detectors * n_outputs)) + 1;
    linear_idx = linear_idx % (n_detectors * n_outputs);
    uint32_t detector_index = linear_idx / n_outputs;
    uint32_t output_index = linear_idx % n_outputs;

    uint32_t batch_index = blockIdx.y;

    // Calculate offsets into lookup_indices arrays
    // Layout: [batch0_t0_det0, batch0_t0_det1, ..., batch0_t0_detN, batch0_t1_det0, ...]
    uint32_t n_detector_items = sequence_length * n_detectors;
    uint32_t batch_offset = batch_index * n_detector_items;

    // Get Q: lookup index from first timestep (j)
    uint32_t q_offset = batch_offset + j * n_detectors + detector_index;
    int32_t Q = r_lookup_indices[q_offset];

    // Get PE lookup indices (shared across batches)
    // PE indices are stored as [(S-1)  N_t]

    // Accumulate weight sum over all i from 0 to j (inclusive)
    double weight_sum = 0.0;

    for(uint32_t i = 0; i < j; i++) {
        // Get K: lookup index from second timestep (i)
        uint32_t k_offset = batch_offset + i * n_detectors + detector_index;
        int32_t K = r_lookup_indices[k_offset];

        // Get PE: positional lookup index from timestep difference (j - i - 1)
        // Note: j - i - 1 can be negative when i == j, but in that case we don't use PE
        int32_t PE = 0;
        if(positional_dim > 0) {
            uint32_t pe_offset = (j - i - 1) * n_detectors + detector_index;
            PE = r_positional_lookup_indices[pe_offset];
        }

        // Calculate concatenated_index
        uint32_t concatenated_index = (
            (static_cast<uint32_t>(Q) << (n_anchors_per_detector + positional_dim)) |
            (static_cast<uint32_t>(K) << positional_dim) |
            static_cast<uint32_t>(PE)
        );

        // Add detector offset to get neuron_id
        uint32_t neuron_id = concatenated_index + detector_index * n_lookup_neurons_per_detector;

        // Get weight connecting this neuron to current output
        // Weight layout: weights[neuron_id * n_outputs + output_index]
        double weight = static_cast<double>(r_weights[neuron_id * n_outputs + output_index]);
        weight_sum += weight;
    }

    // Calculate output offset
    // Output layout: [batch0_t0_out0, batch0_t0_out1, ..., batch0_t0_outN, batch0_t1_out0, ...]
    uint32_t output_offset = (batch_index * sequence_length + j) * n_outputs + output_index;

    // Atomic add accumulated sum to output
    SUMMATION32_DT payload;
    #ifdef INTEGERS_INSTEAD_OF_FLOATS
        payload = static_cast<SUMMATION32_DT>(weight_sum * static_cast<double>(DENOMINATOR32) * int_rescaler);
        #ifdef ATOMIC
        atomicAdd(
            reinterpret_cast<SUMMATION32_DT *>(w_output + output_offset),
            payload
        );
        #else
        *reinterpret_cast<SUMMATION32_DT *>(w_output + output_offset) += payload;
        #endif
    #else
        payload = static_cast<SUMMATION32_DT>(weight_sum);
        #ifdef ATOMIC
        atomicAdd(
            w_output + output_offset,
            payload
        );
        #else
        w_output[output_offset] += payload;
        #endif
    #endif
}

fill_outputs_sparse_seq_logic(
    EXTERNAL_REAL_DT* r_weights,
    int32_t* r_lookup_indices,
    int32_t* r_positional_lookup_indices,
    uint32_t n_detectors,
    uint32_t n_total_tiles,
    uint32_t sequence_length,
    uint32_t n_anchors_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t n_outputs_per_block,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    NeuronDataId_t first_synapse_id,
    uint8_t* lut_data,
    uint32_t n_lookup_neurons_per_detector,
    EXTERNAL_REAL_DT* w_output,
    uint32_t positional_dim,
    double int_rescaler
) {
    // blockIdx.x encodes: tile_id * n_detectors + detector_index
    // Extract tile_id and detector_index
    uint32_t tile_id = blockIdx.x / n_detectors;
    uint32_t detector_index = blockIdx.x % n_detectors;
    uint32_t tile_grid_w = (sequence_length + TILE - 1) / TILE;

    if(tile_id >= n_total_tiles) {
        return;
    }

    uint32_t i = tile_id / tile_grid_w;  // tile row
    uint32_t j = tile_id % tile_grid_w;  // tile col

    // Skip tiles below diagonal (tile i > tile j means all pairs have i > j, which we don't want)
    if(i > j) {
        return;
    }

    if(i == j) {
        i = i * TILE + (threadIdx.x / TILE);
        j = j * TILE + (threadIdx.x % TILE);
        if(i >= j) {
            return;
        }
    } else {
        i = i * TILE + (threadIdx.x / TILE);
        j = j * TILE + (threadIdx.x % TILE);
    }

    if(i >= sequence_length || j >= sequence_length) {
        return;
    }

    // blockIdx.y encodes: batch_index * n_output_blocks + output_block_index
    uint32_t batch_index = blockIdx.y / n_output_blocks;
    uint32_t output_block_index = blockIdx.y % n_output_blocks;
    w_output += (batch_index * sequence_length + j) * n_outputs;

    // Calculate offsets into lookup_indices arrays
    // Layout: [batch0_t0_det0, batch0_t0_det1, ..., batch0_t0_detN, batch0_t1_det0, ...]
    uint32_t n_detector_items = sequence_length * n_detectors;
    uint32_t batch_offset = batch_index * n_detector_items;

    // Get K: lookup index from first timestep (i)
    uint32_t k_offset = batch_offset + i * n_detectors + detector_index;
    int32_t K = r_lookup_indices[k_offset];

    // Get Q: lookup index from second timestep (j)
    uint32_t q_offset = batch_offset + j * n_detectors + detector_index;
    int32_t Q = r_lookup_indices[q_offset];

    // Get PE: positional lookup index from timestep difference (j - i - 1)
    int32_t PE = 0;
    if(positional_dim > 0) {
        uint32_t pe_offset = (j - i - 1) * n_detectors + detector_index;
        PE = r_positional_lookup_indices[pe_offset];
    }

    // Calculate concatenated index
    uint32_t concatenated_index = (
        (Q << (n_anchors_per_detector + positional_dim)) |
        (K << positional_dim) |
        PE
    ) + detector_index * n_lookup_neurons_per_detector;

    uint32_t n_lookup_neurons = n_lookup_neurons_per_detector * n_detectors;

    NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + concatenated_index);

    if(output_block_index < lookup_neuron_synapses_info.n_groups) {
        NeuronDataId_t current_group_id;
        PFX(find_forward_synapse_group)(
            lookup_neuron_synapses_info,
            output_block_index,
            n_outputs_per_block,
            lut_data,
            current_group_id
        );
        ATOMIC_PFX(fire_single_synapse_group)(
            current_group_id,
            lut_data,
            first_synapse_id,
            r_weights,
            w_output,
            n_lookup_neurons,
            int_rescaler
        );
    }
}

// ======================= PRODUCT, FORWARD =============================== //

fill_outputs_product_fc_logic(
    uint32_t sequence_length,
    uint32_t positional_dim,
    EXTERNAL_REAL_DT* r_input_1, // [batch_size * sequence_length * n_inputs_1]
    uint32_t n_inputs_1,
    EXTERNAL_REAL_DT* r_input_2, // [batch_size * sequence_length * n_inputs_2]
    uint32_t n_inputs_2,
    EXTERNAL_REAL_DT* pos_embedding, // [(sequence_length -1 ) * positional_dim]
    AnchorsPair* r_detectors, // [n_detectors * n_anchors_per_detector]
    uint32_t n_detectors,
    uint32_t n_total_tiles,
    uint32_t n_anchors_per_detector,
    EXTERNAL_REAL_DT* r_weights, // [n_detectors * n_lookup_neurons_per_detector * n_outputs]
    uint32_t n_lookup_neurons_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t n_outputs_in_block,
    EXTERNAL_REAL_DT* w_output, // [batch_size * sequence_length * n_outputs]
    bool sliced_mode,
    double int_rescaler
) {
    #ifdef ATOMIC
    // blockIdx.x encodes: tile_id * n_detectors + detector_index
    // Extract tile_id and detector_index
    uint32_t tile_id = blockIdx.x / n_detectors;
    uint32_t detector_index = blockIdx.x % n_detectors;
    uint32_t tile_grid_w = (sequence_length + TILE - 1) / TILE;

    if(tile_id >= n_total_tiles) {
        return;
    }

    uint32_t i = tile_id / tile_grid_w;  // tile row
    uint32_t j = tile_id % tile_grid_w;  // tile col

    // Skip tiles below diagonal (bi > bj means all pairs have i > j, which we don't want)
    if(i > j) {
        return;
    }

    bool valid_pair = true;
    if(i == j) {
        i = i * TILE + (threadIdx.x % TILE);
        j = j * TILE + (threadIdx.x / TILE);
        if(i >= j) {
            valid_pair = false;
        }
    } else {
        i = i * TILE + (threadIdx.x % TILE);
        j = j * TILE + (threadIdx.x / TILE);
    }

    if(i >= sequence_length || j >= sequence_length) {
        valid_pair = false;
    }

    // blockIdx.y encodes: batch_index * n_output_blocks + output_block_index
    uint32_t batch_index = blockIdx.y / n_output_blocks;
    uint32_t output_block_index = blockIdx.y % n_output_blocks;

    // Shared memory layout:
    // - shared_lookup_indices: blockDim.x (lookup indices, one per thread)
    extern __shared__ EXTERNAL_REAL_DT shared_mem[];
    int32_t* shared_lookup_indices = reinterpret_cast<int32_t*>(shared_mem);

    int32_t lookup_index = -1;
    if(valid_pair) {
        // Get detector anchors
        AnchorsPair* detector_anchors = r_detectors + detector_index * n_anchors_per_detector;

        // Access inputs directly from global memory
        uint32_t batch_offset = batch_index * sequence_length;
        EXTERNAL_REAL_DT* input_1_ptr = r_input_1 + (batch_offset + i) * n_inputs_1;
        EXTERNAL_REAL_DT* input_2_ptr = r_input_2 + (batch_offset + j) * n_inputs_2;

        // Access positional embeddings directly from global memory (if needed)
        EXTERNAL_REAL_DT* pos_emb_ptr = nullptr;
        if(positional_dim > 0) {
            pos_emb_ptr = pos_embedding + (j - i - 1) * positional_dim;
        }

        PFX(calculate_product_lookup_index)(
            sliced_mode,
            detector_anchors,
            n_anchors_per_detector,
            n_inputs_1,
            n_inputs_2,
            positional_dim,
            input_1_ptr,
            input_2_ptr,
            pos_emb_ptr,
            lookup_index
        );
    }
    shared_lookup_indices[threadIdx.x] = lookup_index;
    __syncthreads();

    j = threadIdx.x / n_outputs_in_block;
    if(j >= TILE) {
        return;
    }

    uint32_t output_idx = threadIdx.x % n_outputs_in_block;
    output_idx += output_block_index * n_outputs_in_block;

    if(output_idx >= n_outputs) {
        return;
    }

    double sum_output = 0.0;
    for(uint32_t k = 0; k < TILE; k++) {
        int32_t li = shared_lookup_indices[j * TILE + k];
        if(li < 0) {
            continue;
        }
        uint32_t lookup_neuron_id = detector_index * n_lookup_neurons_per_detector + li;
        uint64_t weights_offset = static_cast<uint64_t>(lookup_neuron_id) * n_outputs + output_idx;
        sum_output += static_cast<double>(r_weights[weights_offset]);
    }

    j += (tile_id % tile_grid_w) * TILE;
    w_output += batch_index * sequence_length * n_outputs + j * n_outputs + output_idx;

    #ifdef INTEGERS_INSTEAD_OF_FLOATS
    SUMMATION32_DT payload = static_cast<SUMMATION32_DT>(sum_output * static_cast<double>(DENOMINATOR32) * int_rescaler);
    atomicAdd(reinterpret_cast<SUMMATION32_DT *>(w_output), payload);
    #else
    atomicAdd(w_output, static_cast<EXTERNAL_REAL_DT>(sum_output));
    #endif
    #endif
}

fill_outputs_product_cpu_logic(
    uint32_t sequence_length,
    uint32_t positional_dim,
    EXTERNAL_REAL_DT* r_input_1, // [batch_size * sequence_length * n_inputs_1]
    uint32_t n_inputs_1,
    EXTERNAL_REAL_DT* r_input_2, // [batch_size * sequence_length * n_inputs_2]
    uint32_t n_inputs_2,
    EXTERNAL_REAL_DT* pos_embedding, // [(sequence_length -1 ) * positional_dim] or nullptr
    AnchorsPair* r_detectors, // [n_detectors * n_anchors_per_detector]
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    EXTERNAL_REAL_DT* r_weights, // [n_detectors * n_lookup_neurons_per_detector * n_outputs]
    uint32_t n_lookup_neurons_per_detector,
    uint32_t n_outputs,
    uint32_t batch_size,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos, // nullptr for FC mode
    NeuronDataId_t first_synapse_id, // only used for sparse mode
    uint8_t* lut_data, // only used for sparse mode
    EXTERNAL_REAL_DT* w_output, // [batch_size * sequence_length * n_outputs]
    bool sliced_mode,
    double int_rescaler
) {
    // blockIdx.y encodes: batch_index
    // blockIdx.x encodes: j * n_detectors + detector_index
    // threadIdx.x encodes: i (for position i < j)
    
    uint32_t batch_index = blockIdx.y;
    uint32_t j_detector_combo = blockIdx.x;
    uint32_t j = j_detector_combo / n_detectors;
    uint32_t detector_index = j_detector_combo % n_detectors;
    uint32_t i = threadIdx.x;
    
    // Future masking: only process pairs where i < j
    if(i >= j) {
        return;
    }
    
    // Get detector anchors
    AnchorsPair* detector_anchors = r_detectors + detector_index * n_anchors_per_detector;
    
    // Calculate batch offsets
    uint32_t batch_offset = batch_index * sequence_length;
    
    // Get pointers to input vectors
    EXTERNAL_REAL_DT* input_1_i = r_input_1 + (batch_offset + i) * n_inputs_1;
    EXTERNAL_REAL_DT* input_2_j = r_input_2 + (batch_offset + j) * n_inputs_2;
    
    // Get positional embedding
    EXTERNAL_REAL_DT* pos_emb = nullptr;
    if(positional_dim > 0) {
        pos_emb = pos_embedding + (j - i - 1) * positional_dim;
    }
    
    // Calculate lookup index by applying detector logic to combined input
    int32_t lookup_index = 0;
    
    PFX(calculate_product_lookup_index)(
        sliced_mode,
        detector_anchors,
        n_anchors_per_detector,
        n_inputs_1,
        n_inputs_2,
        positional_dim,
        input_1_i,
        input_2_j,
        pos_emb,
        lookup_index
    );
    
    // Calculate lookup neuron ID
    uint32_t lookup_neuron_id = detector_index * n_lookup_neurons_per_detector + lookup_index;
    w_output += batch_index * sequence_length * n_outputs + j * n_outputs;
    // Fork output filling logic based on sparse connectivity
    if(r_lookup_neuron_synapses_infos != nullptr) {
        // Sparse connectivity: gather outputs using ForwardSynapseGroups
        uint32_t n_lookup_neurons = n_lookup_neurons_per_detector * n_detectors;
        NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + lookup_neuron_id);
        // Iterate through all synapse groups by following the group chain
        NeuronDataId_t current_group_id = lookup_neuron_synapses_info.first_group_id;
        for(uint32_t group_idx = 0; group_idx < lookup_neuron_synapses_info.n_groups; group_idx++) {
            // Fire the current group
            ATOMIC_PFX(fire_single_synapse_group)(
                current_group_id,
                lut_data,
                first_synapse_id,
                r_weights,
                w_output,
                n_lookup_neurons,
                int_rescaler
            );
            
            // Calculate next group ID by reading the current group's size
            if(group_idx + 1 < lookup_neuron_synapses_info.n_groups) {
                ForwardSynapseGroup* fws_group_ptr = GetForwardSynapseGroup(current_group_id, lut_data);
                int32_t current_group_size = static_cast<int32_t>(SynapseGroupSize(fws_group_ptr->meta_info));
                current_group_id += SizeOfForwardSynapseGroup(current_group_size, true);
            }
        }
    } else {
        // Fully connected: directly access weights and accumulate
        uint64_t weights_offset = static_cast<uint64_t>(lookup_neuron_id) * n_outputs;
        
        for(uint32_t output_idx = 0; output_idx < n_outputs; output_idx++) {
            double weight = static_cast<double>(r_weights[weights_offset + output_idx]);
            SUMMATION32_DT payload;
            #ifdef INTEGERS_INSTEAD_OF_FLOATS
            payload = static_cast<SUMMATION32_DT>(weight * static_cast<double>(DENOMINATOR32) * int_rescaler);
            *reinterpret_cast<SUMMATION32_DT *>(w_output + output_idx) += payload;
            #else
            payload = static_cast<SUMMATION32_DT>(weight);
            w_output[output_idx] += payload;
            #endif
        }
    }
}

fill_outputs_product_sparse_logic(
    uint32_t sequence_length,
    uint32_t positional_dim,
    EXTERNAL_REAL_DT* r_input_1, // [batch_size * sequence_length * n_inputs_1]
    uint32_t n_inputs_1,
    EXTERNAL_REAL_DT* r_input_2, // [batch_size * sequence_length * n_inputs_2]
    uint32_t n_inputs_2,
    EXTERNAL_REAL_DT* pos_embedding, // [(sequence_length - 1) * positional_dim]
    AnchorsPair* r_detectors, // [n_detectors * n_anchors_per_detector]
    uint32_t n_detectors,
    uint32_t n_total_tiles,
    uint32_t n_anchors_per_detector,
    EXTERNAL_REAL_DT* r_weights, // [n_detectors * n_lookup_neurons_per_detector * n_outputs]
    uint32_t n_lookup_neurons_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t n_outputs_in_block,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    NeuronDataId_t first_synapse_id,
    uint8_t* lut_data,
    EXTERNAL_REAL_DT* w_output, // [batch_size * sequence_length * n_outputs]
    bool sliced_mode,
    double int_rescaler
) {
    #ifdef ATOMIC
    // blockIdx.x encodes: tile_id * n_detectors + detector_index
    // Extract tile_id and detector_index
    uint32_t tile_id = blockIdx.x / n_detectors;
    uint32_t detector_index = blockIdx.x % n_detectors;
    uint32_t tile_grid_w = (sequence_length + TILE - 1) / TILE;

    if(tile_id >= n_total_tiles) {
        return;
    }

    uint32_t tile_i = tile_id / tile_grid_w;  // tile row
    uint32_t tile_j = tile_id % tile_grid_w;  // tile col

    // Skip tiles below diagonal (tile_i > tile_j means all pairs have i > j, which we don't want)
    if(tile_i > tile_j) {
        return;
    }

    uint32_t i, j;
    if(tile_i == tile_j) {
        i = tile_i * TILE + (threadIdx.x % TILE);
        j = tile_j * TILE + (threadIdx.x / TILE);
        if(i >= j) {
            return;
        }
    } else {
        i = tile_i * TILE + (threadIdx.x % TILE);
        j = tile_j * TILE + (threadIdx.x / TILE);
    }

    if(i >= sequence_length || j >= sequence_length) {
        return;
    }

    // blockIdx.y encodes: batch_index * n_output_blocks + output_block_index
    uint32_t batch_index = blockIdx.y / n_output_blocks;
    uint32_t output_block_index = blockIdx.y % n_output_blocks;

    // Calculate lookup index - each thread calculates for its own (i, j) pair
    int32_t lookup_index = -1; // -1 means invalid/masked
    
    // Get detector anchors
    AnchorsPair* detector_anchors = r_detectors + detector_index * n_anchors_per_detector;

    // Access inputs directly from global memory
    uint32_t batch_offset = batch_index * sequence_length;
    EXTERNAL_REAL_DT* input_1_ptr = r_input_1 + (batch_offset + i) * n_inputs_1;
    EXTERNAL_REAL_DT* input_2_ptr = r_input_2 + (batch_offset + j) * n_inputs_2;

    // Access positional embeddings directly from global memory (if needed)
    EXTERNAL_REAL_DT* pos_emb_ptr = nullptr;
    if(positional_dim > 0) {
        pos_emb_ptr = pos_embedding + (j - i - 1) * positional_dim;
    }

    // Calculate lookup index using helper function
    PFX(calculate_product_lookup_index)(
        sliced_mode,
        detector_anchors,
        n_anchors_per_detector,
        n_inputs_1,
        n_inputs_2,
        positional_dim,
        input_1_ptr,
        input_2_ptr,
        pos_emb_ptr,
        lookup_index
    );

    uint32_t lookup_neuron_id = detector_index * n_lookup_neurons_per_detector + lookup_index;
    uint32_t n_lookup_neurons = n_lookup_neurons_per_detector * n_detectors;

    // Sparse connectivity: find and fire synapse group for current output block
    NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + lookup_neuron_id);

    if(output_block_index < lookup_neuron_synapses_info.n_groups) {
        NeuronDataId_t current_group_id;
        PFX(find_forward_synapse_group)(
            lookup_neuron_synapses_info,
            output_block_index,
            n_outputs_in_block,
            lut_data,
            current_group_id
        );

        // Calculate global output offset for position j
        uint32_t global_offset = batch_index * sequence_length * n_outputs + j * n_outputs;
        w_output += global_offset;

        ATOMIC_PFX(fire_single_synapse_group)(
            current_group_id,
            lut_data,
            first_synapse_id,
            r_weights,
            w_output,
            n_lookup_neurons,
            int_rescaler
        );
    }
    #endif
}

// ======================= PRODUCT, BACKWARD =============================== //

propagate_backward_product_cpu_logic(
    uint32_t sequence_length,
    uint32_t positional_dim,
    EXTERNAL_REAL_DT* r_input_1, // [batch_size * sequence_length * n_inputs_1]
    uint32_t n_inputs_1,
    EXTERNAL_REAL_DT* r_input_2, // [batch_size * sequence_length * n_inputs_2]
    uint32_t n_inputs_2,
    EXTERNAL_REAL_DT* pos_embedding, // [(sequence_length -1 ) * positional_dim] or nullptr
    AnchorsPair* r_detectors, // [n_detectors * n_anchors_per_detector]
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    EXTERNAL_REAL_DT* r_weights, // [n_detectors * n_lookup_neurons_per_detector * n_outputs]
    uint32_t n_lookup_neurons_per_detector,
    uint32_t n_outputs,
    uint32_t batch_size,
    EXTERNAL_REAL_DT* r_output_gradients, // [batch_size * sequence_length * n_outputs]
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos, // nullptr for FC mode
    BaseSynapseMeta* r_synapse_metas, // only used for sparse mode
    NeuronDataId_t first_synapse_id, // only used for sparse mode
    uint8_t* lut_data, // only used for sparse mode
    EXTERNAL_REAL_DT* w_input_gradients_1, // [batch_size * sequence_length * n_inputs_1]
    EXTERNAL_REAL_DT* w_input_gradients_2, // [batch_size * sequence_length * n_inputs_2]
    bool sliced_mode,
    EXTERNAL_REAL_DT* w_weights_gradients, // can be nullptr in internal gradients mode
    double first_synapse_meta_lr,
    EXTERNAL_REAL_DT* w_positional_embeddings_gradients, // [(sequence_length - 1) * positional_dim] or nullptr
    double int_rescaler
) {
    // blockIdx.y encodes: batch_index
    // blockIdx.x encodes: j * n_detectors + detector_index
    // threadIdx.x encodes: i (for position i < j)
    
    uint32_t batch_index = blockIdx.y;
    uint32_t j_detector_combo = blockIdx.x;
    uint32_t j = j_detector_combo / n_detectors;
    uint32_t detector_index = j_detector_combo % n_detectors;
    uint32_t i = threadIdx.x;
    
    // Only process pairs where i < j
    if(i >= j) {
        return;
    }
    
    // Get detector anchors
    AnchorsPair* detector_anchors = r_detectors + detector_index * n_anchors_per_detector;
    
    // Calculate batch offsets
    uint32_t batch_offset = batch_index * sequence_length;
    
    // Get pointers to input vectors
    EXTERNAL_REAL_DT* input_1_i = r_input_1 + (batch_offset + i) * n_inputs_1;
    EXTERNAL_REAL_DT* input_2_j = r_input_2 + (batch_offset + j) * n_inputs_2;
    
    // Get positional embedding
    EXTERNAL_REAL_DT* pos_emb = nullptr;
    if(positional_dim > 0 && pos_embedding != nullptr) {
        pos_emb = pos_embedding + (j - i - 1) * positional_dim;
    }
    
    // Calculate lookup index and track flipping anchors
    int32_t lookup_index;
    int32_t alt_lookup_index;
    uint32_t first_flipping_index;
    uint32_t second_flipping_index;
    EXTERNAL_REAL_DT min_delta;
    uint32_t first_anchor_source;
    uint32_t second_anchor_source;
    
    // Calculate lookup index and find flipping anchors (determines which input component)
    PFX(calculate_product_lookup_index_backprop)(
        sliced_mode,
        detector_anchors,
        n_anchors_per_detector,
        n_inputs_1,
        n_inputs_2,
        positional_dim,
        input_1_i,
        input_2_j,
        pos_emb,
        lookup_index,
        alt_lookup_index,
        first_flipping_index,
        second_flipping_index,
        min_delta,
        first_anchor_source,
        second_anchor_source
    );

    // Calculate lookup neuron ID
    uint32_t lookup_neuron_id = detector_index * n_lookup_neurons_per_detector + lookup_index;
    
    // Get output gradients for position j
    EXTERNAL_REAL_DT* output_grads = r_output_gradients + batch_index * sequence_length * n_outputs + j * n_outputs;
    
    // Compute gradient difference between main and alternative lookup neurons
    double main_grad_sum = 0.0;
    double alt_grad_sum = 0.0;
    
    uint32_t n_lookup_neurons = n_lookup_neurons_per_detector * n_detectors;
    int32_t n_outputs_per_block = static_cast<int32_t>(n_outputs);

    // Compute main gradient sum
    if(r_lookup_neuron_synapses_infos != nullptr) {
        // Sparse connectivity
        NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + lookup_neuron_id);
        NeuronDataId_t current_group_id = lookup_neuron_synapses_info.first_group_id;
        for(uint32_t group_idx = 0; group_idx < lookup_neuron_synapses_info.n_groups; group_idx++) {
            PFX(accumulate_grad_sum_sparse)(
                current_group_id,
                lut_data,
                first_synapse_id,
                output_grads,
                r_weights,
                n_lookup_neurons,
                n_outputs,
                main_grad_sum
            );
            
            if(group_idx + 1 < lookup_neuron_synapses_info.n_groups) {
                ForwardSynapseGroup* fws_group_ptr = GetForwardSynapseGroup(current_group_id, lut_data);
                int32_t current_group_size = static_cast<int32_t>(SynapseGroupSize(fws_group_ptr->meta_info));
                current_group_id += SizeOfForwardSynapseGroup(current_group_size, true);
            }
        }
    } else {
        // Fully connected
        PFX(accumulate_grad_sum_fc)(
            output_grads,
            r_weights,
            lookup_neuron_id,
            n_outputs,
            n_outputs_per_block,
            main_grad_sum
        );
    }
    
    // Compute alternative gradient sum
    NeuronIndex_t alt_neuron_id = detector_index * n_lookup_neurons_per_detector + alt_lookup_index;

    if(r_lookup_neuron_synapses_infos != nullptr) {
        // Sparse connectivity
        NoDelaysIndexedSynapsesInfo alt_synapses_info = *(r_lookup_neuron_synapses_infos + alt_neuron_id);
        NeuronDataId_t current_group_id = alt_synapses_info.first_group_id;
        for(uint32_t group_idx = 0; group_idx < alt_synapses_info.n_groups; group_idx++) {
            PFX(accumulate_grad_sum_sparse)(
                current_group_id,
                lut_data,
                first_synapse_id,
                output_grads,
                r_weights,
                n_lookup_neurons,
                n_outputs,
                alt_grad_sum
            );

            if(group_idx + 1 < alt_synapses_info.n_groups) {
                ForwardSynapseGroup* fws_group_ptr = GetForwardSynapseGroup(current_group_id, lut_data);
                int32_t current_group_size = static_cast<int32_t>(SynapseGroupSize(fws_group_ptr->meta_info));
                current_group_id += SizeOfForwardSynapseGroup(current_group_size, true);
            }
        }
    } else {
        // Fully connected
        PFX(accumulate_grad_sum_fc)(
            output_grads,
            r_weights,
            alt_neuron_id,
            n_outputs,
            n_outputs_per_block,
            alt_grad_sum
        );
    }

    // Compute uncertainty function gradient and propagate based on which input component the flipping anchor belongs to
    PFX(propagate_through_detector_product)(
        min_delta,
        main_grad_sum,
        alt_grad_sum,
        first_flipping_index,
        first_anchor_source,
        second_flipping_index,
        second_anchor_source,
        w_input_gradients_1,
        w_input_gradients_2,
        batch_index,
        sequence_length,
        i,
        j,
        n_inputs_1,
        n_inputs_2,
        positional_dim,
        w_positional_embeddings_gradients,
        int_rescaler
    );
    
    if(w_weights_gradients != nullptr) {
        double external_lr = -1.0;
        if(r_lookup_neuron_synapses_infos != nullptr) {
            // Sparse connectivity: gather weight gradients
            uint32_t output_block_index = 0;
            PFX(gather_weight_gradients_sparse)(
                r_lookup_neuron_synapses_infos,
                lookup_neuron_id,
                output_block_index,
                n_outputs,
                lut_data,
                first_synapse_id,
                output_grads,
                r_synapse_metas,
                first_synapse_meta_lr,
                external_lr,
                n_lookup_neurons,
                w_weights_gradients,
                int_rescaler
            );
        } else {
            // Fully connected: gather weight gradients
            PFX(gather_weight_gradients_fc)(
                output_grads,
                lookup_neuron_id,
                n_outputs,
                n_outputs_per_block,
                first_synapse_meta_lr,
                external_lr,
                w_weights_gradients,
                int_rescaler
            );
        }
    }
}

gather_w_gradients_internal_product_cpu_logic(
    uint32_t sequence_length,
    uint32_t positional_dim,
    EXTERNAL_REAL_DT* r_input_1, // [batch_size * sequence_length * n_inputs_1]
    uint32_t n_inputs_1,
    EXTERNAL_REAL_DT* r_input_2, // [batch_size * sequence_length * n_inputs_2]
    uint32_t n_inputs_2,
    EXTERNAL_REAL_DT* pos_embedding, // [(sequence_length -1 ) * positional_dim] or nullptr
    AnchorsPair* r_detectors, // [n_detectors * n_anchors_per_detector]
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    uint32_t n_lookup_neurons_per_detector,
    uint32_t n_outputs,
    uint32_t batch_size,
    EXTERNAL_REAL_DT* r_output_gradients, // [batch_size * sequence_length * n_outputs]
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos, // nullptr for FC mode
    BaseSynapseMeta* r_synapse_metas, // only used for sparse mode
    NeuronDataId_t first_synapse_id, // only used for sparse mode
    uint8_t* lut_data, // only used for sparse mode
    bool sliced_mode,
    double external_lr,
    EXTERNAL_REAL_DT* w_weights,
    double first_synapse_meta_lr,
    double int_rescaler
) {
    // blockIdx.y encodes: batch_index
    // blockIdx.x encodes: j * n_detectors + detector_index
    // threadIdx.x encodes: i (for position i < j)

    uint32_t batch_index = blockIdx.y;
    uint32_t j_detector_combo = blockIdx.x;
    uint32_t j = j_detector_combo / n_detectors;
    uint32_t detector_index = j_detector_combo % n_detectors;
    uint32_t i = threadIdx.x;

    // Only process pairs where i < j
    if(i >= j) {
        return;
    }

    // Get detector anchors
    AnchorsPair* detector_anchors = r_detectors + detector_index * n_anchors_per_detector;

    // Calculate batch offsets
    uint32_t batch_offset = batch_index * sequence_length;

    // Get pointers to input vectors
    EXTERNAL_REAL_DT* input_1_i = r_input_1 + (batch_offset + i) * n_inputs_1;
    EXTERNAL_REAL_DT* input_2_j = r_input_2 + (batch_offset + j) * n_inputs_2;

    // Get positional embedding
    EXTERNAL_REAL_DT* pos_emb = nullptr;
    if(positional_dim > 0 && pos_embedding != nullptr) {
        pos_emb = pos_embedding + (j - i - 1) * positional_dim;
    }

    // Calculate lookup index and track flipping anchors
    int32_t lookup_index;

    PFX(calculate_product_lookup_index)(
        sliced_mode,
        detector_anchors,
        n_anchors_per_detector,
        n_inputs_1,
        n_inputs_2,
        positional_dim,
        input_1_i,
        input_2_j,
        pos_emb,
        lookup_index
    );

    // Calculate lookup neuron ID
    uint32_t lookup_neuron_id = detector_index * n_lookup_neurons_per_detector + lookup_index;

    // Get output gradients for position j
    EXTERNAL_REAL_DT* output_grads = r_output_gradients + batch_index * sequence_length * n_outputs + j * n_outputs;

    uint32_t n_lookup_neurons = n_lookup_neurons_per_detector * n_detectors;
    int32_t n_outputs_per_block = static_cast<int32_t>(n_outputs);

    if(r_lookup_neuron_synapses_infos != nullptr) {
        // Sparse connectivity: gather weight gradients
        uint32_t output_block_index = 0;
        PFX(gather_weight_gradients_sparse)(
            r_lookup_neuron_synapses_infos,
            lookup_neuron_id,
            output_block_index,
            n_outputs,
            lut_data,
            first_synapse_id,
            output_grads,
            r_synapse_metas,
            first_synapse_meta_lr,
            external_lr,
            n_lookup_neurons,
            w_weights,
            int_rescaler
        );
    } else {
        // Fully connected: gather weight gradients
        PFX(gather_weight_gradients_fc)(
            output_grads,
            lookup_neuron_id,
            n_outputs,
            n_outputs_per_block,
            first_synapse_meta_lr,
            external_lr,
            w_weights,
            int_rescaler
        );
    }
}

propagate_backward_product_sparse_logic(
    uint32_t sequence_length,
    uint32_t positional_dim,
    uint32_t tile_height,
    EXTERNAL_REAL_DT* r_input_1,
    uint32_t n_inputs_1,
    EXTERNAL_REAL_DT* r_input_2,
    uint32_t n_inputs_2,
    EXTERNAL_REAL_DT* r_positional_embeddings,
    AnchorsPair* r_detectors,
    uint32_t n_detectors,
    uint32_t n_detector_blocks,
    uint32_t n_detectors_in_block,
    uint32_t n_anchors_per_detector,
    EXTERNAL_REAL_DT* r_weights,
    uint32_t n_lookup_neurons_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t n_outputs_in_block,
    EXTERNAL_REAL_DT* r_output_gradients,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    BaseSynapseMeta* r_synapse_metas,
    NeuronDataId_t first_synapse_id,
    uint8_t* lut_data,
    EXTERNAL_REAL_DT* w_input_gradients_1, // [batch_size * sequence_length * n_inputs_1]
    EXTERNAL_REAL_DT* w_input_gradients_2, // [batch_size * sequence_length * n_inputs_2]
    bool sliced_mode,
    EXTERNAL_REAL_DT* w_positional_embeddings_gradients, // [(sequence_length - 1) * positional_dim] or nullptr
    double int_rescaler
) {
    #ifdef ATOMIC
    uint32_t t = n_output_blocks * n_detector_blocks;
    uint32_t batch_index = blockIdx.y / t;
    t = blockIdx.y % t;
    uint32_t output_block_index = t / n_detector_blocks;
    uint32_t detector_block_index = t % n_detector_blocks;
    uint64_t linear_idx = static_cast<uint64_t>(blockIdx.x) * blockDim.x + threadIdx.x;
    uint32_t detector_index = detector_block_index * n_detectors_in_block + linear_idx % n_detectors_in_block;

    uint64_t tile_id = linear_idx / n_detectors_in_block; // we use vertical tiles (same j, different i)
    t = tile_id % tile_height;
    tile_id /= tile_height;

    uint32_t i = (tile_id / sequence_length) * tile_height + t;
    uint32_t j = tile_id % sequence_length;

    // calculating i_base
    linear_idx -= threadIdx.x;
    tile_id = linear_idx / n_detectors_in_block;
    t = tile_id % tile_height;
    tile_id /= tile_height;
    uint32_t i_base = (tile_id / sequence_length) * tile_height + t;

    if(i_base >= j) {
        // whole thread block can be skipped
        return;
    }

    // at this point we have absolute i, j, detector_index, batch_index and output_block_index

    // batch_index and output_block_index are fixed for the thread block
    // single thread block always correspond to the certain j (same for all threads in block)
    // single thread block covers tile_height i-s

    // Shared memory layout:
    // - shared_input_2: n_inputs_2 (input_2 for position j)
    // - shared_input_1: tile_height * n_inputs_1 (input_1 for positions i_base to i_base+tile_height-1)
    // - shared_pos_emb: tile_height * positional_dim (positional embeddings, if positional_dim > 0)
    extern __shared__ EXTERNAL_REAL_DT shared_mem[];
    EXTERNAL_REAL_DT* shared_input_2 = shared_mem;
    EXTERNAL_REAL_DT* shared_input_1 = shared_input_2 + n_inputs_2;
    EXTERNAL_REAL_DT* shared_pos_emb = shared_input_1 + tile_height * n_inputs_1;

    // Load inputs to shared memory
    // Thread 0: load input_2 for position j
    if(threadIdx.x == 0) {
        uint32_t batch_offset = batch_index * sequence_length;
        for(uint32_t k = 0; k < n_inputs_2; k++) {
            shared_input_2[k] = r_input_2[(batch_offset + j) * n_inputs_2 + k];
        }
    }

    // Threads 1 to tile_height: load input_1 for positions i_base to i_base+tile_height-1
    if(threadIdx.x >= 1 && threadIdx.x <= tile_height) {
        uint32_t i_idx = threadIdx.x - 1;
        uint32_t actual_i = i_base + i_idx;
        if(actual_i < sequence_length) {
            uint32_t batch_offset = batch_index * sequence_length;
            for(uint32_t k = 0; k < n_inputs_1; k++) {
                shared_input_1[i_idx * n_inputs_1 + k] = r_input_1[(batch_offset + actual_i) * n_inputs_1 + k];
            }
        }
    }

    // Threads tile_height+1 to 2*tile_height: load positional embeddings (if positional_dim > 0)
    if(positional_dim > 0 && threadIdx.x >= tile_height + 1 && threadIdx.x <= 2 * tile_height) {
        uint32_t i_idx = threadIdx.x - (tile_height + 1);
        uint32_t actual_i = i_base + i_idx;
        if(actual_i < sequence_length && j > actual_i) {
            uint32_t pe_offset = (j - actual_i - 1) * positional_dim;
            for(uint32_t k = 0; k < positional_dim; k++) {
                shared_pos_emb[i_idx * positional_dim + k] = r_positional_embeddings[pe_offset + k];
            }
        }
    }

    __syncthreads();

    // Only process valid (i, j) pairs where i < j
    if((detector_index >= n_detectors) || (i >= j)) {
        return;
    }

    // Calculate lookup index with backprop info - each thread calculates for its own (i, j) pair
    int32_t lookup_index;
    int32_t alt_lookup_index;
    uint32_t first_flipping_index = std::numeric_limits<uint32_t>::max();
    uint32_t second_flipping_index = std::numeric_limits<uint32_t>::max();
    EXTERNAL_REAL_DT min_delta = std::numeric_limits<EXTERNAL_REAL_DT>::max();
    uint32_t first_anchor_source = std::numeric_limits<uint32_t>::max();
    uint32_t second_anchor_source = std::numeric_limits<uint32_t>::max();
    
    AnchorsPair* detector_anchors = r_detectors + detector_index * n_anchors_per_detector;

    EXTERNAL_REAL_DT* input_1_ptr = shared_input_1 + (i - i_base) * n_inputs_1;
    EXTERNAL_REAL_DT* input_2_ptr = shared_input_2;
    EXTERNAL_REAL_DT* pos_emb_ptr = nullptr;
    if(positional_dim > 0) {
        pos_emb_ptr = shared_pos_emb + (i - i_base) * positional_dim;
    }

    // Calculate lookup index using backprop helper function
    PFX(calculate_product_lookup_index_backprop)(
        sliced_mode,
        detector_anchors,
        n_anchors_per_detector,
        n_inputs_1,
        n_inputs_2,
        positional_dim,
        input_1_ptr,
        input_2_ptr,
        pos_emb_ptr,
        lookup_index,
        alt_lookup_index,
        first_flipping_index,
        second_flipping_index,
        min_delta,
        first_anchor_source,
        second_anchor_source
    );

    // Calculate lookup neuron IDs
    uint32_t lookup_neuron_id = detector_index * n_lookup_neurons_per_detector + lookup_index;
    uint32_t alt_neuron_id = detector_index * n_lookup_neurons_per_detector + alt_lookup_index;
    
    // Get output gradients for position j
    EXTERNAL_REAL_DT* output_grads = r_output_gradients + batch_index * sequence_length * n_outputs + j * n_outputs;
    
    // Compute gradient sums for both main and alternative lookup neurons (sparse connectivity)
    // Only process the synapse group corresponding to output_block_index
    double main_grad_sum = 0.0;
    double alt_grad_sum = 0.0;
    
    uint32_t n_lookup_neurons = n_lookup_neurons_per_detector * n_detectors;
    
    // Compute main gradient sum (sparse connectivity) - only for output_block_index
    NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + lookup_neuron_id);
    NeuronDataId_t current_group_id;
    
    if(output_block_index < lookup_neuron_synapses_info.n_groups) {
        PFX(find_forward_synapse_group)(
            lookup_neuron_synapses_info,
            output_block_index,
            n_outputs_in_block,
            lut_data,
            current_group_id
        );
        
        ATOMIC_PFX(accumulate_grad_sum_sparse)(
            current_group_id,
            lut_data,
            first_synapse_id,
            output_grads,
            r_weights,
            n_lookup_neurons,
            n_outputs,
            main_grad_sum
        );
    }
    
    // Compute alternative gradient sum (sparse connectivity) - only for output_block_index
    NoDelaysIndexedSynapsesInfo alt_synapses_info = *(r_lookup_neuron_synapses_infos + alt_neuron_id);
    
    if(output_block_index < alt_synapses_info.n_groups) {
        PFX(find_forward_synapse_group)(
            alt_synapses_info,
            output_block_index,
            n_outputs_in_block,
            lut_data,
            current_group_id
        );
        
        ATOMIC_PFX(accumulate_grad_sum_sparse)(
            current_group_id,
            lut_data,
            first_synapse_id,
            output_grads,
            r_weights,
            n_lookup_neurons,
            n_outputs,
            alt_grad_sum
        );
    }

    // Compute uncertainty function gradient and propagate based on which input component the flipping anchor belongs to
    ATOMIC_PFX(propagate_through_detector_product)(
        min_delta,
        main_grad_sum,
        alt_grad_sum,
        first_flipping_index,
        first_anchor_source,
        second_flipping_index,
        second_anchor_source,
        w_input_gradients_1,
        w_input_gradients_2,
        batch_index,
        sequence_length,
        i,
        j,
        n_inputs_1,
        n_inputs_2,
        positional_dim,
        w_positional_embeddings_gradients,
        int_rescaler
    );
    #endif
}

gather_w_gradients_product_sparse_logic(
    uint32_t sequence_length,
    uint32_t positional_dim,
    uint32_t tile_height,
    EXTERNAL_REAL_DT* r_output_gradients,
    EXTERNAL_REAL_DT* r_input_1,
    uint32_t n_inputs_1,
    EXTERNAL_REAL_DT* r_input_2,
    uint32_t n_inputs_2,
    EXTERNAL_REAL_DT* r_positional_embeddings,
    AnchorsPair* r_detectors,
    uint32_t n_detectors,
    uint32_t n_detector_blocks,
    uint32_t n_detectors_in_block,
    uint32_t n_anchors_per_detector,
    EXTERNAL_REAL_DT* w_weights_gradients,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t n_outputs_in_block,
    uint32_t n_lookup_neurons_per_detector,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    BaseSynapseMeta* r_synapse_metas,
    NeuronDataId_t first_synapse_id,
    uint8_t* lut_data,
    bool sliced_mode,
    double external_lr,
    double first_synapse_meta_lr,
    double int_rescaler
) {
    // threads layout similar to gather_w_gradients_product_fc_logic
    // shared memory layout similar to gather_w_gradients_product_fc_logic
    // the logic of the kernel should be consistent with gather_w_gradients_internal_product_cpu_logic (sparse path)
    #ifdef ATOMIC
    uint32_t t = n_output_blocks * n_detector_blocks;
    uint32_t batch_index = blockIdx.y / t;
    t = blockIdx.y % t;
    uint32_t output_block_index = t / n_detector_blocks;
    uint32_t detector_block_index = t % n_detector_blocks;
    uint64_t linear_idx = static_cast<uint64_t>(blockIdx.x) * blockDim.x + threadIdx.x;
    uint32_t detector_index = detector_block_index * n_detectors_in_block + linear_idx % n_detectors_in_block;

    uint64_t tile_id = linear_idx / n_detectors_in_block; // we use vertical tiles (same j, different i)
    t = tile_id % tile_height;
    tile_id /= tile_height;

    uint32_t i = (tile_id / sequence_length) * tile_height + t;
    uint32_t j = tile_id % sequence_length;

    // calculating i_base
    linear_idx -= threadIdx.x;
    tile_id = linear_idx / n_detectors_in_block;
    t = tile_id % tile_height;
    tile_id /= tile_height;
    uint32_t i_base = (tile_id / sequence_length) * tile_height + t;

    if(i_base >= j) {
        // whole thread block can be skipped
        return;
    }

    // at this point we have absolute i, j, detector_index, batch_index and output_block_index

    // batch_index and output_block_index are fixed for the thread block
    // single thread block always correspond to the certain j (same for all threads in block)
    // single thread block covers tile_height i-s

    // Shared memory layout:
    // - shared_input_2: n_inputs_2 (input_2 for position j)
    // - shared_input_1: tile_height * n_inputs_1 (input_1 for positions i_base to i_base+tile_height-1)
    // - shared_pos_emb: tile_height * positional_dim (positional embeddings, if positional_dim > 0)
    extern __shared__ EXTERNAL_REAL_DT shared_mem[];
    EXTERNAL_REAL_DT* shared_input_2 = shared_mem;
    EXTERNAL_REAL_DT* shared_input_1 = shared_input_2 + n_inputs_2;
    EXTERNAL_REAL_DT* shared_pos_emb = shared_input_1 + tile_height * n_inputs_1;

    // Load inputs to shared memory
    // Thread 0: load input_2 for position j
    if(threadIdx.x == 0) {
        uint32_t batch_offset = batch_index * sequence_length;
        for(uint32_t k = 0; k < n_inputs_2; k++) {
            shared_input_2[k] = r_input_2[(batch_offset + j) * n_inputs_2 + k];
        }
    }

    // Threads 1 to tile_height: load input_1 for positions i_base to i_base+tile_height-1
    if(threadIdx.x >= 1 && threadIdx.x <= tile_height) {
        uint32_t i_idx = threadIdx.x - 1;
        uint32_t actual_i = i_base + i_idx;
        if(actual_i < sequence_length) {
            uint32_t batch_offset = batch_index * sequence_length;
            for(uint32_t k = 0; k < n_inputs_1; k++) {
                shared_input_1[i_idx * n_inputs_1 + k] = r_input_1[(batch_offset + actual_i) * n_inputs_1 + k];
            }
        }
    }

    // Threads tile_height+1 to 2*tile_height: load positional embeddings (if positional_dim > 0)
    if(positional_dim > 0 && threadIdx.x >= tile_height + 1 && threadIdx.x <= 2 * tile_height) {
        uint32_t i_idx = threadIdx.x - (tile_height + 1);
        uint32_t actual_i = i_base + i_idx;
        if(actual_i < sequence_length && j > actual_i) {
            uint32_t pe_offset = (j - actual_i - 1) * positional_dim;
            for(uint32_t k = 0; k < positional_dim; k++) {
                shared_pos_emb[i_idx * positional_dim + k] = r_positional_embeddings[pe_offset + k];
            }
        }
    }

    __syncthreads();

    // Only process valid (i, j) pairs where i < j
    if((detector_index >= n_detectors) || (i >= j)) {
        return;
    }

    // Calculate lookup index - each thread calculates for its own (i, j) pair
    int32_t lookup_index;
    
    AnchorsPair* detector_anchors = r_detectors + detector_index * n_anchors_per_detector;

    EXTERNAL_REAL_DT* input_1_ptr = shared_input_1 + (i - i_base) * n_inputs_1;
    EXTERNAL_REAL_DT* input_2_ptr = shared_input_2;
    EXTERNAL_REAL_DT* pos_emb_ptr = nullptr;
    if(positional_dim > 0) {
        pos_emb_ptr = shared_pos_emb + (i - i_base) * positional_dim;
    }

    // Calculate lookup index using helper function
    PFX(calculate_product_lookup_index)(
        sliced_mode,
        detector_anchors,
        n_anchors_per_detector,
        n_inputs_1,
        n_inputs_2,
        positional_dim,
        input_1_ptr,
        input_2_ptr,
        pos_emb_ptr,
        lookup_index
    );

    // Calculate lookup neuron ID
    uint32_t lookup_neuron_id = detector_index * n_lookup_neurons_per_detector + lookup_index;
    
    // Get output gradients for position j
    EXTERNAL_REAL_DT* output_grads = r_output_gradients + batch_index * sequence_length * n_outputs + j * n_outputs;
    
    uint32_t n_lookup_neurons = n_lookup_neurons_per_detector * n_detectors;
    
    // Gather weight gradients (sparse connectivity)
    // output_block_index corresponds to the synapse group index
    ATOMIC_PFX(gather_weight_gradients_sparse)(
        r_lookup_neuron_synapses_infos,
        lookup_neuron_id,
        output_block_index,
        n_outputs_in_block,
        lut_data,
        first_synapse_id,
        output_grads,
        r_synapse_metas,
        first_synapse_meta_lr,
        external_lr,
        n_lookup_neurons,
        w_weights_gradients,
        int_rescaler
    );
    #endif
}

propagate_backward_product_fc_logic(
    uint32_t sequence_length,
    uint32_t positional_dim,
    uint32_t tile_height,
    EXTERNAL_REAL_DT* r_input_1,
    uint32_t n_inputs_1,
    EXTERNAL_REAL_DT* r_input_2,
    uint32_t n_inputs_2,
    EXTERNAL_REAL_DT* r_positional_embeddings,
    AnchorsPair* r_detectors,
    uint32_t n_detectors,
    uint32_t n_detector_blocks,
    uint32_t n_detectors_in_block,
    uint32_t n_anchors_per_detector,
    EXTERNAL_REAL_DT* r_weights,
    uint32_t n_lookup_neurons_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t n_outputs_in_block,
    EXTERNAL_REAL_DT* r_output_gradients,
    EXTERNAL_REAL_DT* w_input_gradients_1, // [batch_size * sequence_length * n_inputs_1]
    EXTERNAL_REAL_DT* w_input_gradients_2, // [batch_size * sequence_length * n_inputs_2]
    bool sliced_mode,
    EXTERNAL_REAL_DT* w_positional_embeddings_gradients, // [(sequence_length - 1) * positional_dim] or nullptr
    double int_rescaler
) {
    // threads layout similar to fill_outputs_product_fc_logic
    // shared memory layout almost similar to fill_outputs_product_fc_logic (here you don't need to store outputs in shared memory)
    // the logic of the kernel should be consistent with propagate_backward_product_cpu_logic
    #ifdef ATOMIC
    uint32_t t = n_output_blocks * n_detector_blocks;
    uint32_t batch_index = blockIdx.y / t;
    t = blockIdx.y % t;
    uint32_t output_block_index = t / n_detector_blocks;
    uint32_t detector_block_index = t % n_detector_blocks;
    uint64_t linear_idx = static_cast<uint64_t>(blockIdx.x) * blockDim.x + threadIdx.x;
    uint32_t detector_index = detector_block_index * n_detectors_in_block + linear_idx % n_detectors_in_block;

    uint64_t tile_id = linear_idx / n_detectors_in_block; // we use vertical tiles (same j, different i)
    t = tile_id % tile_height;
    tile_id /= tile_height;

    uint32_t i = (tile_id / sequence_length) * tile_height + t;
    uint32_t j = tile_id % sequence_length;

    // calculating i_base
    linear_idx -= threadIdx.x;
    tile_id = linear_idx / n_detectors_in_block;
    t = tile_id % tile_height;
    tile_id /= tile_height;
    uint32_t i_base = (tile_id / sequence_length) * tile_height + t;

    if(i_base >= j) {
        // whole thread block can be skipped
        return;
    }

    // at this point we have absolute i, j, detector_index, batch_index and output_block_index

    // batch_index and output_block_index are fixed for the thread block
    // single thread block always correspond to the certain j (same for all threads in block)
    // single thread block covers tile_height i-s

    // Shared memory layout:
    // - shared_input_2: n_inputs_2 (input_2 for position j)
    // - shared_input_1: tile_height * n_inputs_1 (input_1 for positions i_base to i_base+tile_height-1)
    // - shared_pos_emb: tile_height * positional_dim (positional embeddings, if positional_dim > 0)
    extern __shared__ EXTERNAL_REAL_DT shared_mem[];
    EXTERNAL_REAL_DT* shared_input_2 = shared_mem;
    EXTERNAL_REAL_DT* shared_input_1 = shared_input_2 + n_inputs_2;
    EXTERNAL_REAL_DT* shared_pos_emb = shared_input_1 + tile_height * n_inputs_1;

    // Load inputs to shared memory
    // Thread 0: load input_2 for position j
    if(threadIdx.x == 0) {
        uint32_t batch_offset = batch_index * sequence_length;
        for(uint32_t k = 0; k < n_inputs_2; k++) {
            shared_input_2[k] = r_input_2[(batch_offset + j) * n_inputs_2 + k];
        }
    }

    // Threads 1 to tile_height: load input_1 for positions i_base to i_base+tile_height-1
    if(threadIdx.x >= 1 && threadIdx.x <= tile_height) {
        uint32_t i_idx = threadIdx.x - 1;
        uint32_t actual_i = i_base + i_idx;
        if(actual_i < sequence_length) {
            uint32_t batch_offset = batch_index * sequence_length;
            for(uint32_t k = 0; k < n_inputs_1; k++) {
                shared_input_1[i_idx * n_inputs_1 + k] = r_input_1[(batch_offset + actual_i) * n_inputs_1 + k];
            }
        }
    }

    // Threads tile_height+1 to 2*tile_height: load positional embeddings (if positional_dim > 0)
    if(positional_dim > 0 && threadIdx.x >= tile_height + 1 && threadIdx.x <= 2 * tile_height) {
        uint32_t i_idx = threadIdx.x - (tile_height + 1);
        uint32_t actual_i = i_base + i_idx;
        if(actual_i < sequence_length && j > actual_i) {
            uint32_t pe_offset = (j - actual_i - 1) * positional_dim;
            for(uint32_t k = 0; k < positional_dim; k++) {
                shared_pos_emb[i_idx * positional_dim + k] = r_positional_embeddings[pe_offset + k];
            }
        }
    }

    __syncthreads();

    // Only process valid (i, j) pairs where i < j and lookup_index is valid
    if((detector_index >= n_detectors) || (i >= j)) {
        return;
    }

    // Calculate lookup index with backprop info - each thread calculates for its own (i, j) pair
    int32_t lookup_index;
    int32_t alt_lookup_index;
    uint32_t first_flipping_index = std::numeric_limits<uint32_t>::max();
    uint32_t second_flipping_index = std::numeric_limits<uint32_t>::max();
    EXTERNAL_REAL_DT min_delta = std::numeric_limits<EXTERNAL_REAL_DT>::max();
    uint32_t first_anchor_source = std::numeric_limits<uint32_t>::max();
    uint32_t second_anchor_source = std::numeric_limits<uint32_t>::max();
    
    AnchorsPair* detector_anchors = r_detectors + detector_index * n_anchors_per_detector;

    EXTERNAL_REAL_DT* input_1_ptr = shared_input_1 + (i - i_base) * n_inputs_1;
    EXTERNAL_REAL_DT* input_2_ptr = shared_input_2;
    EXTERNAL_REAL_DT* pos_emb_ptr = nullptr;
    if(positional_dim > 0) {
        pos_emb_ptr = shared_pos_emb + (i - i_base) * positional_dim;
    }

    // Calculate lookup index using backprop helper function
    PFX(calculate_product_lookup_index_backprop)(
        sliced_mode,
        detector_anchors,
        n_anchors_per_detector,
        n_inputs_1,
        n_inputs_2,
        positional_dim,
        input_1_ptr,
        input_2_ptr,
        pos_emb_ptr,
        lookup_index,
        alt_lookup_index,
        first_flipping_index,
        second_flipping_index,
        min_delta,
        first_anchor_source,
        second_anchor_source
    );

    // Calculate lookup neuron IDs
    uint32_t lookup_neuron_id = detector_index * n_lookup_neurons_per_detector + lookup_index;
    uint32_t alt_neuron_id = detector_index * n_lookup_neurons_per_detector + alt_lookup_index;
    
    // Get output gradients for position j
    EXTERNAL_REAL_DT* output_grads = r_output_gradients + batch_index * sequence_length * n_outputs + j * n_outputs;
    output_grads += output_block_index * n_outputs_in_block;
    r_weights += output_block_index * n_outputs_in_block;

    // Compute gradient sums for both main and alternative lookup neurons (fully connected)
    double main_grad_sum = 0.0;
    double alt_grad_sum = 0.0;
    
    int32_t n_outputs_per_block;
    if((output_block_index == n_output_blocks - 1) && (n_outputs % n_outputs_in_block)) {
        n_outputs_per_block = n_outputs % n_outputs_in_block;
    } else {
        n_outputs_per_block = static_cast<int32_t>(n_outputs_in_block);
    }

    ATOMIC_PFX(accumulate_pair_of_gradient_sums_fc)(
        output_grads,
        r_weights,
        lookup_neuron_id,
        alt_neuron_id,
        n_outputs,
        n_outputs_per_block,
        main_grad_sum,
        alt_grad_sum
    );

    // Compute uncertainty function gradient and propagate based on which input component the flipping anchor belongs to
    ATOMIC_PFX(propagate_through_detector_product)(
        min_delta,
        main_grad_sum,
        alt_grad_sum,
        first_flipping_index,
        first_anchor_source,
        second_flipping_index,
        second_anchor_source,
        w_input_gradients_1,
        w_input_gradients_2,
        batch_index,
        sequence_length,
        i,
        j,
        n_inputs_1,
        n_inputs_2,
        positional_dim,
        w_positional_embeddings_gradients,
        int_rescaler
    );
    #endif
}

gather_w_gradients_product_fc_logic(
    uint32_t sequence_length,
    uint32_t positional_dim,
    uint32_t tile_height,
    EXTERNAL_REAL_DT* r_output_gradients,
    EXTERNAL_REAL_DT* r_input_1,
    uint32_t n_inputs_1,
    EXTERNAL_REAL_DT* r_input_2,
    uint32_t n_inputs_2,
    EXTERNAL_REAL_DT* r_positional_embeddings,
    AnchorsPair* r_detectors,
    uint32_t n_detectors,
    uint32_t n_detector_blocks,
    uint32_t n_detectors_in_block,
    uint32_t n_anchors_per_detector,
    EXTERNAL_REAL_DT* w_weights_gradients,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t n_outputs_in_block,
    uint32_t n_lookup_neurons_per_detector,
    bool sliced_mode,
    double external_lr,
    double first_synapse_meta_lr,
    double int_rescaler
) {
    // threads layout similar to propagate_backward_product_fc_logic
    // shared memory layout similar to propagate_backward_product_fc_logic
    // the logic of the kernel should be consistent with gather_w_gradients_internal_product_cpu_logic
    #ifdef ATOMIC
    uint32_t t = n_output_blocks * n_detector_blocks;
    uint32_t batch_index = blockIdx.y / t;
    t = blockIdx.y % t;
    uint32_t output_block_index = t / n_detector_blocks;
    uint32_t detector_block_index = t % n_detector_blocks;
    uint64_t linear_idx = static_cast<uint64_t>(blockIdx.x) * blockDim.x + threadIdx.x;
    uint32_t detector_index = detector_block_index * n_detectors_in_block + linear_idx % n_detectors_in_block;

    uint64_t tile_id = linear_idx / n_detectors_in_block; // we use vertical tiles (same j, different i)
    t = tile_id % tile_height;
    tile_id /= tile_height;

    uint32_t i = (tile_id / sequence_length) * tile_height + t;
    uint32_t j = tile_id % sequence_length;

    // calculating i_base
    linear_idx -= threadIdx.x;
    tile_id = linear_idx / n_detectors_in_block;
    t = tile_id % tile_height;
    tile_id /= tile_height;
    uint32_t i_base = (tile_id / sequence_length) * tile_height + t;

    if(i_base >= j) {
        // whole thread block can be skipped
        return;
    }

    // at this point we have absolute i, j, detector_index, batch_index and output_block_index

    // batch_index and output_block_index are fixed for the thread block
    // single thread block always correspond to the certain j (same for all threads in block)
    // single thread block covers tile_height i-s

    // Shared memory layout:
    // - shared_input_2: n_inputs_2 (input_2 for position j)
    // - shared_input_1: tile_height * n_inputs_1 (input_1 for positions i_base to i_base+tile_height-1)
    // - shared_pos_emb: tile_height * positional_dim (positional embeddings, if positional_dim > 0)
    extern __shared__ EXTERNAL_REAL_DT shared_mem[];
    EXTERNAL_REAL_DT* shared_input_2 = shared_mem;
    EXTERNAL_REAL_DT* shared_input_1 = shared_input_2 + n_inputs_2;
    EXTERNAL_REAL_DT* shared_pos_emb = shared_input_1 + tile_height * n_inputs_1;

    // Load inputs to shared memory
    // Thread 0: load input_2 for position j
    if(threadIdx.x == 0) {
        uint32_t batch_offset = batch_index * sequence_length;
        for(uint32_t k = 0; k < n_inputs_2; k++) {
            shared_input_2[k] = r_input_2[(batch_offset + j) * n_inputs_2 + k];
        }
    }

    // Threads 1 to tile_height: load input_1 for positions i_base to i_base+tile_height-1
    if(threadIdx.x >= 1 && threadIdx.x <= tile_height) {
        uint32_t i_idx = threadIdx.x - 1;
        uint32_t actual_i = i_base + i_idx;
        if(actual_i < sequence_length) {
            uint32_t batch_offset = batch_index * sequence_length;
            for(uint32_t k = 0; k < n_inputs_1; k++) {
                shared_input_1[i_idx * n_inputs_1 + k] = r_input_1[(batch_offset + actual_i) * n_inputs_1 + k];
            }
        }
    }

    // Threads tile_height+1 to 2*tile_height: load positional embeddings (if positional_dim > 0)
    if(positional_dim > 0 && threadIdx.x >= tile_height + 1 && threadIdx.x <= 2 * tile_height) {
        uint32_t i_idx = threadIdx.x - (tile_height + 1);
        uint32_t actual_i = i_base + i_idx;
        if(actual_i < sequence_length && j > actual_i) {
            uint32_t pe_offset = (j - actual_i - 1) * positional_dim;
            for(uint32_t k = 0; k < positional_dim; k++) {
                shared_pos_emb[i_idx * positional_dim + k] = r_positional_embeddings[pe_offset + k];
            }
        }
    }

    __syncthreads();

    // Only process valid (i, j) pairs where i < j
    if((detector_index >= n_detectors) || (i >= j) ) {
        return;
    }

    // Calculate lookup index - each thread calculates for its own (i, j) pair
    int32_t lookup_index;
    
    AnchorsPair* detector_anchors = r_detectors + detector_index * n_anchors_per_detector;

    EXTERNAL_REAL_DT* input_1_ptr = shared_input_1 + (i - i_base) * n_inputs_1;
    EXTERNAL_REAL_DT* input_2_ptr = shared_input_2;
    EXTERNAL_REAL_DT* pos_emb_ptr = nullptr;
    if(positional_dim > 0) {
        pos_emb_ptr = shared_pos_emb + (i - i_base) * positional_dim;
    }

    // Calculate lookup index using helper function
    PFX(calculate_product_lookup_index)(
        sliced_mode,
        detector_anchors,
        n_anchors_per_detector,
        n_inputs_1,
        n_inputs_2,
        positional_dim,
        input_1_ptr,
        input_2_ptr,
        pos_emb_ptr,
        lookup_index
    );

    // Calculate lookup neuron ID
    uint32_t lookup_neuron_id = detector_index * n_lookup_neurons_per_detector + lookup_index;
    
    // Get output gradients for position j
    EXTERNAL_REAL_DT* output_grads = r_output_gradients + batch_index * sequence_length * n_outputs + j * n_outputs;
    output_grads += output_block_index * n_outputs_in_block;
    w_weights_gradients += output_block_index * n_outputs_in_block;

    int32_t n_outputs_per_block;
    if((output_block_index == n_output_blocks - 1) && (n_outputs % n_outputs_in_block)) {
        n_outputs_per_block = n_outputs % n_outputs_in_block;
    } else {
        n_outputs_per_block = static_cast<int32_t>(n_outputs_in_block);
    }

    // Gather weight gradients (fully connected)
    ATOMIC_PFX(gather_weight_gradients_fc)(
        output_grads,
        lookup_neuron_id,
        n_outputs,
        n_outputs_per_block,
        first_synapse_meta_lr,
        external_lr,
        w_weights_gradients,
        int_rescaler
    );
    #endif
}

propagate_backward_product_fc_no_shared_logic(
    uint32_t sequence_length,
    uint32_t positional_dim,
    EXTERNAL_REAL_DT* r_input_1,
    uint32_t n_inputs_1,
    EXTERNAL_REAL_DT* r_input_2,
    uint32_t n_inputs_2,
    EXTERNAL_REAL_DT* r_positional_embeddings,
    AnchorsPair* r_detectors,
    uint32_t n_detectors,
    uint32_t n_total_tiles,
    uint32_t n_anchors_per_detector,
    EXTERNAL_REAL_DT* r_weights,
    uint32_t n_lookup_neurons_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t n_outputs_in_block,
    EXTERNAL_REAL_DT* r_output_gradients,
    EXTERNAL_REAL_DT* w_input_gradients_1, // [batch_size * sequence_length * n_inputs_1]
    EXTERNAL_REAL_DT* w_input_gradients_2, // [batch_size * sequence_length * n_inputs_2]
    bool sliced_mode,
    EXTERNAL_REAL_DT* w_positional_embeddings_gradients, // [(sequence_length - 1) * positional_dim] or nullptr
    double int_rescaler
) {
    // threads layout similar to propagate_through_detectors_seq_fc_logic
    // no shared memory - direct global memory access
    // the logic of the kernel should be consistent with propagate_backward_product_cpu_logic
    #ifdef ATOMIC
    // blockIdx.x encodes: tile_id * n_detectors + detector_index
    // Extract tile_id and detector_index
    uint32_t tile_id = blockIdx.x / n_detectors;
    uint32_t detector_index = blockIdx.x % n_detectors;
    uint32_t tile_grid_w = (sequence_length + TILE - 1) / TILE;

    if(tile_id >= n_total_tiles) {
        return;
    }

    uint32_t i = tile_id / tile_grid_w;  // tile row
    uint32_t j = tile_id % tile_grid_w;  // tile col

    // Skip tiles below diagonal (bi > bj means all pairs have i > j, which we don't want)
    if(i > j) {
        return;
    }

    if(i == j) {
        i = i * TILE + (threadIdx.x % TILE);
        j = j * TILE + (threadIdx.x / TILE);
        if(i >= j) {
            return;
        }
    } else {
        i = i * TILE + (threadIdx.x % TILE);
        j = j * TILE + (threadIdx.x / TILE);
    }

    if(i >= sequence_length || j >= sequence_length) {
        return;
    }

    // blockIdx.y encodes: batch_index * n_output_blocks + output_block_index
    uint32_t batch_index = blockIdx.y / n_output_blocks;
    uint32_t output_block_index = blockIdx.y % n_output_blocks;

    // Direct global memory access (no shared memory)
    uint32_t batch_offset = batch_index * sequence_length;
    
    // Access inputs directly from global memory
    EXTERNAL_REAL_DT* input_1_ptr = r_input_1 + (batch_offset + i) * n_inputs_1;
    EXTERNAL_REAL_DT* input_2_ptr = r_input_2 + (batch_offset + j) * n_inputs_2;
    
    // Access positional embeddings directly from global memory (if needed)
    EXTERNAL_REAL_DT* pos_emb_ptr = nullptr;
    if(positional_dim > 0) {
        pos_emb_ptr = r_positional_embeddings + (j - i - 1) * positional_dim;
    }

    // Calculate lookup index with backprop info - each thread calculates for its own (i, j) pair
    int32_t lookup_index;
    int32_t alt_lookup_index;
    uint32_t first_flipping_index = std::numeric_limits<uint32_t>::max();
    uint32_t second_flipping_index = std::numeric_limits<uint32_t>::max();
    EXTERNAL_REAL_DT min_delta = std::numeric_limits<EXTERNAL_REAL_DT>::max();
    uint32_t first_anchor_source = std::numeric_limits<uint32_t>::max();
    uint32_t second_anchor_source = std::numeric_limits<uint32_t>::max();
    
    AnchorsPair* detector_anchors = r_detectors + detector_index * n_anchors_per_detector;

    // Calculate lookup index using backprop helper function
    PFX(calculate_product_lookup_index_backprop)(
        sliced_mode,
        detector_anchors,
        n_anchors_per_detector,
        n_inputs_1,
        n_inputs_2,
        positional_dim,
        input_1_ptr,
        input_2_ptr,
        pos_emb_ptr,
        lookup_index,
        alt_lookup_index,
        first_flipping_index,
        second_flipping_index,
        min_delta,
        first_anchor_source,
        second_anchor_source
    );

    // Calculate lookup neuron IDs
    uint32_t lookup_neuron_id = detector_index * n_lookup_neurons_per_detector + lookup_index;
    uint32_t alt_neuron_id = detector_index * n_lookup_neurons_per_detector + alt_lookup_index;
    
    // Get output gradients for position j
    EXTERNAL_REAL_DT* output_grads = r_output_gradients + batch_index * sequence_length * n_outputs + j * n_outputs;
    output_grads += output_block_index * n_outputs_in_block;
    r_weights += output_block_index * n_outputs_in_block;

    // Compute gradient sums for both main and alternative lookup neurons (fully connected)
    double main_grad_sum = 0.0;
    double alt_grad_sum = 0.0;
    
    int32_t n_outputs_per_block;
    if((output_block_index == n_output_blocks - 1) && (n_outputs % n_outputs_in_block)) {
        n_outputs_per_block = n_outputs % n_outputs_in_block;
    } else {
        n_outputs_per_block = static_cast<int32_t>(n_outputs_in_block);
    }

    ATOMIC_PFX(accumulate_pair_of_gradient_sums_fc)(
        output_grads,
        r_weights,
        lookup_neuron_id,
        alt_neuron_id,
        n_outputs,
        n_outputs_per_block,
        main_grad_sum,
        alt_grad_sum
    );

    // Compute uncertainty function gradient and propagate based on which input component the flipping anchor belongs to
    ATOMIC_PFX(propagate_through_detector_product)(
        min_delta,
        main_grad_sum,
        alt_grad_sum,
        first_flipping_index,
        first_anchor_source,
        second_flipping_index,
        second_anchor_source,
        w_input_gradients_1,
        w_input_gradients_2,
        batch_index,
        sequence_length,
        i,
        j,
        n_inputs_1,
        n_inputs_2,
        positional_dim,
        w_positional_embeddings_gradients,
        int_rescaler
    );
    #endif
}

gather_w_gradients_product_fc_no_shared_logic(
    uint32_t sequence_length,
    uint32_t positional_dim,
    EXTERNAL_REAL_DT* r_output_gradients,
    EXTERNAL_REAL_DT* r_input_1,
    uint32_t n_inputs_1,
    EXTERNAL_REAL_DT* r_input_2,
    uint32_t n_inputs_2,
    EXTERNAL_REAL_DT* r_positional_embeddings,
    AnchorsPair* r_detectors,
    uint32_t n_detectors,
    uint32_t n_total_tiles,
    uint32_t n_anchors_per_detector,
    EXTERNAL_REAL_DT* w_weights_gradients,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t n_outputs_in_block,
    uint32_t n_lookup_neurons_per_detector,
    bool sliced_mode,
    double external_lr,
    double first_synapse_meta_lr,
    double int_rescaler
) {
    // threads layout similar to gather_w_gradients_seq_fc_cpu_logic
    // no shared memory - direct global memory access
    // the logic of the kernel should be consistent with gather_w_gradients_internal_product_cpu_logic
    #ifdef ATOMIC
    // blockIdx.x encodes: tile_id * n_detectors + detector_index
    // Extract tile_id and detector_index
    uint32_t tile_id = blockIdx.x / n_detectors;
    uint32_t detector_index = blockIdx.x % n_detectors;
    uint32_t tile_grid_w = (sequence_length + TILE - 1) / TILE;

    if(tile_id >= n_total_tiles) {
        return;
    }

    uint32_t i = tile_id / tile_grid_w;  // tile row
    uint32_t j = tile_id % tile_grid_w;  // tile col

    // Skip tiles below diagonal (bi > bj means all pairs have i > j, which we don't want)
    if(i > j) {
        return;
    }

    if(i == j) {
        i = i * TILE + (threadIdx.x % TILE);
        j = j * TILE + (threadIdx.x / TILE);
        if(i >= j) {
            return;
        }
    } else {
        i = i * TILE + (threadIdx.x % TILE);
        j = j * TILE + (threadIdx.x / TILE);
    }

    if(i >= sequence_length || j >= sequence_length) {
        return;
    }

    // blockIdx.y encodes: batch_index * n_output_blocks + output_block_index
    uint32_t batch_index = blockIdx.y / n_output_blocks;
    uint32_t output_block_index = blockIdx.y % n_output_blocks;

    // Direct global memory access (no shared memory)
    uint32_t batch_offset = batch_index * sequence_length;
    
    // Access inputs directly from global memory
    EXTERNAL_REAL_DT* input_1_ptr = r_input_1 + (batch_offset + i) * n_inputs_1;
    EXTERNAL_REAL_DT* input_2_ptr = r_input_2 + (batch_offset + j) * n_inputs_2;
    
    // Access positional embeddings directly from global memory (if needed)
    EXTERNAL_REAL_DT* pos_emb_ptr = nullptr;
    if(positional_dim > 0) {
        pos_emb_ptr = r_positional_embeddings + (j - i - 1) * positional_dim;
    }

    // Calculate lookup index - each thread calculates for its own (i, j) pair
    int32_t lookup_index;
    
    AnchorsPair* detector_anchors = r_detectors + detector_index * n_anchors_per_detector;

    // Calculate lookup index using helper function
    PFX(calculate_product_lookup_index)(
        sliced_mode,
        detector_anchors,
        n_anchors_per_detector,
        n_inputs_1,
        n_inputs_2,
        positional_dim,
        input_1_ptr,
        input_2_ptr,
        pos_emb_ptr,
        lookup_index
    );

    // Calculate lookup neuron ID
    uint32_t lookup_neuron_id = detector_index * n_lookup_neurons_per_detector + lookup_index;
    
    // Get output gradients for position j
    EXTERNAL_REAL_DT* output_grads = r_output_gradients + batch_index * sequence_length * n_outputs + j * n_outputs;
    output_grads += output_block_index * n_outputs_in_block;
    w_weights_gradients += output_block_index * n_outputs_in_block;

    int32_t n_outputs_per_block;
    if((output_block_index == n_output_blocks - 1) && (n_outputs % n_outputs_in_block)) {
        n_outputs_per_block = n_outputs % n_outputs_in_block;
    } else {
        n_outputs_per_block = static_cast<int32_t>(n_outputs_in_block);
    }

    // Gather weight gradients (fully connected)
    ATOMIC_PFX(gather_weight_gradients_fc)(
        output_grads,
        lookup_neuron_id,
        n_outputs,
        n_outputs_per_block,
        first_synapse_meta_lr,
        external_lr,
        w_weights_gradients,
        int_rescaler
    );
    #endif
}

// ======================= SEQUENCE, BACKWARD =============================== //

propagate_through_detectors_seq_sparse_logic(
    EXTERNAL_REAL_DT* r_output_gradients,
    EXTERNAL_REAL_DT* r_weights,
    int32_t* r_lookup_indices,
    EXTERNAL_REAL_DT* r_min_anchor_deltas,
    int32_t* r_min_anchor_delta_indices,
    int32_t* r_positional_lookup_indices,
    EXTERNAL_REAL_DT* r_positional_min_deltas,
    int32_t* r_positional_min_delta_indices,
    uint32_t n_detectors,
    uint32_t n_total_tiles,
    uint32_t sequence_length,
    uint32_t n_anchors_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t n_outputs_per_block,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    BaseSynapseMeta* r_synapse_metas,
    NeuronDataId_t first_synapse_id,
    uint8_t* lut_data,
    AnchorsPair* r_detectors,
    uint32_t n_lookup_neurons_per_detector,
    EXTERNAL_REAL_DT* w_input_gradients,
    EXTERNAL_REAL_DT* w_positional_embeddings_gradients,
    uint32_t n_inputs,
    uint32_t positional_dim,
    double int_rescaler
) {
    // blockIdx.x encodes: tile_id * n_detectors + detector_index
    // Extract tile_id and detector_index
    uint32_t tile_id = blockIdx.x / n_detectors;
    uint32_t detector_index = blockIdx.x % n_detectors;
    uint32_t tile_grid_w = (sequence_length + TILE - 1) / TILE;

    if(tile_id >= n_total_tiles) {
        return;
    }

    uint32_t i = tile_id / tile_grid_w;  // tile row
    uint32_t j = tile_id % tile_grid_w;  // tile col

    // Skip tiles below diagonal (bi > bj means all pairs have i > j, which we don't want)
    if(i > j) {
        return;
    }

    if(i == j) {
        i = i * TILE + (threadIdx.x / TILE);
        j = j * TILE + (threadIdx.x % TILE);
        if(i >= j) {
            return;
        }
    } else {
        i = i * TILE + (threadIdx.x / TILE);
        j = j * TILE + (threadIdx.x % TILE);
    }

    if(i >= sequence_length || j >= sequence_length) {
        return;
    }

    // blockIdx.y encodes: batch_index * n_output_blocks + output_block_index
    uint32_t batch_index = blockIdx.y / n_output_blocks;
    uint32_t output_block_index = blockIdx.y % n_output_blocks;
    r_output_gradients += (batch_index * sequence_length + j) * n_outputs;

    // Calculate offsets into lookup_indices arrays
    // Layout: [batch0_t0_det0, batch0_t0_det1, ..., batch0_t0_detN, batch0_t1_det0, ...]
    uint32_t n_detector_items = sequence_length * n_detectors;
    uint32_t batch_offset = batch_index * n_detector_items;

    // Get K: lookup index from first timestep (i)
    uint32_t k_offset = batch_offset + i * n_detectors + detector_index;
    int32_t K = r_lookup_indices[k_offset];
    int32_t k_min_anchor_delta_index = r_min_anchor_delta_indices[k_offset];
    EXTERNAL_REAL_DT k_min_anchor_delta = r_min_anchor_deltas[k_offset];

    // Get Q: lookup index from second timestep (j)
    uint32_t q_offset = batch_offset + j * n_detectors + detector_index;
    int32_t Q = r_lookup_indices[q_offset];
    int32_t q_min_anchor_delta_index = r_min_anchor_delta_indices[q_offset];
    EXTERNAL_REAL_DT q_min_anchor_delta = r_min_anchor_deltas[q_offset];

    // Get PE: positional lookup index from timestep difference (j - i - 1)
    int32_t PE = 0;
    int32_t pe_min_delta_index = -1;
    EXTERNAL_REAL_DT pe_min_delta = 0.0;
    if(positional_dim > 0) {
        uint32_t pe_offset = (j - i - 1) * n_detectors + detector_index;
        PE = r_positional_lookup_indices[pe_offset];
        pe_min_delta_index = r_positional_min_delta_indices[pe_offset];
        pe_min_delta = r_positional_min_deltas[pe_offset];
    }
    w_input_gradients += batch_index * n_inputs * sequence_length;

    // Calculate concatenated index
    uint32_t concatenated_index = (
        (Q << (n_anchors_per_detector + positional_dim)) |
        (K << positional_dim) |
        PE
    ) + detector_index * n_lookup_neurons_per_detector;

    double du;
    uint32_t concatenated_index_alt;
    uint32_t flipping_anchor_index;
    uint32_t i_or_j;
    double main_grad_sum;
    double alt_grad_sum;

    if(fabs(q_min_anchor_delta) < fabs(k_min_anchor_delta)) {
        concatenated_index_alt = (
            ((Q ^ (1 << q_min_anchor_delta_index)) << (n_anchors_per_detector + positional_dim)) |
            (K << positional_dim) |
            PE
        ) + detector_index * n_lookup_neurons_per_detector;

        du = static_cast<double>(q_min_anchor_delta);
        flipping_anchor_index = q_min_anchor_delta_index;
        i_or_j = j;
    } else {
        concatenated_index_alt = (
            (Q << (n_anchors_per_detector + positional_dim)) |
            ((K ^ (1 << k_min_anchor_delta_index)) << positional_dim) |
            PE
        ) + detector_index * n_lookup_neurons_per_detector;

        du = static_cast<double>(k_min_anchor_delta);
        flipping_anchor_index = k_min_anchor_delta_index;
        i_or_j = i;
    }

    if(du > 0) {
        du = 1.0 / (1.0 + fabs(du));
        du *= 0.5 * du;
    } else {
        du = 1.0 / (1.0 + fabs(du));
        du *= -0.5 * du;
    }
    uint32_t n_lookup_neurons = n_lookup_neurons_per_detector * n_detectors;

    main_grad_sum = 0.0;
    alt_grad_sum = 0.0;
    NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info;
    
    // Sparse connectivity: gather output gradients using ForwardSynapseGroups
    lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + concatenated_index);
    NeuronDataId_t current_group_id;

    if(output_block_index < lookup_neuron_synapses_info.n_groups) {
        PFX(find_forward_synapse_group)(
            lookup_neuron_synapses_info,
            output_block_index,
            n_outputs_per_block,
            lut_data,
            current_group_id
        );

        ATOMIC_PFX(accumulate_grad_sum_sparse)(
            current_group_id,
            lut_data,
            first_synapse_id,
            r_output_gradients,
            r_weights,
            n_lookup_neurons,
            n_outputs,
            main_grad_sum
        );
    }

    lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + concatenated_index_alt);

    // Process alt concatenated_index_alt
    if(output_block_index < lookup_neuron_synapses_info.n_groups) {
        PFX(find_forward_synapse_group)(
            lookup_neuron_synapses_info,
            output_block_index,
            n_outputs_per_block,
            lut_data,
            current_group_id
        );

        ATOMIC_PFX(accumulate_grad_sum_sparse)(
            current_group_id,
            lut_data,
            first_synapse_id,
            r_output_gradients,
            r_weights,
            n_lookup_neurons,
            n_outputs,
            alt_grad_sum
        );
    }
    du *= main_grad_sum - alt_grad_sum;

    ATOMIC_PFX(propagate_through_detector)(
        du,
        r_detectors,
        detector_index,
        n_anchors_per_detector,
        flipping_anchor_index,
        w_input_gradients,
        i_or_j,
        n_inputs,
        int_rescaler
    );

    if(positional_dim > 0 && (fabs(pe_min_delta) < fabs(q_min_anchor_delta)) && (fabs(pe_min_delta) < fabs(k_min_anchor_delta))) {
        uint32_t pe_offset = (j - i - 1) * n_detectors + detector_index;
        concatenated_index_alt = (
            (Q << (n_anchors_per_detector + positional_dim)) |
            (K << positional_dim) |
            (PE ^ (1 << r_positional_min_delta_indices[pe_offset]))
        ) + detector_index * n_lookup_neurons_per_detector;

        du = static_cast<double>(pe_min_delta);
        if(du > 0) {
            du = 1.0 / (1.0 + fabs(du));
            du *= 0.5 * du;
        } else {
            du = 1.0 / (1.0 + fabs(du));
            du *= -0.5 * du;
        }
        alt_grad_sum = 0.0;
        // Sparse connectivity: gather output gradients using ForwardSynapseGroups
        lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + concatenated_index_alt);

        if(output_block_index < lookup_neuron_synapses_info.n_groups) {
            PFX(find_forward_synapse_group)(
                lookup_neuron_synapses_info,
                output_block_index,
                n_outputs_per_block,
                lut_data,
                current_group_id
            );

            ATOMIC_PFX(accumulate_grad_sum_sparse)(
                current_group_id,
                lut_data,
                first_synapse_id,
                r_output_gradients,
                r_weights,
                n_lookup_neurons,
                n_outputs,
                alt_grad_sum
            );
        }
        du *= main_grad_sum - alt_grad_sum;
        ATOMIC_PFX(update_positional_gradients)(
            du,
            w_positional_embeddings_gradients,
            pe_offset,
            positional_dim,
            pe_min_delta_index,
            int_rescaler
        );
    }
}

propagate_through_detectors_seq_fc_logic(
    EXTERNAL_REAL_DT* r_output_gradients,
    EXTERNAL_REAL_DT* r_weights,
    int32_t* r_lookup_indices,
    EXTERNAL_REAL_DT* r_min_anchor_deltas,
    int32_t* r_min_anchor_delta_indices,
    int32_t* r_positional_lookup_indices,
    EXTERNAL_REAL_DT* r_positional_min_deltas,
    int32_t* r_positional_min_delta_indices,
    uint32_t n_detectors,
    uint32_t n_total_tiles,
    uint32_t sequence_length,
    uint32_t n_anchors_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    int32_t n_outputs_per_block,
    AnchorsPair* r_detectors,
    uint32_t n_lookup_neurons_per_detector,
    EXTERNAL_REAL_DT* w_input_gradients,
    EXTERNAL_REAL_DT* w_positional_embeddings_gradients,
    uint32_t n_inputs,
    uint32_t positional_dim,
    double int_rescaler
) {
    // blockIdx.x encodes: tile_id * n_detectors + detector_index
    // Extract tile_id and detector_index
    uint32_t tile_id = blockIdx.x / n_detectors;
    uint32_t detector_index = blockIdx.x % n_detectors;
    uint32_t tile_grid_w = (sequence_length + TILE - 1) / TILE;

    if(tile_id >= n_total_tiles) {
        return;
    }

    uint32_t i = tile_id / tile_grid_w;  // tile row
    uint32_t j = tile_id % tile_grid_w;  // tile col

    // Skip tiles below diagonal (bi > bj means all pairs have i > j, which we don't want)
    if(i > j) {
        return;
    }

    if(i == j) {
        i = i * TILE + (threadIdx.x % TILE);
        j = j * TILE + (threadIdx.x / TILE);
        if(i >= j) {
            return;
        }
    } else {
        i = i * TILE + (threadIdx.x % TILE);
        j = j * TILE + (threadIdx.x / TILE);
    }

    if(i >= sequence_length || j >= sequence_length) {
        return;
    }

    // blockIdx.y encodes: batch_index * n_output_blocks + output_block_index
    uint32_t batch_index = blockIdx.y / n_output_blocks;
    uint32_t output_block_index = blockIdx.y % n_output_blocks;
    r_output_gradients += (batch_index * sequence_length + j) * n_outputs;

    r_weights += output_block_index * n_outputs_per_block;
    r_output_gradients += output_block_index * n_outputs_per_block;
    if((output_block_index == n_output_blocks - 1) && (n_outputs % n_outputs_per_block)) {
        n_outputs_per_block = n_outputs % n_outputs_per_block;
    }

    // Calculate offsets into lookup_indices arrays
    // Layout: [batch0_t0_det0, batch0_t0_det1, ..., batch0_t0_detN, batch0_t1_det0, ...]
    uint32_t n_detector_items = sequence_length * n_detectors;
    uint32_t batch_offset = batch_index * n_detector_items;

    // Get K: lookup index from first timestep (i)
    uint32_t k_offset = batch_offset + i * n_detectors + detector_index;
    int32_t K = r_lookup_indices[k_offset];
    int32_t k_min_anchor_delta_index = r_min_anchor_delta_indices[k_offset];
    EXTERNAL_REAL_DT k_min_anchor_delta = r_min_anchor_deltas[k_offset];

    // Get Q: lookup index from second timestep (j)
    uint32_t q_offset = batch_offset + j * n_detectors + detector_index;
    int32_t Q = r_lookup_indices[q_offset];
    int32_t q_min_anchor_delta_index = r_min_anchor_delta_indices[q_offset];
    EXTERNAL_REAL_DT q_min_anchor_delta = r_min_anchor_deltas[q_offset];

    // Get PE: positional lookup index from timestep difference (j - i - 1)
    int32_t PE = 0;
    int32_t pe_min_delta_index = -1;
    EXTERNAL_REAL_DT pe_min_delta = 0.0;
    if(positional_dim > 0) {
        uint32_t pe_offset = (j - i - 1) * n_detectors + detector_index;
        PE = r_positional_lookup_indices[pe_offset];
        pe_min_delta_index = r_positional_min_delta_indices[pe_offset];
        pe_min_delta = r_positional_min_deltas[pe_offset];
    }
    w_input_gradients += batch_index * n_inputs * sequence_length;

    // Calculate concatenated index
    uint32_t concatenated_index = (
        (Q << (n_anchors_per_detector + positional_dim)) |
        (K << positional_dim) |
        PE
    ) + detector_index * n_lookup_neurons_per_detector;

    double du;
    uint32_t concatenated_index_alt;
    uint32_t flipping_anchor_index;
    uint32_t i_or_j;
    double main_grad_sum;
    double alt_grad_sum;

    if(fabs(q_min_anchor_delta) < fabs(k_min_anchor_delta)) {
        concatenated_index_alt = (
            ((Q ^ (1 << q_min_anchor_delta_index)) << (n_anchors_per_detector + positional_dim)) |
            (K << positional_dim) |
            PE
        ) + detector_index * n_lookup_neurons_per_detector;

        du = static_cast<double>(q_min_anchor_delta);
        flipping_anchor_index = q_min_anchor_delta_index;
        i_or_j = j;
    } else {
        concatenated_index_alt = (
            (Q << (n_anchors_per_detector + positional_dim)) |
            ((K ^ (1 << k_min_anchor_delta_index)) << positional_dim) |
            PE
        ) + detector_index * n_lookup_neurons_per_detector;

        du = static_cast<double>(k_min_anchor_delta);
        flipping_anchor_index = k_min_anchor_delta_index;
        i_or_j = i;
    }

    if(du > 0) {
        du = 1.0 / (1.0 + fabs(du));
        du *= 0.5 * du;
    } else {
        du = 1.0 / (1.0 + fabs(du));
        du *= -0.5 * du;
    }

    main_grad_sum = 0.0;
    alt_grad_sum = 0.0;
    
    // Fully connected: direct indexing
    ATOMIC_PFX(accumulate_pair_of_gradient_sums_fc)(
        r_output_gradients,
        r_weights,
        concatenated_index,
        concatenated_index_alt,
        n_outputs,
        n_outputs_per_block,
        main_grad_sum,
        alt_grad_sum
    );
    du *= main_grad_sum - alt_grad_sum;

    ATOMIC_PFX(propagate_through_detector)(
        du,
        r_detectors,
        detector_index,
        n_anchors_per_detector,
        flipping_anchor_index,
        w_input_gradients,
        i_or_j,
        n_inputs,
        int_rescaler
    );

    if(positional_dim > 0 && (fabs(pe_min_delta) < fabs(q_min_anchor_delta)) && (fabs(pe_min_delta) < fabs(k_min_anchor_delta))) {
        uint32_t pe_offset = (j - i - 1) * n_detectors + detector_index;
        concatenated_index_alt = (
            (Q << (n_anchors_per_detector + positional_dim)) |
            (K << positional_dim) |
            (PE ^ (1 << r_positional_min_delta_indices[pe_offset]))
        ) + detector_index * n_lookup_neurons_per_detector;

        du = static_cast<double>(pe_min_delta);
        if(du > 0) {
            du = 1.0 / (1.0 + fabs(du));
            du *= 0.5 * du;
        } else {
            du = 1.0 / (1.0 + fabs(du));
            du *= -0.5 * du;
        }
        alt_grad_sum = 0.0;
        
        // Fully connected: direct indexing
        ATOMIC_PFX(accumulate_grad_sum_fc)(
            r_output_gradients,
            r_weights,
            concatenated_index_alt,
            n_outputs,
            n_outputs_per_block,
            alt_grad_sum
        );
        du *= main_grad_sum - alt_grad_sum;
        ATOMIC_PFX(update_positional_gradients)(
            du,
            w_positional_embeddings_gradients,
            pe_offset,
            positional_dim,
            pe_min_delta_index,
            int_rescaler
        );
    }
}

gather_w_gradients_seq_sparse_logic(
    EXTERNAL_REAL_DT* r_output_gradients,
    int32_t* r_lookup_indices,
    int32_t* r_positional_lookup_indices,
    uint32_t n_detectors,
    uint32_t n_total_tiles,
    uint32_t sequence_length,
    uint32_t n_anchors_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t n_outputs_per_block,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    BaseSynapseMeta* r_synapse_metas,
    NeuronDataId_t first_synapse_id,
    uint8_t* lut_data,
    uint32_t n_lookup_neurons_per_detector,
    EXTERNAL_REAL_DT* w_weights_gradients,
    uint32_t positional_dim,
    double external_lr,
    double first_synapse_meta_lr,
    double int_rescaler
) {
    // blockIdx.x encodes: tile_id * n_detectors + detector_index
    // Extract tile_id and detector_index
    uint32_t tile_id = blockIdx.x / n_detectors;
    uint32_t detector_index = blockIdx.x % n_detectors;
    uint32_t tile_grid_w = (sequence_length + TILE - 1) / TILE;

    if(tile_id >= n_total_tiles) {
        return;
    }

    uint32_t i = tile_id / tile_grid_w;  // tile row
    uint32_t j = tile_id % tile_grid_w;  // tile col

    // Skip tiles below diagonal (bi > bj means all pairs have i > j, which we don't want)
    if(i > j) {
        return;
    }

    if(i == j) {
        i = i * TILE + (threadIdx.x / TILE);
        j = j * TILE + (threadIdx.x % TILE);
        if(i >= j) {
            return;
        }
    } else {
        i = i * TILE + (threadIdx.x / TILE);
        j = j * TILE + (threadIdx.x % TILE);
    }

    if(i >= sequence_length || j >= sequence_length) {
        return;
    }

    // blockIdx.y encodes: batch_index * n_output_blocks + output_block_index
    uint32_t batch_index = blockIdx.y / n_output_blocks;
    uint32_t output_block_index = blockIdx.y % n_output_blocks;
    r_output_gradients += (batch_index * sequence_length + j) * n_outputs;

    // Calculate offsets into lookup_indices arrays
    // Layout: [batch0_t0_det0, batch0_t0_det1, ..., batch0_t0_detN, batch0_t1_det0, ...]
    uint32_t n_detector_items = sequence_length * n_detectors;
    uint32_t batch_offset = batch_index * n_detector_items;

    // Get K: lookup index from first timestep (i)
    uint32_t k_offset = batch_offset + i * n_detectors + detector_index;
    int32_t K = r_lookup_indices[k_offset];

    // Get Q: lookup index from second timestep (j)
    uint32_t q_offset = batch_offset + j * n_detectors + detector_index;
    int32_t Q = r_lookup_indices[q_offset];

    // Get PE: positional lookup index from timestep difference (j - i - 1)
    int32_t PE = 0;
    if(positional_dim > 0) {
        uint32_t pe_offset = (j - i - 1) * n_detectors + detector_index;
        PE = r_positional_lookup_indices[pe_offset];
    }

    // Calculate concatenated index
    uint32_t concatenated_index = (
        (Q << (n_anchors_per_detector + positional_dim)) |
        (K << positional_dim) |
        PE
    ) + detector_index * n_lookup_neurons_per_detector;

    uint32_t n_lookup_neurons = n_lookup_neurons_per_detector * n_detectors;

    // Sparse connectivity: gather weight gradients using ForwardSynapseGroups
    ATOMIC_PFX(gather_weight_gradients_sparse)(
        r_lookup_neuron_synapses_infos,
        concatenated_index,
        output_block_index,
        n_outputs_per_block,
        lut_data,
        first_synapse_id,
        r_output_gradients,
        r_synapse_metas,
        first_synapse_meta_lr,
        external_lr,
        n_lookup_neurons,
        w_weights_gradients,
        int_rescaler
    );
}

gather_w_gradients_seq_fc_cpu_logic(
    EXTERNAL_REAL_DT* r_output_gradients,
    int32_t* r_lookup_indices,
    int32_t* r_positional_lookup_indices,
    uint32_t n_detectors,
    uint32_t n_total_tiles,
    uint32_t sequence_length,
    uint32_t n_anchors_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    int32_t n_outputs_per_block,
    uint32_t n_lookup_neurons_per_detector,
    EXTERNAL_REAL_DT* w_weights_gradients,
    uint32_t positional_dim,
    double external_lr,
    double first_synapse_meta_lr,
    double int_rescaler
) {
    // blockIdx.x encodes: tile_id * n_detectors + detector_index
    // Extract tile_id and detector_index
    uint32_t tile_id = blockIdx.x / n_detectors;
    uint32_t detector_index = blockIdx.x % n_detectors;
    uint32_t tile_grid_w = (sequence_length + TILE - 1) / TILE;

    if(tile_id >= n_total_tiles) {
        return;
    }

    uint32_t i = tile_id / tile_grid_w;  // tile row
    uint32_t j = tile_id % tile_grid_w;  // tile col

    // Skip tiles below diagonal (bi > bj means all pairs have i > j, which we don't want)
    if(i > j) {
        return;
    }

    if(i == j) {
        i = i * TILE + (threadIdx.x % TILE);
        j = j * TILE + (threadIdx.x / TILE);
        if(i >= j) {
            return;
        }
    } else {
        i = i * TILE + (threadIdx.x % TILE);
        j = j * TILE + (threadIdx.x / TILE);
    }

    if(i >= sequence_length || j >= sequence_length) {
        return;
    }

    // blockIdx.y encodes: batch_index * n_output_blocks + output_block_index
    uint32_t batch_index = blockIdx.y / n_output_blocks;
    uint32_t output_block_index = blockIdx.y % n_output_blocks;
    r_output_gradients += (batch_index * sequence_length + j) * n_outputs;

    w_weights_gradients += output_block_index * n_outputs_per_block;
    r_output_gradients += output_block_index * n_outputs_per_block;
    if((output_block_index == n_output_blocks - 1) && (n_outputs % n_outputs_per_block)) {
        n_outputs_per_block = n_outputs % n_outputs_per_block;
    }

    // Calculate offsets into lookup_indices arrays
    // Layout: [batch0_t0_det0, batch0_t0_det1, ..., batch0_t0_detN, batch0_t1_det0, ...]
    uint32_t n_detector_items = sequence_length * n_detectors;
    uint32_t batch_offset = batch_index * n_detector_items;

    // Get K: lookup index from first timestep (i)
    uint32_t k_offset = batch_offset + i * n_detectors + detector_index;
    int32_t K = r_lookup_indices[k_offset];

    // Get Q: lookup index from second timestep (j)
    uint32_t q_offset = batch_offset + j * n_detectors + detector_index;
    int32_t Q = r_lookup_indices[q_offset];

    // Get PE: positional lookup index from timestep difference (j - i - 1)
    int32_t PE = 0;
    if(positional_dim > 0) {
        uint32_t pe_offset = (j - i - 1) * n_detectors + detector_index;
        PE = r_positional_lookup_indices[pe_offset];
    }

    // Calculate concatenated index
    uint32_t concatenated_index = (
        (Q << (n_anchors_per_detector + positional_dim)) |
        (K << positional_dim) |
        PE
    ) + detector_index * n_lookup_neurons_per_detector;

    // Fully connected: direct indexing
    ATOMIC_PFX(gather_weight_gradients_fc)(
        r_output_gradients,
        concatenated_index,
        n_outputs,
        n_outputs_per_block,
        first_synapse_meta_lr,
        external_lr,
        w_weights_gradients,
        int_rescaler
    );
}

gather_w_gradients_seq_fc_cuda_logic(
    EXTERNAL_REAL_DT* r_output_gradients,
    int32_t* r_lookup_indices,
    int32_t* r_positional_lookup_indices,
    uint32_t n_detectors,
    uint32_t n_total_tiles,
    uint32_t sequence_length,
    uint32_t n_anchors_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    int32_t n_outputs_per_block,
    uint32_t n_lookup_neurons_per_detector,
    EXTERNAL_REAL_DT* w_weights_gradients,
    uint32_t positional_dim,
    double external_lr,
    double first_synapse_meta_lr,
    double int_rescaler
) {
    #ifdef ATOMIC
    // blockIdx.x encodes: tile_id * n_detectors + detector_index
    // Extract tile_id and detector_index
    uint32_t tile_id = blockIdx.x / n_detectors;
    uint32_t detector_index = blockIdx.x % n_detectors;
    uint32_t tile_grid_w = (sequence_length + TILE - 1) / TILE;

    if(tile_id >= n_total_tiles) {
        return;
    }

    // Shared memory for K[i], Q[j], and PE[j - i - 1]
    __shared__ int32_t shared_K[TILE];
    __shared__ int32_t shared_Q[TILE];
    __shared__ int32_t shared_PE[TILE - 1];

    uint32_t tile_i = tile_id / tile_grid_w;  // tile row
    uint32_t tile_j = tile_id % tile_grid_w;  // tile col

    // Skip tiles below diagonal (bi > bj means all pairs have i > j, which we don't want)
    if(tile_i > tile_j) {
        return;
    }

    // blockIdx.y encodes: batch_index * n_output_blocks + output_block_index
    uint32_t batch_index = blockIdx.y / n_output_blocks;
    uint32_t output_block_index = blockIdx.y % n_output_blocks;

    // Calculate offsets into lookup_indices arrays
    // Layout: [batch0_t0_det0, batch0_t0_det1, ..., batch0_t0_detN, batch0_t1_det0, ...]
    uint32_t n_detector_items = sequence_length * n_detectors;
    uint32_t batch_offset = batch_index * n_detector_items;

    // Load K, Q, and P values into shared memory cooperatively
    // Different threads load different values:
    // - Threads 0 to TILE-1 load K values
    // - Threads TILE to 2*TILE-1 load Q values
    // - All threads compute and load their own P value based on their (i, j) position
    uint32_t tid = threadIdx.x;
    uint32_t i_base = tile_i * TILE;
    uint32_t j_base = tile_j * TILE;

    // Load K values (threads 0 to TILE-1)
    if(tid < TILE) {
        if((i_base + tid) < sequence_length) {
            uint32_t k_offset = batch_offset + (i_base + tid) * n_detectors + detector_index;
            shared_K[tid] = r_lookup_indices[k_offset];
        } else {
            shared_K[tid] = 0;  // Padding for out-of-bounds
        }
    }

    // Load Q values (threads TILE to 2*TILE-1)
    if(tid >= TILE && tid < 2 * TILE) {
        uint32_t q_idx = tid - TILE;
        if((j_base + q_idx) < sequence_length) {
            uint32_t q_offset = batch_offset + (j_base + q_idx) * n_detectors + detector_index;
            shared_Q[q_idx] = r_lookup_indices[q_offset];
        } else {
            shared_Q[q_idx] = 0;  // Padding for out-of-bounds
        }
    }

    // Load P values (threads TILE to 3*TILE-1)
    if(tid >= 2 * TILE && tid < 3 * TILE - 1) {
        uint32_t p_idx = tid - 2 * TILE;
        uint32_t pe_offset = (j_base - i_base + p_idx) * n_detectors + detector_index;
        shared_PE[p_idx] = r_positional_lookup_indices[pe_offset];
    }

    __syncthreads();

    uint32_t i_local = tid / TILE;
    uint32_t j_local = tid % TILE;
    // Now compute i and j for this thread
    uint32_t i, j;
    if(tile_i == tile_j) {
        i = i_base + i_local;
        j = j_base + j_local;
        if(i >= j) {
            return;
        }
    } else {
        i = i_base + i_local;
        j = j_base + j_local;
    }

    if(i >= sequence_length || j >= sequence_length) {
        return;
    }

    r_output_gradients += (batch_index * sequence_length + j) * n_outputs;
    w_weights_gradients += output_block_index * n_outputs_per_block;
    r_output_gradients += output_block_index * n_outputs_per_block;
    if((output_block_index == n_output_blocks - 1) && (n_outputs % n_outputs_per_block)) {
        n_outputs_per_block = n_outputs % n_outputs_per_block;
    }

    // Get K, Q, and PE from shared memory
    int32_t K = shared_K[i_local];
    int32_t Q = shared_Q[j_local];
    int32_t PE = (positional_dim > 0) ? shared_PE[j_local - i_local - 1] : 0;

    // Calculate concatenated index
    uint32_t concatenated_index = (
        (Q << (n_anchors_per_detector + positional_dim)) |
        (K << positional_dim) |
        PE
    ) + detector_index * n_lookup_neurons_per_detector;

    // Fully connected: direct indexing
    ATOMIC_PFX(gather_weight_gradients_fc)(
        r_output_gradients,
        concatenated_index,
        n_outputs,
        n_outputs_per_block,
        first_synapse_meta_lr,
        external_lr,
        w_weights_gradients,
        int_rescaler
    );
    #endif
}

gather_w_gradients_seq_fc_cuda_no_tiles_logic(
    EXTERNAL_REAL_DT* __restrict__ r_output_gradients,
    int32_t* __restrict__ r_lookup_indices,
    int32_t* __restrict__ r_positional_lookup_indices,
    uint32_t n_detectors,
    uint32_t n_items,
    uint32_t sequence_length,
    uint32_t n_anchors_per_detector,
    uint32_t n_outputs,
    uint32_t n_outputs_aligned,
    uint32_t n_lookup_neurons_per_detector,
    EXTERNAL_REAL_DT* __restrict__ w_weights_gradients,
    uint32_t positional_dim,
    double external_lr,
    double first_synapse_meta_lr,
    double int_rescaler
) {
    #ifdef ATOMIC
    uint64_t linear_idx = static_cast<uint64_t>(blockIdx.x) * blockDim.x + threadIdx.x;

    if(linear_idx >= n_items) {
        return;
    }

    // linear_idx encodes: pair_id * n_outputs + output_index

    uint32_t pair_id = linear_idx / n_outputs_aligned;
    uint32_t output_index = linear_idx % n_outputs_aligned;

    if(output_index >= n_outputs) {
        return;
    }

    uint32_t i = pair_id / sequence_length;
    uint32_t j = pair_id % sequence_length;

    if(i >= j) {
        return;
    }

    // blockIdx.y encodes: batch_index * n_detectors + detector_index
    uint32_t batch_index = blockIdx.y / n_detectors;
    uint32_t detector_index = blockIdx.y % n_detectors;

    // Calculate offsets into lookup_indices arrays
    // Layout: [batch0_t0_det0, batch0_t0_det1, ..., batch0_t0_detN, batch0_t1_det0, ...]
    uint32_t n_detector_items = sequence_length * n_detectors;
    uint32_t batch_offset = batch_index * n_detector_items;

    // K, Q, and PE are constant per warp (since pair_id is constant per warp)
    // Compute concatenated_index once per warp (in lane 0) and broadcast it
    uint32_t lane_id = threadIdx.x & 0x1F;  // lane ID within warp (0-31)
    uint32_t warp_mask = 0xFFFFFFFF;  // Full warp mask
    
    uint32_t concatenated_index;
    if(lane_id == 0) {
        // Lane 0 loads K, Q, and PE, then computes concatenated_index for the warp
        uint32_t k_offset = batch_offset + i * n_detectors + detector_index;
        int32_t K = r_lookup_indices[k_offset];
        
        uint32_t q_offset = batch_offset + j * n_detectors + detector_index;
        int32_t Q = r_lookup_indices[q_offset];
        
        int32_t PE = 0;
        if(positional_dim > 0) {
            uint32_t pe_offset = (j - i - 1) * n_detectors + detector_index;
            PE = r_positional_lookup_indices[pe_offset];
        }
        
        concatenated_index = (
            (Q << (n_anchors_per_detector + positional_dim)) |
            (K << positional_dim) |
            PE
        ) + detector_index * n_lookup_neurons_per_detector;
    }
    
    // Broadcast concatenated_index from lane 0 to all threads in the warp
    concatenated_index = __shfl_sync(warp_mask, concatenated_index, 0);

    r_output_gradients += (batch_index * sequence_length + j) * n_outputs;
    w_weights_gradients += output_index;
    r_output_gradients += output_index;

    SUMMATION32_DT sum32_delta;
    double lr = first_synapse_meta_lr;
    if(external_lr >= 0) {
        lr *= -external_lr;
    }

    uint64_t weights_shift = static_cast<uint64_t>(concatenated_index) * n_outputs;

    double output_grad = static_cast<double>(*r_output_gradients);
    if(fabs(output_grad) > EPS) {
        #ifdef INTEGERS_INSTEAD_OF_FLOATS
            sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(
                reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients) + weights_shift,
                sum32_delta
            );
            #else
            *reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients + weights_shift) += sum32_delta;
            #endif
        #else
            sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr);
            #ifdef ATOMIC
            atomicAdd(
                w_weights_gradients + weights_shift,
                sum32_delta
            );
            #else
            w_weights_gradients[weights_shift] += sum32_delta;
            #endif
        #endif
    }
    #endif
}
