check_detectors_logic(
    EXTERNAL_REAL_DT* r_input,
    uint32_t n_inputs,
    AnchorsPair* r_detectors,
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    int32_t* w_lookup_indices,
    EXTERNAL_REAL_DT* w_min_anchor_deltas,
    int32_t* w_min_anchor_delta_indices
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;

    if(i < n_detectors) {
        // Offset input for current batch (sequence_length == 1)
        r_input += blockIdx.y * n_inputs;
        w_lookup_indices += blockIdx.y * n_detectors + i;
        w_min_anchor_deltas += blockIdx.y * n_detectors + i;
        w_min_anchor_delta_indices += blockIdx.y * n_detectors + i;

        // Get detector's anchor pairs
        AnchorsPair* detector_anchors = r_detectors + i * n_anchors_per_detector;

        // Calculate differences between anchor pairs and form bit representation
        int32_t lookup_index = 0;
        EXTERNAL_REAL_DT min_delta = 0.0;
        int32_t min_delta_anchor_idx = -1;
        AnchorsPair ap;

        for(uint32_t anchor_idx = 0; anchor_idx < n_anchors_per_detector; anchor_idx++) {
            ap = detector_anchors[anchor_idx];

            // Skip if anchor IDs are invalid
            if(ap.anchor1_id == std::numeric_limits<NeuronIndex_t>::max() ||
               ap.anchor2_id == std::numeric_limits<NeuronIndex_t>::max()) {
                continue;
            }

            EXTERNAL_REAL_DT delta = r_input[ap.anchor1_id] - r_input[ap.anchor2_id];

            // Form bit representation: 1 if delta > 0, 0 if delta <= 0
            if(delta > 0.0) {
                lookup_index |= (1 << anchor_idx);
            }

            // Track anchor pair with minimum absolute delta
            if((min_delta_anchor_idx == -1) || (fabs(delta) < fabs(min_delta))) {
                min_delta = delta;
                min_delta_anchor_idx = anchor_idx;
            }
        }

        // Store lookup index and related data
        *w_lookup_indices = lookup_index;
        *w_min_anchor_deltas = min_delta;
        *w_min_anchor_delta_indices = min_delta_anchor_idx;

        __SUPER_DETAILED_TRACE__("check_detectors_logic: detector_index %d, lookup_index %d, min_delta %f, min_delta_anchor_idx %d\n", i, lookup_index, min_delta, min_delta_anchor_idx);
    }
}

fire_detectors_logic(
    EXTERNAL_REAL_DT* r_input,
    uint32_t n_inputs,
    AnchorsPair* r_detectors,
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    uint32_t n_lookup_neurons_per_detector,
    int32_t* w_lookup_indices,
    EXTERNAL_REAL_DT* w_min_anchor_deltas,
    int32_t* w_min_anchor_delta_indices,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    Firing* rw_firings_buffer,
    uint64_t* rw_firings_counter_ptr,
    uint32_t synapse_group_size,
    uint8_t* lut_data,
    int device
) {
    uint32_t tid = threadIdx.x;
    uint32_t i = blockIdx.x * blockDim.x + tid; // blockDim.x should be power of 2

    NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info;
    lookup_neuron_synapses_info.n_groups = 0;

    if(i < n_detectors) {
        // Offset input for current batch (sequence_length == 1)
        r_input += blockIdx.y * n_inputs;
        w_lookup_indices += blockIdx.y * n_detectors + i;
        w_min_anchor_deltas += blockIdx.y * n_detectors + i;
        w_min_anchor_delta_indices += blockIdx.y * n_detectors + i;

        // Get detector's anchor pairs
        AnchorsPair* detector_anchors = r_detectors + i * n_anchors_per_detector;

        // Calculate differences between anchor pairs and form bit representation
        int32_t lookup_index = 0;
        EXTERNAL_REAL_DT min_delta = 0.0;
        int32_t min_delta_anchor_idx = -1;
        AnchorsPair ap;

        for(uint32_t anchor_idx = 0; anchor_idx < n_anchors_per_detector; anchor_idx++) {
            ap = detector_anchors[anchor_idx];

            // Skip if anchor IDs are invalid
            if(ap.anchor1_id == std::numeric_limits<NeuronIndex_t>::max() ||
               ap.anchor2_id == std::numeric_limits<NeuronIndex_t>::max()) {
                continue;
            }

            EXTERNAL_REAL_DT delta = r_input[ap.anchor1_id] - r_input[ap.anchor2_id];

            // Form bit representation: 1 if delta > 0, 0 if delta <= 0
            if(delta > 0.0) {
                lookup_index |= (1 << anchor_idx);
            }

            // Track anchor pair with minimum absolute delta
            if((min_delta_anchor_idx == -1) || (fabs(delta) < fabs(min_delta))) {
                min_delta = delta;
                min_delta_anchor_idx = anchor_idx;
            }
        }

        // Store lookup index and related data
        *w_lookup_indices = lookup_index;
        *w_min_anchor_deltas = min_delta;
        *w_min_anchor_delta_indices = min_delta_anchor_idx;

        __SUPER_DETAILED_TRACE__("fire_detectors_logic: detector_index %d, lookup_index %d, min_delta %f, min_delta_anchor_idx %d\n", i, lookup_index, min_delta, min_delta_anchor_idx);

        if(rw_firings_buffer != nullptr) {
            lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + i * n_lookup_neurons_per_detector + lookup_index);
        }
    }

    if(rw_firings_buffer != nullptr) {
        uint64_t firings_offset;
        if(device == -1) {
            if(lookup_neuron_synapses_info.n_groups > 0) {
                firings_offset = *rw_firings_counter_ptr;
                (*rw_firings_counter_ptr) += lookup_neuron_synapses_info.n_groups;
            }
        } else {
            #ifdef ATOMIC
            extern __shared__ __align__(16) uint8_t __sm[];
            uint32_t *sdata = reinterpret_cast<uint32_t *>(__sm);
            sdata[tid] = lookup_neuron_synapses_info.n_groups;
            __syncthreads();
            uint32_t t;
            int offset;
            int idx;

            // upsweep (reduce)
            for(offset = 1; offset < blockDim.x; offset <<= 1) {
                idx = ((tid + 1) * (offset << 1)) - 1;
                if(idx < blockDim.x) {
                    t = sdata[idx - offset];
                    if(t > 0) {
                        sdata[idx] += t;
                    }
                }
                __syncthreads();
            }

            // inject global shift
            if(tid == 0) {
                sdata[blockDim.x - 1] = atomicAdd(
                    reinterpret_cast<unsigned long long*>(rw_firings_counter_ptr),
                    static_cast<unsigned long long>(sdata[blockDim.x - 1])
                );
            }
            __syncthreads();

            // downsweep
            for(offset = blockDim.x >> 1; offset > 0; offset >>= 1) {
                idx = ((tid + 1) * (offset << 1)) - 1;
                if (idx < blockDim.x) {
                    t = sdata[idx - offset];
                    sdata[idx - offset] = sdata[idx];
                    if(t > 0) {
                        sdata[idx] += t;
                    }
                }
                __syncthreads();
            }

            firings_offset = sdata[tid];
            #endif
        }

        if(lookup_neuron_synapses_info.n_groups > 0) {
            if(lookup_neuron_synapses_info.n_synapse_metas == 1) {
                NeuronDataId_t current_group_id = lookup_neuron_synapses_info.first_group_id;
                for(uint32_t j=0;j < lookup_neuron_synapses_info.n_groups;j++) {
                    rw_firings_buffer[firings_offset + j] = Firing{
                        blockIdx.y, 1.0, current_group_id
                    };
                    current_group_id += SizeOfForwardSynapseGroup(synapse_group_size, true);
                }
            } else {
                DelayInfo* delays_info = DelayInfos(lookup_neuron_synapses_info.delays_info_id, lut_data);
                for(uint32_t j=0;j < lookup_neuron_synapses_info.n_synapse_metas;j++) {
                    DelayInfo delay_info = delays_info[j];
                    if(delay_info != 0) {
                        uint32_t n_groups = DELAY_INFO_N_GROUPS(delay_info);
                        NeuronDataId_t current_group_id = lookup_neuron_synapses_info.first_group_id + DELAY_INFO_BYTE_SHIFT_FROM_FIRST_GROUP(delay_info);
                        for(uint32_t k=0;k < n_groups;k++) {
                            rw_firings_buffer[firings_offset + k] = Firing{
                                blockIdx.y, 1.0, current_group_id
                            };
                            current_group_id += SizeOfForwardSynapseGroup(synapse_group_size, true);
                        }
                        firings_offset += n_groups;
                    }
                }
            }
        }
    }
}

process_single_synapse_group(
    NeuronDataId_t group_id,
    uint8_t* lut_data,
    NeuronDataId_t first_synapse_id,
    EXTERNAL_REAL_DT* r_weights,
    EXTERNAL_REAL_DT* w_output,
    uint32_t n_lookup_neurons,
    double multiplier,
    double int_rescaler
) {
    ForwardSynapseGroup* fws_group_ptr = GetForwardSynapseGroup(group_id, lut_data);
    int32_t current_group_size = static_cast<int32_t>(SynapseGroupSize(fws_group_ptr->meta_info));
    NeuronIndex_t* current_target_ptr = TargetNeuronsInForwardGroup(group_id, lut_data);
    long weight_shift = (reinterpret_cast<uint8_t *>(current_target_ptr) - (lut_data + first_synapse_id)) >> 2;
    EXTERNAL_REAL_DT* weight_ptr = r_weights + weight_shift;
    uint4 targets_quad = make_uint4(0, 0, 0, 0);
    float4 weights_quad = make_float4(0.0, 0.0, 0.0, 0.0);

    NeuronIndex_t target_id;
    double weight;
    SUMMATION32_DT payload;

    __SUPER_DETAILED_TRACE__("[process_single_synapse_group] i %d, current_group_size %d, weight_shift %ld\n", i, current_group_size, weight_shift);

    int32_t n_t_buffered = 0;
    int32_t n_w_buffered = 0;
    for(int32_t cursor=0;cursor < current_group_size;cursor++, current_target_ptr++, weight_ptr++) {
        if(((reinterpret_cast<uintptr_t>(current_target_ptr) & 15) == 0) && (cursor < current_group_size - 3)) {
            #ifdef ATOMIC
            asm volatile(
                "ld.global.v4.s32 {%0,%1,%2,%3}, [%4];"
                : "=r"(targets_quad.x), "=r"(targets_quad.y), "=r"(targets_quad.z), "=r"(targets_quad.w)
                : "l"(current_target_ptr)
            );
            #else
            targets_quad = *reinterpret_cast<uint4 *>(current_target_ptr);
            #endif
            target_id = targets_quad.x;
            n_t_buffered = 3;
        } else if(n_t_buffered > 0) {
            if(n_t_buffered == 3) {
                target_id = targets_quad.y;
            } else if(n_t_buffered == 2) {
                target_id = targets_quad.z;
            } else {
                target_id = targets_quad.w;
            }
            n_t_buffered--;
        } else {
            target_id = *current_target_ptr;
        }

        if(((reinterpret_cast<uintptr_t>(weight_ptr) & 15) == 0) && (cursor < current_group_size - 3)) {
            #ifdef ATOMIC
            asm volatile(
                "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                : "=f"(weights_quad.x), "=f"(weights_quad.y), "=f"(weights_quad.z), "=f"(weights_quad.w)
                : "l"(weight_ptr)
            );
            #else
            weights_quad = *reinterpret_cast<float4 *>(weight_ptr);
            #endif
            weight = static_cast<double>(weights_quad.x);
            n_w_buffered = 3;
        } else if(n_w_buffered > 0) {
            if(n_w_buffered == 3) {
                weight = static_cast<double>(weights_quad.y);
            } else if(n_w_buffered == 2) {
                weight = static_cast<double>(weights_quad.z);
            } else {
                weight = static_cast<double>(weights_quad.w);
            }
            n_w_buffered--;
        } else {
            weight = static_cast<double>(*weight_ptr);
        }

        target_id -= n_lookup_neurons;
        __SUPER_DETAILED_TRACE__("[fill_outputs_by_forward_groups_logic] target_id %d, weight %f\n", target_id, weight);

        weight *= multiplier;

        #ifdef INTEGERS_INSTEAD_OF_FLOATS
            payload = static_cast<SUMMATION32_DT>(weight * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(
                reinterpret_cast<SUMMATION32_DT *>(w_output + target_id),
                payload
            );
            #else
            *reinterpret_cast<SUMMATION32_DT *>(w_output + target_id) += payload;
            #endif
        #else
            payload = static_cast<SUMMATION32_DT>(weight);
            #ifdef ATOMIC
            atomicAdd(
                w_output + target_id,
                payload
            );
            #else
            __SUPER_DETAILED_TRACE__("[fill_outputs_by_forward_groups_logic] target_id %d, payload %f\n", target_id, payload);
            w_output[target_id] += payload;
            #endif
        #endif
    }
}

fill_outputs_by_forward_groups_logic(
    EXTERNAL_REAL_DT* r_weights,
    NeuronDataId_t first_synapse_id,
    Firing* r_firings_buffer,
    uint64_t* r_firings_counter_ptr,
    uint64_t max_firings,
    EXTERNAL_REAL_DT* w_output,
    uint32_t n_lookup_neurons,
    uint32_t n_outputs,
    uint8_t* lut_data,
    double int_rescaler
) {
    uint64_t i = static_cast<uint64_t>(blockIdx.x) * blockDim.x + threadIdx.x;
    if(i < max_firings) {
        if(i < *r_firings_counter_ptr) {
            Firing firing = r_firings_buffer[i];
            w_output += firing.batch_index * n_outputs;
            double multiplier = 1.0;
            ATOMIC_PFX(process_single_synapse_group)(
                firing.data_id,
                lut_data,
                first_synapse_id,
                r_weights,
                w_output,
                n_lookup_neurons,
                multiplier,
                int_rescaler
            );
        }
    }
}

fill_outputs_fully_connected_logic(
    EXTERNAL_REAL_DT* r_weights,
    int32_t* r_lookup_indices,
    EXTERNAL_REAL_DT* w_output,
    uint32_t n_outputs,
    uint32_t n_detectors,
    uint32_t n_detector_blocks,
    uint32_t n_lookup_neurons_per_detector,
    int32_t n_detectors_per_block,
    double int_rescaler
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    if(i < n_outputs * n_detector_blocks) {
        uint32_t output_index = i / n_detector_blocks;
        uint32_t detector_block_index = i % n_detector_blocks;
        __SUPER_DETAILED_TRACE__("fill_outputs_fully_connected_logic: batch_index %d, output_index %d, detector_block_index %d\n", blockIdx.y, output_index, detector_block_index);
        r_lookup_indices += blockIdx.y * n_detectors;
        r_lookup_indices += detector_block_index * n_detectors_per_block;
        w_output += blockIdx.y * n_outputs;
        uint64_t weights_per_detector = static_cast<uint64_t>(n_lookup_neurons_per_detector) * n_outputs;
        __SUPER_DETAILED_TRACE__("fill_outputs_fully_connected_logic: weights_per_detector %lu\n", weights_per_detector);
        r_weights += weights_per_detector * detector_block_index * n_detectors_per_block;
        int4 li_quad = make_int4(0, 0, 0, 0);
        int32_t current_lookup_index;
        int32_t n_buffered = 0;
        double weight_sum = 0.0;
        if((detector_block_index == n_detector_blocks - 1) && (n_detectors % n_detectors_per_block)) {
            n_detectors_per_block = n_detectors % n_detectors_per_block;
        }
        __SUPER_DETAILED_TRACE__("fill_outputs_fully_connected_logic: n_detectors_per_block %d\n", n_detectors_per_block);
        for(int32_t cursor=0;cursor < n_detectors_per_block;cursor++, r_lookup_indices++, r_weights+=weights_per_detector) {
            if(((reinterpret_cast<uintptr_t>(r_lookup_indices) & 15) == 0) && (cursor < n_detectors_per_block - 3)) {
                #ifdef ATOMIC
                asm volatile(
                    "ld.global.v4.s32 {%0,%1,%2,%3}, [%4];"
                    : "=r"(li_quad.x), "=r"(li_quad.y), "=r"(li_quad.z), "=r"(li_quad.w)
                    : "l"(r_lookup_indices)
                );
                #else
                li_quad = *reinterpret_cast<int4 *>(r_lookup_indices);
                #endif
                current_lookup_index = li_quad.x;
                n_buffered = 3;
            } else if(n_buffered > 0) {
                if(n_buffered == 3) {
                    current_lookup_index = li_quad.y;
                } else if(n_buffered == 2) {
                    current_lookup_index = li_quad.z;
                } else {
                    current_lookup_index = li_quad.w;
                }
                n_buffered--;
            } else {
                current_lookup_index = *r_lookup_indices;
            }
            __SUPER_DETAILED_TRACE__("fill_outputs_fully_connected_logic: current_lookup_index %d, output_index %d, weight %f\n", current_lookup_index, output_index, r_weights[current_lookup_index * n_outputs + output_index]);
            weight_sum += static_cast<double>(r_weights[current_lookup_index * n_outputs + output_index]);
        }

        __SUPER_DETAILED_TRACE__("fill_outputs_fully_connected_logic: weight_sum %f\n", weight_sum);

        SUMMATION32_DT payload;
        #ifdef INTEGERS_INSTEAD_OF_FLOATS
            payload = static_cast<SUMMATION32_DT>(weight_sum * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(
                reinterpret_cast<SUMMATION32_DT *>(w_output + output_index),
                payload
            );
            #else
            *reinterpret_cast<SUMMATION32_DT *>(w_output + output_index) += payload;
            #endif
        #else
            payload = static_cast<SUMMATION32_DT>(weight_sum);
            #ifdef ATOMIC
            atomicAdd(
                w_output + output_index,
                payload
            );
            #else
            __SUPER_DETAILED_TRACE__("fill_outputs_fully_connected_logic: payload %f\n", payload);
            w_output[output_index] += payload;
            #endif
        #endif
    }
}

fill_outputs_fully_connected_for_sequence_logic(
    EXTERNAL_REAL_DT* r_weights,
    int32_t* r_lookup_indices,
    int32_t* r_positional_lookup_indices,
    EXTERNAL_REAL_DT* w_output,
    uint32_t n_outputs,
    uint32_t n_detectors,
    uint32_t sequence_length,
    uint32_t n_anchors_per_detector,
    uint32_t positional_dim,
    uint32_t n_lookup_neurons_per_detector,
    double int_rescaler
) {
    // blockIdx.x * blockDim.x + threadIdx.x encodes: detector_index * n_outputs + output_index
    // This ensures threads in one warp accumulate weights for the same output
    uint32_t linear_idx = blockIdx.x * blockDim.x + threadIdx.x;

    if(linear_idx >= n_detectors * n_outputs) {
        return;
    }

    uint32_t detector_index = linear_idx / n_outputs;
    uint32_t output_index = linear_idx % n_outputs;

    
    // blockIdx.y encodes: batch_index * (sequence_length - 1) + j_offset
    // j_offset ranges from 0 to sequence_length - 2, j = j_offset + 1 ranges from 1 to sequence_length - 1
    uint32_t batch_index = blockIdx.y / (sequence_length - 1);
    uint32_t j = blockIdx.y % (sequence_length - 1) + 1;

    // Calculate offsets into lookup_indices arrays
    // Layout: [batch0_t0_det0, batch0_t0_det1, ..., batch0_t0_detN, batch0_t1_det0, ...]
    uint32_t n_detector_items = sequence_length * n_detectors;
    uint32_t batch_offset = batch_index * n_detector_items;

    // Get Q: lookup index from first timestep (j)
    uint32_t q_offset = batch_offset + j * n_detectors + detector_index;
    int32_t Q = r_lookup_indices[q_offset];

    // Get PE lookup indices (shared across batches)
    // PE indices are stored as [(S-1) Ã— N_t]
    
    // Accumulate weight sum over all i from 0 to j (inclusive)
    double weight_sum = 0.0;
    
    for(uint32_t i = 0; i < j; i++) {
        // Get K: lookup index from second timestep (i)
        uint32_t k_offset = batch_offset + i * n_detectors + detector_index;
        int32_t K = r_lookup_indices[k_offset];
        
        // Get PE: positional lookup index from timestep difference (j - i - 1)
        // Note: j - i - 1 can be negative when i == j, but in that case we don't use PE
        int32_t PE = 0;
        uint32_t pe_offset = (j - i - 1) * n_detectors + detector_index;
        PE = r_positional_lookup_indices[pe_offset];

        // Calculate concatenated_index
        uint32_t concatenated_index = (
            (static_cast<uint32_t>(Q) << (n_anchors_per_detector + positional_dim)) |
            (static_cast<uint32_t>(K) << positional_dim) |
            static_cast<uint32_t>(PE)
        );
        
        // Add detector offset to get neuron_id
        uint32_t neuron_id = concatenated_index + detector_index * n_lookup_neurons_per_detector;
        
        // Get weight connecting this neuron to current output
        // Weight layout: weights[neuron_id * n_outputs + output_index]
        double weight = static_cast<double>(r_weights[neuron_id * n_outputs + output_index]);
        weight_sum += weight;
    }
    
    // Calculate output offset
    // Output layout: [batch0_t0_out0, batch0_t0_out1, ..., batch0_t0_outN, batch0_t1_out0, ...]
    uint32_t output_offset = (batch_index * sequence_length + j) * n_outputs + output_index;
    
    // Atomic add accumulated sum to output
    SUMMATION32_DT payload;
    #ifdef INTEGERS_INSTEAD_OF_FLOATS
        payload = static_cast<SUMMATION32_DT>(weight_sum * static_cast<double>(DENOMINATOR32) * int_rescaler);
        #ifdef ATOMIC
        atomicAdd(
            reinterpret_cast<SUMMATION32_DT *>(w_output + output_offset),
            payload
        );
        #else
        *reinterpret_cast<SUMMATION32_DT *>(w_output + output_offset) += payload;
        #endif
    #else
        payload = static_cast<SUMMATION32_DT>(weight_sum);
        #ifdef ATOMIC
        atomicAdd(
            w_output + output_offset,
            payload
        );
        #else
        w_output[output_offset] += payload;
        #endif
    #endif
}

fire_detectors_by_lookup_indices_logic(
    uint32_t n_detectors,
    int32_t* r_lookup_indices,
    int32_t* r_min_anchor_delta_indices,
    uint32_t n_lookup_neurons_per_detector,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    Firing* rw_firings_buffer,
    uint64_t* rw_firings_counter_ptr,
    uint32_t synapse_group_size,
    uint8_t* lut_data,
    int device
) {
    uint32_t tid = threadIdx.x;
    uint32_t i = blockIdx.x * blockDim.x + tid; // blockDim.x should be power of 2

    NoDelaysIndexedSynapsesInfo chosen_lookup_neuron_synapses_info;
    NoDelaysIndexedSynapsesInfo chosen_alternative_lookup_neuron_synapses_info;
    chosen_lookup_neuron_synapses_info.n_groups = 0;
    chosen_alternative_lookup_neuron_synapses_info.n_groups = 0;

    if(i < n_detectors) {
        r_lookup_indices += blockIdx.y * n_detectors;
        r_min_anchor_delta_indices += blockIdx.y * n_detectors;

        int32_t lookup_index = r_lookup_indices[i];
        chosen_lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + i * n_lookup_neurons_per_detector + lookup_index);
        chosen_alternative_lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + i * n_lookup_neurons_per_detector + (lookup_index ^ (1 << r_min_anchor_delta_indices[i])));
    }

    uint64_t firings_offset;
    if(device == -1) {
        if((chosen_lookup_neuron_synapses_info.n_groups + chosen_lookup_neuron_synapses_info.n_groups) > 0) {
            firings_offset = *rw_firings_counter_ptr;
            (*rw_firings_counter_ptr) += chosen_lookup_neuron_synapses_info.n_groups + chosen_alternative_lookup_neuron_synapses_info.n_groups;
        }
    } else {
        #ifdef ATOMIC
        extern __shared__ __align__(16) uint8_t __sm[];
        uint32_t *sdata = reinterpret_cast<uint32_t *>(__sm);
        sdata[tid] = chosen_lookup_neuron_synapses_info.n_groups + chosen_alternative_lookup_neuron_synapses_info.n_groups;
        __syncthreads();
        uint32_t t;
        int offset;
        int idx;

        // upsweep (reduce)
        for(offset = 1; offset < blockDim.x; offset <<= 1) {
            idx = ((tid + 1) * (offset << 1)) - 1;
            if(idx < blockDim.x) {
                t = sdata[idx - offset];
                if(t > 0) {
                    sdata[idx] += t;
                }
            }
            __syncthreads();
        }

        // inject global shift
        if(tid == 0) {
            sdata[blockDim.x - 1] = atomicAdd(
                reinterpret_cast<unsigned long long*>(rw_firings_counter_ptr),
                static_cast<unsigned long long>(sdata[blockDim.x - 1])
            );
        }
        __syncthreads();

        // downsweep
        for(offset = blockDim.x >> 1; offset > 0; offset >>= 1) {
            idx = ((tid + 1) * (offset << 1)) - 1;
            if (idx < blockDim.x) {
                t = sdata[idx - offset];
                sdata[idx - offset] = sdata[idx];
                if(t > 0) {
                    sdata[idx] += t;
                }
            }
            __syncthreads();
        }

        firings_offset = sdata[tid];
        #endif
    }

    if((chosen_lookup_neuron_synapses_info.n_groups + chosen_lookup_neuron_synapses_info.n_groups) > 0) {
        if(chosen_lookup_neuron_synapses_info.n_synapse_metas == 1) {
            NeuronDataId_t current_group_id = chosen_lookup_neuron_synapses_info.first_group_id;
            for(uint32_t j=0;j < chosen_lookup_neuron_synapses_info.n_groups;j++) {
                rw_firings_buffer[firings_offset + j] = Firing{
                    blockIdx.y, 1.0, current_group_id
                };
                current_group_id += SizeOfForwardSynapseGroup(synapse_group_size, true);
            }
            firings_offset += chosen_lookup_neuron_synapses_info.n_groups;
            current_group_id = chosen_alternative_lookup_neuron_synapses_info.first_group_id;
            for(uint32_t j=0;j < chosen_alternative_lookup_neuron_synapses_info.n_groups;j++) {
                rw_firings_buffer[firings_offset + j] = Firing{
                    blockIdx.y, -1.0, current_group_id
                };
                current_group_id += SizeOfForwardSynapseGroup(synapse_group_size, true);
            }
        } else {
            DelayInfo* delays_info = DelayInfos(chosen_lookup_neuron_synapses_info.delays_info_id, lut_data);
            for(uint32_t j=0;j < chosen_lookup_neuron_synapses_info.n_synapse_metas;j++) {
                DelayInfo delay_info = delays_info[j];
                if(delay_info != 0) {
                    uint32_t n_groups = DELAY_INFO_N_GROUPS(delay_info);
                    NeuronDataId_t current_group_id = chosen_lookup_neuron_synapses_info.first_group_id + DELAY_INFO_BYTE_SHIFT_FROM_FIRST_GROUP(delay_info);
                    for(uint32_t k=0;k < n_groups;k++) {
                        rw_firings_buffer[firings_offset + k] = Firing{
                            blockIdx.y, 1.0, current_group_id
                        };
                        current_group_id += SizeOfForwardSynapseGroup(synapse_group_size, true);
                    }
                    firings_offset += n_groups;
                }
            }
            delays_info = DelayInfos(chosen_alternative_lookup_neuron_synapses_info.delays_info_id, lut_data);
            for(uint32_t j=0;j < chosen_alternative_lookup_neuron_synapses_info.n_synapse_metas;j++) {
                DelayInfo delay_info = delays_info[j];
                if(delay_info != 0) {
                    uint32_t n_groups = DELAY_INFO_N_GROUPS(delay_info);
                    NeuronDataId_t current_group_id = chosen_alternative_lookup_neuron_synapses_info.first_group_id + DELAY_INFO_BYTE_SHIFT_FROM_FIRST_GROUP(delay_info);
                    for(uint32_t k=0;k < n_groups;k++) {
                        rw_firings_buffer[firings_offset + k] = Firing{
                            blockIdx.y, -1.0, current_group_id
                        };
                        current_group_id += SizeOfForwardSynapseGroup(synapse_group_size, true);
                    }
                    firings_offset += n_groups;
                }
            }
        }
    }
}

gather_gradients_logic(
    EXTERNAL_REAL_DT* r_weights,
    NeuronDataId_t first_synapse_id,
    Firing* r_firings,
    uint64_t* r_firings_counter_ptr,
    uint64_t max_firings,
    EXTERNAL_REAL_DT* r_output_gradients,
    SUMMATION32_DT* w_before_detectors_gradients,
    EXTERNAL_REAL_DT* w_weights_gradients,
    uint32_t n_lookup_neurons,
    uint32_t n_detectors,
    uint32_t n_outputs,
    uint8_t* lut_data,
    REAL_DT first_synapse_meta_lr,
    REAL_DT external_lr, // affects only weights in Internal gradients mode
    BaseSynapseMeta* r_synapse_metas,
    double int_rescaler
) {
    uint64_t i = static_cast<uint64_t>(blockIdx.x) * blockDim.x + threadIdx.x;
    if(i < max_firings) {
        if(i < *r_firings_counter_ptr) {
            Firing firing = r_firings[i];
            r_output_gradients += firing.batch_index * n_outputs;
            w_before_detectors_gradients += firing.batch_index * 2 * n_detectors;

            ForwardSynapseGroup fws_group = *GetForwardSynapseGroup(firing.data_id, lut_data);
            double lr;
            uint32_t sm_index = SynapseGroupSynapseMetaIndex(fws_group.meta_info);
            if(sm_index == 0) {
                lr = static_cast<double>(first_synapse_meta_lr);
            } else {
                lr = static_cast<double>((r_synapse_metas + sm_index)->lr);
            }

            int32_t current_group_size = static_cast<int32_t>(SynapseGroupSize(fws_group.meta_info));
            NeuronIndex_t* current_target_ptr = TargetNeuronsInForwardGroup(firing.data_id, lut_data);
            long weight_shift = (reinterpret_cast<uint8_t *>(current_target_ptr) - (lut_data + first_synapse_id)) >> 2;
            EXTERNAL_REAL_DT* weight_ptr = r_weights + weight_shift;
            uint4 targets_quad = make_uint4(0, 0, 0, 0);
            float4 weights_quad = make_float4(0.0, 0.0, 0.0, 0.0);

            NeuronIndex_t target_id;
            double weight;
            double output_grad;
            double grad_x = 0.0;
            SUMMATION32_DT sum32_delta;

            __SUPER_DETAILED_TRACE__("[gather_gradients_logic] i %d, current_group_size %d, max_firings %d, weight_shift %ld\n", i, current_group_size, max_firings, weight_shift);

            if(w_weights_gradients == nullptr) {
                // internal gradients mode
                lr = -lr * external_lr;
                w_weights_gradients = r_weights;
            }

            int32_t n_t_buffered = 0;
            int32_t n_w_buffered = 0;
            for(int32_t cursor=0;cursor < current_group_size;cursor++, current_target_ptr++, weight_ptr++, weight_shift++) {
                if(((reinterpret_cast<uintptr_t>(current_target_ptr) & 15) == 0) && (cursor < current_group_size - 3)) {
                    #ifdef ATOMIC
                    asm volatile(
                        "ld.global.v4.s32 {%0,%1,%2,%3}, [%4];"
                        : "=r"(targets_quad.x), "=r"(targets_quad.y), "=r"(targets_quad.z), "=r"(targets_quad.w)
                        : "l"(current_target_ptr)
                    );
                    #else
                    targets_quad = *reinterpret_cast<uint4 *>(current_target_ptr);
                    #endif
                    target_id = targets_quad.x;
                    n_t_buffered = 3;
                } else if(n_t_buffered > 0) {
                    if(n_t_buffered == 3) {
                        target_id = targets_quad.y;
                    } else if(n_t_buffered == 2) {
                        target_id = targets_quad.z;
                    } else {
                        target_id = targets_quad.w;
                    }
                    n_t_buffered--;
                } else {
                    target_id = *current_target_ptr;
                }

                if(((reinterpret_cast<uintptr_t>(weight_ptr) & 15) == 0) && (cursor < current_group_size - 3)) {
                    #ifdef ATOMIC
                    asm volatile(
                        "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                        : "=f"(weights_quad.x), "=f"(weights_quad.y), "=f"(weights_quad.z), "=f"(weights_quad.w)
                        : "l"(weight_ptr)
                    );
                    #else
                    weights_quad = *reinterpret_cast<float4 *>(weight_ptr);
                    #endif
                    weight = static_cast<double>(weights_quad.x);
                    n_w_buffered = 3;
                } else if(n_w_buffered > 0) {
                    if(n_w_buffered == 3) {
                        weight = static_cast<double>(weights_quad.y);
                    } else if(n_w_buffered == 2) {
                        weight = static_cast<double>(weights_quad.z);
                    } else {
                        weight = static_cast<double>(weights_quad.w);
                    }
                    n_w_buffered--;
                } else {
                    weight = static_cast<double>(*weight_ptr);
                }

                __SUPER_DETAILED_TRACE__("[gather_gradients_logic] target_id %d, weight %f\n", target_id, weight);

                target_id -= n_lookup_neurons;
                output_grad = static_cast<double>(r_output_gradients[target_id]);

                if(fabs(output_grad) > EPS) {
                    grad_x += weight * output_grad;
                    if(firing.payload == 1.0) {
                        #ifdef INTEGERS_INSTEAD_OF_FLOATS
                            sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
                            #ifdef ATOMIC
                            atomicAdd(
                                reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients) + weight_shift,
                                sum32_delta
                            );
                            #else
                            *reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients + weight_shift) += sum32_delta;
                            #endif
                        #else
                            sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr);
                            #ifdef ATOMIC
                            atomicAdd(
                                w_weights_gradients + weight_shift,
                                sum32_delta
                            );
                            #else
                            w_weights_gradients[weight_shift] += sum32_delta;
                            #endif
                        #endif
                    }
                }
            }

            if(lr < 0.0) {
                // x_grad should be normal gradient
                if(sm_index == 0) {
                    lr = static_cast<double>(first_synapse_meta_lr);
                } else {
                    lr = static_cast<double>((r_synapse_metas + sm_index)->lr);
                }
            }

            n_lookup_neurons /= n_detectors;
            fws_group.source_neuron_index /= n_lookup_neurons;

            if(firing.payload != 1.0) {
                fws_group.source_neuron_index += n_detectors;
            }

            #ifdef INTEGERS_INSTEAD_OF_FLOATS
                sum32_delta = static_cast<SUMMATION32_DT>(grad_x * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
                #ifdef ATOMIC
                atomicAdd(
                    w_before_detectors_gradients + fws_group.source_neuron_index,
                    sum32_delta
                );
                #else
                w_before_detectors_gradients[fws_group.source_neuron_index] += sum32_delta;
                #endif
            #else
                sum32_delta = static_cast<SUMMATION32_DT>(grad_x * lr);
                #ifdef ATOMIC
                atomicAdd(
                    w_before_detectors_gradients + fws_group.source_neuron_index,
                    sum32_delta
                );
                #else
                __SUPER_DETAILED_TRACE__(
                    "[gather_gradients_logic] firing.batch_index %d, fws_group.source_neuron_index %d, firing.payload %f, sum32_delta %f\n",
                    firing.batch_index, fws_group.source_neuron_index, firing.payload, sum32_delta
                );
                w_before_detectors_gradients[fws_group.source_neuron_index] += sum32_delta;
                #endif
            #endif
        }
    }
}

gather_x_gradients_fully_connected_logic(
    EXTERNAL_REAL_DT* r_weights,
    EXTERNAL_REAL_DT* r_output_gradients,
    int32_t* r_lookup_indices,
    int32_t* r_min_anchor_delta_indices,
    SUMMATION32_DT* w_before_detectors_gradients,
    uint32_t n_outputs,
    uint32_t n_detectors,
    uint32_t n_output_blocks,
    int32_t n_outputs_per_block,
    uint32_t n_lookup_neurons_per_detector,
    double lr,
    double int_rescaler
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    if(i < n_detectors * n_output_blocks) {
        uint32_t detector_index = i / n_output_blocks;
        uint32_t output_block_index = i % n_output_blocks;
        r_lookup_indices += blockIdx.y * n_detectors;
        int32_t lookup_index = r_lookup_indices[detector_index];
        if(r_min_anchor_delta_indices != nullptr) {
            r_min_anchor_delta_indices += blockIdx.y * n_detectors;
            lookup_index ^= (1 << r_min_anchor_delta_indices[detector_index]);
        }
        uint64_t weight_shift = static_cast<uint64_t>(n_lookup_neurons_per_detector) * n_outputs * detector_index;
        weight_shift += n_outputs * lookup_index + output_block_index * n_outputs_per_block;
        r_weights += weight_shift;

        r_output_gradients += blockIdx.y * n_outputs + output_block_index * n_outputs_per_block;
        w_before_detectors_gradients += blockIdx.y * 2 * n_detectors;
        if((output_block_index == n_output_blocks - 1) && (n_outputs % n_outputs_per_block)) {
            n_outputs_per_block = n_outputs % n_outputs_per_block;
        }
        double grad_x = 0.0;
        double output_grad;
        double weight;
        SUMMATION32_DT sum32_delta;
        for(int32_t cursor=0;cursor < n_outputs_per_block;cursor++) {
             weight = static_cast<double>(r_weights[cursor]);
             output_grad = static_cast<double>(r_output_gradients[cursor]);
             grad_x += weight * output_grad;
        }

        if(fabs(grad_x) > EPS) {
            if(r_min_anchor_delta_indices != nullptr) {
                detector_index += n_detectors;
            }
            #ifdef INTEGERS_INSTEAD_OF_FLOATS
                sum32_delta = static_cast<SUMMATION32_DT>(grad_x * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
                #ifdef ATOMIC
                atomicAdd(
                    w_before_detectors_gradients + detector_index,
                    sum32_delta
                );
                #else
                w_before_detectors_gradients[detector_index] += sum32_delta;
                #endif
            #else
                sum32_delta = static_cast<SUMMATION32_DT>(grad_x * lr);
                #ifdef ATOMIC
                atomicAdd(
                    w_before_detectors_gradients + detector_index,
                    sum32_delta
                );
                #else
                w_before_detectors_gradients[detector_index] += sum32_delta;
                #endif
            #endif
        }
    }
}

gather_w_gradients_fully_connected_logic(
    EXTERNAL_REAL_DT* r_output_gradients,
    int32_t* r_lookup_indices,
    EXTERNAL_REAL_DT* w_weights_gradients,
    uint32_t n_outputs,
    uint32_t n_detectors,
    uint32_t n_output_blocks,
    int32_t n_outputs_per_block,
    uint32_t n_lookup_neurons_per_detector,
    double lr,
    double int_rescaler
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    if(i < n_detectors * n_output_blocks) {
        uint32_t detector_index = i / n_output_blocks;
        uint32_t output_block_index = i % n_output_blocks;
        r_lookup_indices += blockIdx.y * n_detectors;
        int32_t lookup_index = r_lookup_indices[detector_index];
        uint64_t weight_shift = static_cast<uint64_t>(n_lookup_neurons_per_detector) * n_outputs * detector_index;
        weight_shift += n_outputs * lookup_index + output_block_index * n_outputs_per_block;
        w_weights_gradients += weight_shift;
        r_output_gradients += blockIdx.y * n_outputs + output_block_index * n_outputs_per_block;
        if((output_block_index == n_output_blocks - 1) && (n_outputs % n_outputs_per_block)) {
            n_outputs_per_block = n_outputs % n_outputs_per_block;
        }
        double output_grad;
        SUMMATION32_DT sum32_delta;
        for(int32_t cursor=0;cursor < n_outputs_per_block;cursor++) {
            output_grad = static_cast<double>(r_output_gradients[cursor]);

            if(fabs(output_grad) > EPS) {
                #ifdef INTEGERS_INSTEAD_OF_FLOATS
                    sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
                    #ifdef ATOMIC
                    atomicAdd(
                        reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients) + cursor,
                        sum32_delta
                    );
                    #else
                    *reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients + cursor) += sum32_delta;
                    #endif
                #else
                    sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr);
                    #ifdef ATOMIC
                    atomicAdd(
                        w_weights_gradients + cursor,
                        sum32_delta
                    );
                    #else
                    w_weights_gradients[cursor] += sum32_delta;
                    #endif
                #endif
            }
        }
    }
}

propagate_through_detectors_logic(
    EXTERNAL_REAL_DT* r_min_anchor_deltas,
    int32_t* r_min_anchor_delta_indices,
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    AnchorsPair* r_detectors,
    uint32_t n_lookup_neurons_per_detector,
    SUMMATION32_DT* rw_before_detectors_gradients,
    EXTERNAL_REAL_DT* w_input_gradients,
    uint32_t n_inputs,
    double int_rescaler
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;

    if(i < n_detectors) {
        uint64_t shift = blockIdx.y * n_detectors;
        r_min_anchor_deltas += shift;
        r_min_anchor_delta_indices += shift;
        rw_before_detectors_gradients += 2 * shift;
        w_input_gradients += blockIdx.y * n_inputs;

        double du = static_cast<double>(r_min_anchor_deltas[i]);
        if(du > 0) {
            du = 1.0 / (1.0 + fabs(du));
            du *= 0.5 * du;
        } else {
            du = 1.0 / (1.0 + fabs(du));
            du *= -0.5 * du;
        }
        du *= static_cast<double>(rw_before_detectors_gradients[i]) - static_cast<double>(rw_before_detectors_gradients[n_detectors + i]);
        #ifdef INTEGERS_INSTEAD_OF_FLOATS
        du *= static_cast<double>(DENOMINATOR32_RECIPROC) / int_rescaler;
        #endif

        if(fabs(du) > EPS) {
            AnchorsPair ap = r_detectors[i * n_anchors_per_detector + r_min_anchor_delta_indices[i]];
        
            #ifdef INTEGERS_INSTEAD_OF_FLOATS
                SUMMATION32_DT sum32_delta = static_cast<SUMMATION32_DT>(du * static_cast<double>(DENOMINATOR32) * int_rescaler);
                #ifdef ATOMIC
                atomicAdd(
                    reinterpret_cast<SUMMATION32_DT *>(w_input_gradients) + ap.anchor1_id,
                    sum32_delta
                );
                atomicAdd(
                    reinterpret_cast<SUMMATION32_DT *>(w_input_gradients) + ap.anchor2_id,
                    -sum32_delta
                );
                #else
                *reinterpret_cast<SUMMATION32_DT *>(w_input_gradients + ap.anchor1_id) += sum32_delta;
                *reinterpret_cast<SUMMATION32_DT *>(w_input_gradients + ap.anchor2_id) -= sum32_delta;
                #endif
            #else
                #ifdef ATOMIC
                atomicAdd(
                    w_input_gradients + ap.anchor1_id,
                    du
                );
                atomicAdd(
                    w_input_gradients + ap.anchor2_id,
                    -du
                );
                #else
                w_input_gradients[ap.anchor1_id] += du;
                w_input_gradients[ap.anchor2_id] -= du;
                #endif
            #endif
        }
        __SUPER_DETAILED_TRACE__(
            "[propagate_through_detectors_logic] batch_index %d, lookup_index %d, alt_lookup_neuron_idx %d\n",
            blockIdx.y, lookup_index, alt_lookup_neuron_idx
        );
        rw_before_detectors_gradients[i] = 0.0;
        rw_before_detectors_gradients[n_detectors + i] = 0.0;
    }
}

convert_integers_to_floats_logic(
    EXTERNAL_REAL_DT* buffer,
    uint64_t n,
    double int_rescaler
) {
    uint64_t i = static_cast<uint64_t>(blockIdx.x) * blockDim.x + threadIdx.x;
    if(i < n) {
        #ifdef INTEGERS_INSTEAD_OF_FLOATS
        buffer += n * blockIdx.y;
        double y = static_cast<double>(*reinterpret_cast<SUMMATION32_DT *>(buffer + i)) * static_cast<double>(DENOMINATOR32_RECIPROC);
        buffer[i] = static_cast<EXTERNAL_REAL_DT>(y / int_rescaler);
        #endif
    }
}

// Kernel for checking detectors across a sequence of timesteps
// Grid: (n_detector_items blocks, batch_size batches) where n_detector_items = sequence_length * n_detectors
// Each thread processes one detector at one timestep
check_detectors_for_sequence_logic(
    EXTERNAL_REAL_DT* r_input,
    uint32_t n_inputs,
    uint32_t sequence_length,
    AnchorsPair* r_detectors,
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    int32_t* w_lookup_indices,
    EXTERNAL_REAL_DT* w_min_anchor_deltas,
    int32_t* w_min_anchor_delta_indices
) {
    // i represents n_detectors * timestep + detector_index
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    uint32_t n_detector_items = sequence_length * n_detectors;
    
    if(i < n_detector_items) {
        uint32_t timestep = i / n_detectors;
        uint32_t detector_index = i % n_detectors;

        // Offset input for current batch and timestep
        // Input layout: [batch0_t0, batch0_t1, ..., batch0_tN, batch1_t0, ...]
        r_input += blockIdx.y * n_inputs * sequence_length + timestep * n_inputs;
        w_lookup_indices += blockIdx.y * n_detector_items + i;
        w_min_anchor_deltas += blockIdx.y * n_detector_items + i;
        w_min_anchor_delta_indices += blockIdx.y * n_detector_items + i;
        
        // Get detector's anchor pairs
        AnchorsPair* detector_anchors = r_detectors + detector_index * n_anchors_per_detector;
        
        // Calculate differences between anchor pairs and form bit representation
        int32_t lookup_index = 0;
        EXTERNAL_REAL_DT min_delta = 0.0;
        int32_t min_delta_anchor_idx = -1;
        AnchorsPair ap;
        
        for(uint32_t anchor_idx = 0; anchor_idx < n_anchors_per_detector; anchor_idx++) {
            ap = detector_anchors[anchor_idx];
            
            // Skip if anchor IDs are invalid
            if(ap.anchor1_id == std::numeric_limits<NeuronIndex_t>::max() ||
               ap.anchor2_id == std::numeric_limits<NeuronIndex_t>::max()) {
                continue;
            }
            
            EXTERNAL_REAL_DT delta = r_input[ap.anchor1_id] - r_input[ap.anchor2_id];
            
            // Form bit representation: 1 if delta > 0, 0 if delta <= 0
            if(delta > 0.0) {
                lookup_index |= (1 << anchor_idx);
            }
            
            // Track anchor pair with minimum absolute delta
            if((min_delta_anchor_idx == -1) || (fabs(delta) < fabs(min_delta))) {
                min_delta = delta;
                min_delta_anchor_idx = anchor_idx;
            }
        }
        
        // Store lookup index and related data
        *w_lookup_indices = lookup_index;
        *w_min_anchor_deltas = min_delta;
        *w_min_anchor_delta_indices = min_delta_anchor_idx;
        
        __SUPER_DETAILED_TRACE__("check_detectors_for_sequence_logic: batch_index %d, detector_index %d, timestep %d, lookup_index %d, min_delta %f, min_delta_anchor_idx %d\n", blockIdx.y, detector_index, timestep, lookup_index, min_delta, min_delta_anchor_idx);
    }
}

check_positional_embeddings_logic(
    uint32_t sequence_length,
    EXTERNAL_REAL_DT* r_positional_embeddings,
    uint32_t n_detectors,
    uint32_t positional_dim,
    int32_t* w_positional_lookup_indices,
    EXTERNAL_REAL_DT* w_positional_min_deltas,
    int32_t* w_positional_min_delta_indices
) {
    // i represents n_detectors * timestep + detector_index
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    uint32_t n_detector_items = (sequence_length - 1) * n_detectors;
    
    if(i < n_detector_items) {
        uint32_t timestep = i / n_detectors;
        uint32_t detector_index = i % n_detectors;
        
        // Calculate offset for this batch, detector, and timestep
        // positional embeddings are not batched
        w_positional_lookup_indices += i;
        w_positional_min_deltas += i;
        w_positional_min_delta_indices += i;
        
        // Get positional embedding for this detector and timestep
        // Positional embeddings are stored as [(sequence_length - 1) * n_detectors * positional_dim]
        r_positional_embeddings += timestep * n_detectors * positional_dim;
        r_positional_embeddings += detector_index * positional_dim;
        
        int32_t lookup_index = 0;
        int32_t min_dim_index = -1;
        EXTERNAL_REAL_DT min_delta;
        EXTERNAL_REAL_DT embedding_value;
        
        for(uint32_t dim = 0; dim < positional_dim; dim++) {
            embedding_value = r_positional_embeddings[dim];
            
            if(embedding_value > 0.0) {
                lookup_index |= (1 << dim);
            }
            
            if((min_dim_index == -1) || (fabs(embedding_value) < fabs(min_delta))) {
                min_delta = embedding_value;
                min_dim_index = dim;
            }
        }
        
        *w_positional_lookup_indices = lookup_index;
        *w_positional_min_deltas = min_delta;
        *w_positional_min_delta_indices = min_dim_index;
        
        __SUPER_DETAILED_TRACE__(
            "check_positional_embeddings_logic: batch_index %d, detector_index %d, timestep %d, lookup_index %d, min_dim_index %d, min_delta %f\n",
            blockIdx.y, detector_index, timestep, lookup_index, min_dim_index, min_delta
        );
    }
}

fill_after_detectors_firing_stat_logic(
    int32_t* r_lookup_indices,
    EXTERNAL_REAL_DT* r_min_anchor_deltas,
    int32_t* r_min_anchor_delta_indices,
    int32_t* r_positional_lookup_indices,
    EXTERNAL_REAL_DT* r_positional_min_deltas,
    int32_t* r_positional_min_delta_indices,
    uint32_t n_total_tiles,
    uint32_t sequence_length,
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    uint32_t positional_dim,
    uint32_t n_lookup_neurons_per_detector,
    NeuronShiftFiring* rw_firings_buffer,
    uint64_t* rw_firings_counter_ptr,
    NeuronShiftFiring* rw_firings_buffer_alternative,
    uint64_t* rw_firings_counter_ptr_alternative,
    int device
) {
    // blockIdx.x encodes: tile_id * n_detectors + detector_index
    // Extract tile_id and detector_index
    uint32_t tile_id = blockIdx.x / n_detectors;
    uint32_t detector_index = blockIdx.x % n_detectors;
    uint32_t tile_grid_w = (sequence_length + TILE - 1) / TILE;

    if(tile_id >= n_total_tiles) {
        return;
    }

    uint32_t bi = tile_id / tile_grid_w;  // tile row
    uint32_t bj = tile_id % tile_grid_w;  // tile col

    // Skip tiles below diagonal (bi > bj means all pairs have i > j, which we don't want)
    if(bi > bj) {
        return;
    }

    uint32_t concatenated_index = std::numeric_limits<uint32_t>::max();
    uint32_t alt_index_1 = std::numeric_limits<uint32_t>::max();
    uint32_t alt_index_2 = std::numeric_limits<uint32_t>::max();
    uint32_t batch_index = blockIdx.y;

    // Calculate tile start positions
    uint32_t i0 = bi * TILE;
    uint32_t j0 = bj * TILE;

    // Get thread coordinates within tile
    uint32_t i = i0 + (threadIdx.x / TILE);  // first timestep (row)
    uint32_t j = j0 + (threadIdx.x % TILE);  // second timestep (col)

    // Check if this (i, j) pair is valid (i < j and within bounds)
    bool valid_pair = false;
    if(bi < bj) {
        // Tile strictly above diagonal - all elements are valid (i < j)
        if(i < sequence_length && j < sequence_length) {
            valid_pair = true;
        }
    } else {
        // Diagonal tile - only process where i < j
        if(i < sequence_length && j < sequence_length && i < j) {
            valid_pair = true;
        }
    }

    if(valid_pair) {
        // Calculate offsets into lookup_indices arrays
        // Layout: [batch0_t0_det0, batch0_t0_det1, ..., batch0_t0_detN, batch0_t1_det0, ...]
        uint32_t n_detector_items = sequence_length * n_detectors;
        uint32_t batch_offset = batch_index * n_detector_items;

        // Get K: lookup index from first timestep (i)
        uint32_t k_offset = batch_offset + i * n_detectors + detector_index;
        int32_t K = r_lookup_indices[k_offset];

        // Get Q: lookup index from second timestep (j)
        uint32_t q_offset = batch_offset + j * n_detectors + detector_index;
        int32_t Q = r_lookup_indices[q_offset];

        // Get PE: positional lookup index from timestep difference (j - i - 1)
        uint32_t pe_offset = (j - i - 1) * n_detectors + detector_index;
        int32_t PE = r_positional_lookup_indices[pe_offset];

        concatenated_index = (
            (Q << (n_anchors_per_detector + positional_dim)) |
            (K << positional_dim) |
            PE
        ) + detector_index * n_lookup_neurons_per_detector;

        if(r_min_anchor_deltas != nullptr) {
            EXTERNAL_REAL_DT q_fabs_min_delta = fabs(r_min_anchor_deltas[q_offset]);
            EXTERNAL_REAL_DT k_fabs_min_delta = fabs(r_min_anchor_deltas[k_offset]);
            if(q_fabs_min_delta < k_fabs_min_delta) {
                alt_index_1 = (
                    ((Q ^ (1 << r_min_anchor_delta_indices[q_offset])) << (n_anchors_per_detector + positional_dim)) |
                    (K << positional_dim) |
                    PE
                ) + detector_index * n_lookup_neurons_per_detector;
            } else {
                alt_index_1 = (
                    (Q << (n_anchors_per_detector + positional_dim)) |
                    ((K ^ (1 << r_min_anchor_delta_indices[k_offset])) << positional_dim) |
                    PE
                ) + detector_index * n_lookup_neurons_per_detector;
            }

            EXTERNAL_REAL_DT pe_fabs_min_delta = fabs(r_positional_min_deltas[pe_offset]);
            if((pe_fabs_min_delta < q_fabs_min_delta) && (pe_fabs_min_delta < k_fabs_min_delta)) {
                alt_index_2 = (
                    (Q << (n_anchors_per_detector + positional_dim)) |
                    (K << positional_dim) |
                    (PE ^ (1 << r_positional_min_delta_indices[pe_offset]))
                ) + detector_index * n_lookup_neurons_per_detector;
            }
        }
    }

    if(device == -1) {
        uint64_t firings_offset;
        if(concatenated_index != std::numeric_limits<uint32_t>::max()) {
            firings_offset = *rw_firings_counter_ptr;
            (*rw_firings_counter_ptr)++;
            rw_firings_buffer[firings_offset] = NeuronShiftFiring{
                batch_index,  // batch_index
                1.0,  // payload
                concatenated_index,  // neuron_id
                j  // shift (timestep)
            };
            if(alt_index_1 != std::numeric_limits<uint32_t>::max()) {
                firings_offset = *rw_firings_counter_ptr_alternative;
                (*rw_firings_counter_ptr_alternative)++;

                rw_firings_buffer_alternative[firings_offset] = NeuronShiftFiring{
                    batch_index,  // batch_index
                    1.0,  // payload (here we just need to remember that the gradient from this element will be used on a backward pass)
                    alt_index_1,  // neuron_id
                    j  // shift (timestep)
                };
            }
            if(alt_index_2 != std::numeric_limits<uint32_t>::max()) {
                firings_offset = *rw_firings_counter_ptr_alternative;
                (*rw_firings_counter_ptr_alternative)++;

                rw_firings_buffer_alternative[firings_offset] = NeuronShiftFiring{
                    batch_index,  // batch_index
                    1.0,  // payload (here we just need to remember that the gradient from this element will be used on a backward pass)
                    alt_index_2,  // neuron_id
                    j  // shift (timestep)
                };
            }
        }
    } else {
    #ifdef ATOMIC
        int tpb    = blockDim.x; // TILE * TILE = 1024
        int tid    = threadIdx.x;
        int lane   = tid & 31;
        int warpId = tid >> 5;
        int NWARPS = tpb >> 5;

        const unsigned FULL = 0xffffffffu;
        __shared__ uint32_t warp_sums_main[32];      // up to 1024 threads
        __shared__ uint32_t warp_sums_alt[32];       // for alternative firings
        __shared__ uint64_t block_base_main;
        __shared__ uint64_t block_base_alt;

        uint32_t n_main = 0;
        uint32_t n_alt = 0;

        if(concatenated_index != std::numeric_limits<uint32_t>::max()) {
            n_main++;
            if(alt_index_1 != std::numeric_limits<uint32_t>::max()) {
                n_alt++;
            }
            if(alt_index_2 != std::numeric_limits<uint32_t>::max()) {
                n_alt++;
            }
        }

        // --- warp inclusive scan on n_main ---
        uint32_t prefix_main = n_main;
        #pragma unroll
        for (int off = 1; off < 32; off <<= 1) {
            uint32_t x = __shfl_up_sync(FULL, prefix_main, off);
            if (lane >= off) prefix_main += x;
        }

        if (lane == 31)
            warp_sums_main[warpId] = prefix_main;

        // --- warp inclusive scan on n_alt ---
        uint32_t prefix_alt = n_alt;
        if(rw_firings_counter_ptr_alternative != nullptr) {
            #pragma unroll
            for (int off = 1; off < 32; off <<= 1) {
                uint32_t x = __shfl_up_sync(FULL, prefix_alt, off);
                if (lane >= off) prefix_alt += x;
            }

            if (lane == 31)
                warp_sums_alt[warpId] = prefix_alt;
        }

        __syncthreads();

        // --- warp 0 scans warp sums for main ---
        if (warpId == 0) {
            uint32_t wv = (lane < NWARPS) ? warp_sums_main[lane] : 0;

            #pragma unroll
            for (int off = 1; off < NWARPS; off <<= 1) {
                uint32_t x = __shfl_up_sync(FULL, wv, off);
                if (lane >= off) wv += x;
            }

            if (lane < NWARPS)
                warp_sums_main[lane] = wv;
        }

        // --- warp 1 scans warp sums for alt ---
        if(rw_firings_counter_ptr_alternative != nullptr && warpId == 1) {
            uint32_t wv = (lane < NWARPS) ? warp_sums_alt[lane] : 0;

            #pragma unroll
            for (int off = 1; off < NWARPS; off <<= 1) {
                uint32_t x = __shfl_up_sync(FULL, wv, off);
                if (lane >= off) wv += x;
            }

            if (lane < NWARPS)
                warp_sums_alt[lane] = wv;
        }

        __syncthreads();

        // Add warp prefix to thread prefix
        if (warpId > 0) {
            prefix_main += warp_sums_main[warpId - 1];
            if(rw_firings_counter_ptr_alternative != nullptr) {
                prefix_alt += warp_sums_alt[warpId - 1];
            }
        }

        // --- one atomic per block for main ---
        if (tid == 0) {
            uint32_t block_sum_main = warp_sums_main[NWARPS - 1];
            block_base_main = atomicAdd(
                reinterpret_cast<unsigned long long*>(rw_firings_counter_ptr),
                static_cast<unsigned long long>(block_sum_main)
            );
        }

        // --- one atomic per block for alt ---
        if(rw_firings_counter_ptr_alternative != nullptr) {
            if (tid == 1) {
                uint32_t block_sum_alt = warp_sums_alt[NWARPS - 1];
                block_base_alt = atomicAdd(
                    reinterpret_cast<unsigned long long*>(rw_firings_counter_ptr_alternative),
                    static_cast<unsigned long long>(block_sum_alt)
                );
            }
        }

        __syncthreads();

        // Convert inclusive â†’ exclusive and add block base
        uint64_t offset_main = block_base_main + prefix_main - n_main;
        uint64_t offset_alt = 0;
        if(rw_firings_counter_ptr_alternative != nullptr) {
            offset_alt = block_base_alt + prefix_alt - n_alt;
        }

        if(concatenated_index != std::numeric_limits<uint32_t>::max()) {
            rw_firings_buffer[offset_main] = NeuronShiftFiring{
                batch_index,  // batch_index
                1.0,  // payload
                concatenated_index,  // neuron_id
                j  // shift (timestep)
            };
            if(alt_index_1 != std::numeric_limits<uint32_t>::max()) {
                rw_firings_buffer_alternative[offset_alt++] = NeuronShiftFiring{
                    batch_index,  // batch_index
                    1.0,  // payload
                    alt_index_1,  // neuron_id
                    j  // shift (timestep)
                };
            }
            if(alt_index_2 != std::numeric_limits<uint32_t>::max()) {
                rw_firings_buffer_alternative[offset_alt] = NeuronShiftFiring{
                    batch_index,  // batch_index
                    1.0,  // payload (here we just need to remember that the gradient from this element will be used on a backward pass)
                    alt_index_2,  // neuron_id
                    j  // shift (timestep)
                };
            }
        }
    #endif
    }
}

densify_firing_stat_cpu_logic(
    EXTERNAL_REAL_DT* rw_firing_stat,
    NeuronShiftFiring* rw_firings_buffer,
    uint64_t* rw_firings_counter_ptr,
    NeuronShiftFiring* rw_firings_buffer_alternative,
    uint64_t* rw_firings_counter_ptr_alternative,
    uint32_t n_lookup_neurons,
    uint32_t sequence_length
) {
    uint32_t tid = threadIdx.x;
    uint32_t neuron_index = blockIdx.x * blockDim.x + tid;
    EXTERNAL_REAL_DT firing_stat_value = 0.0;
    uint32_t batch_index;
    uint32_t timestep;
    if(neuron_index < n_lookup_neurons) {
        batch_index = blockIdx.y / sequence_length;
        timestep = blockIdx.y % sequence_length;

        rw_firing_stat += batch_index * n_lookup_neurons * sequence_length;
        rw_firing_stat += timestep * n_lookup_neurons;
        firing_stat_value = rw_firing_stat[neuron_index];
        if(firing_stat_value != 0.0) {
            rw_firing_stat[neuron_index] = 0.0;
        }
    }

    if(firing_stat_value != 0.0) {
        if(firing_stat_value >= 1.0) {
            uint64_t firings_offset = *rw_firings_counter_ptr;
            (*rw_firings_counter_ptr)++;

            rw_firings_buffer[firings_offset] = NeuronShiftFiring{
                batch_index,  // batch_index
                std::floor(firing_stat_value),  // payload (whole number from firing_stat, we floor it to be able to handle rare situations when the same element is positive and alternative at the same time)
                neuron_index,  // neuron_id
                timestep  // shift (timestep)
            };
        } else if(rw_firings_counter_ptr_alternative != nullptr) {
            uint64_t firings_offset = *rw_firings_counter_ptr_alternative;
            (*rw_firings_counter_ptr_alternative)++;

            rw_firings_buffer_alternative[firings_offset] = NeuronShiftFiring{
                batch_index,  // batch_index
                1.0,  // payload (here we just need to remember that the gradient from this element will be used on a backward pass)
                neuron_index,  // neuron_id
                timestep  // shift (timestep)
            };
        }
    }
}

densify_firing_stat_cuda_logic(
    float4* rw_firing_stat,
    NeuronShiftFiring* rw_firings_buffer,
    uint64_t* rw_firings_counter_ptr,
    NeuronShiftFiring* rw_firings_buffer_alternative,
    uint64_t* rw_firings_counter_ptr_alternative,
    uint32_t n_lookup_neurons,
    uint32_t n_quads,
    uint32_t sequence_length
) {
    #ifdef ATOMIC
    #ifndef NO_CUDA
    int tpb    = blockDim.x;       // multiple of 32 and >= 64 is guaranteed
    int tid    = threadIdx.x;
    int lane   = tid & 31;
    int warpId = tid >> 5;
    int NWARPS = tpb >> 5;         // tpb / 32

    const unsigned FULL = 0xffffffffu;

    __shared__ uint32_t warp_sums_main[32];      // up to 1024 threads
    __shared__ uint32_t warp_sums_alt[32];       // for alternative firings
    __shared__ uint64_t block_base_main;
    __shared__ uint64_t block_base_alt;

    uint32_t batch_index = blockIdx.y / sequence_length;
    uint32_t timestep = blockIdx.y % sequence_length;
    uint64_t quad_idx = static_cast<uint64_t>(blockIdx.x) * tpb + tid;

    bool valid_quad = (quad_idx < n_quads);

    // Compute base pointer for this batch/timestep
    rw_firing_stat += batch_index * (n_lookup_neurons >> 2) * sequence_length;
    rw_firing_stat += timestep * (n_lookup_neurons >> 2);

    // Load quad of firing stat values (or zeros if out of bounds)
    __align__(16) float4 quad = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
    if(valid_quad) {
        quad = rw_firing_stat[quad_idx];
    }
    
    uint32_t mask_main = 0;
    uint32_t mask_alt = 0;
    uint32_t n_main = 0;
    uint32_t n_alt = 0;
    
    // Check each value and build masks (only if valid quad)
    if(valid_quad) {
        if(quad.x >= 1.0f) {
            mask_main |= 1;
            n_main++;
        } else if(quad.x > 0.0f && quad.x < 1.0f && rw_firings_counter_ptr_alternative != nullptr) {
            mask_alt |= 1;
            n_alt++;
        }
        
        if(quad.y >= 1.0f) {
            mask_main |= 2;
            n_main++;
        } else if(quad.y > 0.0f && quad.y < 1.0f && rw_firings_counter_ptr_alternative != nullptr) {
            mask_alt |= 2;
            n_alt++;
        }
        
        if(quad.z >= 1.0f) {
            mask_main |= 4;
            n_main++;
        } else if(quad.z > 0.0f && quad.z < 1.0f && rw_firings_counter_ptr_alternative != nullptr) {
            mask_alt |= 4;
            n_alt++;
        }
        
        if(quad.w >= 1.0f) {
            mask_main |= 8;
            n_main++;
        } else if(quad.w > 0.0f && quad.w < 1.0f && rw_firings_counter_ptr_alternative != nullptr) {
            mask_alt |= 8;
            n_alt++;
        }

        // Clear input if any value is non-zero
        if((mask_main | mask_alt) != 0) {
            void* p = &rw_firing_stat[quad_idx];
            asm volatile (
                "st.global.v4.f32 [%0], {%1, %2, %3, %4};\n"
                :
                : "l"(p), "f"(0.0f), "f"(0.0f), "f"(0.0f), "f"(0.0f)
                : "memory"
            );
        }
    }

    // --- warp inclusive scan on n_main ---
    uint32_t prefix_main = n_main;
    #pragma unroll
    for (int off = 1; off < 32; off <<= 1) {
        uint32_t x = __shfl_up_sync(FULL, prefix_main, off);
        if (lane >= off) prefix_main += x;
    }

    if (lane == 31)
        warp_sums_main[warpId] = prefix_main;

    // --- warp inclusive scan on n_alt ---
    uint32_t prefix_alt = n_alt;
    if(rw_firings_counter_ptr_alternative != nullptr) {
        #pragma unroll
        for (int off = 1; off < 32; off <<= 1) {
            uint32_t x = __shfl_up_sync(FULL, prefix_alt, off);
            if (lane >= off) prefix_alt += x;
        }

        if (lane == 31)
            warp_sums_alt[warpId] = prefix_alt;
    }

    __syncthreads();

    // --- warp 0 scans warp sums for main ---
    if (warpId == 0) {
        uint32_t wv = (lane < NWARPS) ? warp_sums_main[lane] : 0;

        #pragma unroll
        for (int off = 1; off < NWARPS; off <<= 1) {
            uint32_t x = __shfl_up_sync(FULL, wv, off);
            if (lane >= off) wv += x;
        }

        if (lane < NWARPS)
            warp_sums_main[lane] = wv;
    }

    // --- warp 1 scans warp sums for alt ---
    if(rw_firings_counter_ptr_alternative != nullptr && warpId == 1) {
        uint32_t wv = (lane < NWARPS) ? warp_sums_alt[lane] : 0;

        #pragma unroll
        for (int off = 1; off < NWARPS; off <<= 1) {
            uint32_t x = __shfl_up_sync(FULL, wv, off);
            if (lane >= off) wv += x;
        }

        if (lane < NWARPS)
            warp_sums_alt[lane] = wv;
    }

    __syncthreads();

    // Add warp prefix to thread prefix
    if (warpId > 0) {
        prefix_main += warp_sums_main[warpId - 1];
        if(rw_firings_counter_ptr_alternative != nullptr) {
            prefix_alt += warp_sums_alt[warpId - 1];
        }
    }

    // --- one atomic per block for main ---
    if (tid == 0) {
        uint32_t block_sum_main = warp_sums_main[NWARPS - 1];
        block_base_main = atomicAdd(
            reinterpret_cast<unsigned long long*>(rw_firings_counter_ptr),
            static_cast<unsigned long long>(block_sum_main)
        );
    }

    // --- one atomic per block for alt ---
    if(rw_firings_counter_ptr_alternative != nullptr) {
        if (tid == 0) {
            uint32_t block_sum_alt = warp_sums_alt[NWARPS - 1];
            block_base_alt = atomicAdd(
                reinterpret_cast<unsigned long long*>(rw_firings_counter_ptr_alternative),
                static_cast<unsigned long long>(block_sum_alt)
            );
        }
    }

    __syncthreads();

    // Convert inclusive â†’ exclusive and add block base
    uint64_t offset_main = block_base_main + prefix_main - n_main;
    uint64_t offset_alt = 0;
    if(rw_firings_counter_ptr_alternative != nullptr) {
        offset_alt = block_base_alt + prefix_alt - n_alt;
    }
    NeuronIndex_t base_neuron_idx = quad_idx << 2;

    // Write firings using masks (only if valid quad)
    if(valid_quad && n_main > 0) {
        if(mask_main & 1) {
            rw_firings_buffer[offset_main++] = NeuronShiftFiring{
                batch_index,
                std::floor(quad.x),
                base_neuron_idx,
                timestep
            };
        }
        if(mask_main & 2) {
            rw_firings_buffer[offset_main++] = NeuronShiftFiring{
                batch_index,
                std::floor(quad.y),
                base_neuron_idx + 1,
                timestep
            };
        }
        if(mask_main & 4) {
            rw_firings_buffer[offset_main++] = NeuronShiftFiring{
                batch_index,
                std::floor(quad.z),
                base_neuron_idx + 2,
                timestep
            };
        }
        if(mask_main & 8) {
            rw_firings_buffer[offset_main++] = NeuronShiftFiring{
                batch_index,
                std::floor(quad.w),
                base_neuron_idx + 3,
                timestep
            };
        }
    }

    if(valid_quad && n_alt > 0 && rw_firings_counter_ptr_alternative != nullptr) {
        if(mask_alt & 1) {
            rw_firings_buffer_alternative[offset_alt++] = NeuronShiftFiring{
                batch_index,
                1.0,
                base_neuron_idx,
                timestep
            };
        }
        if(mask_alt & 2) {
            rw_firings_buffer_alternative[offset_alt++] = NeuronShiftFiring{
                batch_index,
                1.0,
                base_neuron_idx + 1,
                timestep
            };
        }
        if(mask_alt & 4) {
            rw_firings_buffer_alternative[offset_alt++] = NeuronShiftFiring{
                batch_index,
                1.0,
                base_neuron_idx + 2,
                timestep
            };
        }
        if(mask_alt & 8) {
            rw_firings_buffer_alternative[offset_alt++] = NeuronShiftFiring{
                batch_index,
                1.0,
                base_neuron_idx + 3,
                timestep
            };
        }
    }
    #endif
    #endif
}

fill_outputs_by_sparse_firings_logic(
    EXTERNAL_REAL_DT* r_weights,
    NeuronDataId_t first_synapse_id,
    NeuronShiftFiring* r_firings_buffer,
    uint64_t* r_firings_counter_ptr,
    uint64_t max_firings,
    EXTERNAL_REAL_DT* w_output,
    uint32_t n_lookup_neurons,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t sequence_length,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    int32_t n_outputs_per_block,
    uint8_t* lut_data,
    double int_rescaler
) {
    uint64_t i = static_cast<uint64_t>(blockIdx.x) * blockDim.x + threadIdx.x;
    
    if(i >= max_firings) {
        return;
    }

    if(i >= *r_firings_counter_ptr) {
        return;
    }

    NeuronShiftFiring firing = r_firings_buffer[i];

    __SUPER_DETAILED_TRACE__(
        "[fill_outputs_by_sparse_firings] firing.batch_index %d, firing.shift %d, firing.neuron_id %d, blockIdx.y %d, n_outputs_per_block %d, sequence_length %d, n_outputs %d\n",
        firing.batch_index, firing.shift, firing.neuron_id, blockIdx.y, n_outputs_per_block, sequence_length, n_outputs
    );

    w_output += (firing.batch_index * sequence_length + firing.shift) * n_outputs;

    if(r_lookup_neuron_synapses_infos != nullptr) {
        NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + firing.neuron_id);
        if(blockIdx.y < lookup_neuron_synapses_info.n_groups) {
            NeuronDataId_t current_group_id = 0;
            if(lookup_neuron_synapses_info.n_synapse_metas == 1) {
                current_group_id = lookup_neuron_synapses_info.first_group_id;
                current_group_id += blockIdx.y * SizeOfForwardSynapseGroup(n_outputs_per_block, true);
            } else {
                DelayInfo* delays_info = DelayInfos(lookup_neuron_synapses_info.delays_info_id, lut_data);
                uint32_t current_group = 0;
                for(uint32_t j=0;j < lookup_neuron_synapses_info.n_synapse_metas;j++) {
                    DelayInfo delay_info = delays_info[j];
                    if(delay_info != 0) {
                        uint32_t n_groups = DELAY_INFO_N_GROUPS(delay_info);
                        if(current_group + n_groups <= blockIdx.y) {
                            current_group += n_groups;
                            continue;
                        }
                        current_group_id = lookup_neuron_synapses_info.first_group_id + DELAY_INFO_BYTE_SHIFT_FROM_FIRST_GROUP(delay_info);
                        current_group_id += (blockIdx.y - current_group) * SizeOfForwardSynapseGroup(n_outputs_per_block, true);
                        break;
                    }
                }
            }

            double multiplier = static_cast<double>(firing.payload);
            ATOMIC_PFX(process_single_synapse_group)(
                current_group_id,
                lut_data,
                first_synapse_id,
                r_weights,
                w_output,
                n_lookup_neurons,
                multiplier,
                int_rescaler
            );
        }
    } else { // fully connected case
        w_output += blockIdx.y * n_outputs_per_block;
        r_weights += static_cast<uint64_t>(firing.neuron_id) * n_outputs + blockIdx.y * n_outputs_per_block;
        if((blockIdx.y == n_output_blocks - 1) && (n_outputs % n_outputs_per_block)) {
            n_outputs_per_block = n_outputs % n_outputs_per_block;
        }
        float4 weights_quad = make_float4(0.0, 0.0, 0.0, 0.0);
        double weight;
        SUMMATION32_DT payload;

        __SUPER_DETAILED_TRACE__("[fill_outputs_by_sparse_firings] i %d, current_group_size %d, n_firings %d, weight_shift %ld\n", i, current_group_size, n_firings, weight_shift);

        int32_t n_w_buffered = 0;
        for(int32_t cursor=0;cursor < n_outputs_per_block;cursor++, r_weights++, w_output++) {
            if(((reinterpret_cast<uintptr_t>(r_weights) & 15) == 0) && (cursor < n_outputs_per_block - 3)) {
                #ifdef ATOMIC
                asm volatile(
                    "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                    : "=f"(weights_quad.x), "=f"(weights_quad.y), "=f"(weights_quad.z), "=f"(weights_quad.w)
                    : "l"(r_weights)
                );
                #else
                weights_quad = *reinterpret_cast<float4 *>(r_weights);
                #endif
                weight = static_cast<double>(weights_quad.x);
                n_w_buffered = 3;
            } else if(n_w_buffered > 0) {
                if(n_w_buffered == 3) {
                    weight = static_cast<double>(weights_quad.y);
                } else if(n_w_buffered == 2) {
                    weight = static_cast<double>(weights_quad.z);
                } else {
                    weight = static_cast<double>(weights_quad.w);
                }
                n_w_buffered--;
            } else {
                weight = static_cast<double>(*r_weights);
            }

            weight *= firing.payload;

            #ifdef INTEGERS_INSTEAD_OF_FLOATS
                payload = static_cast<SUMMATION32_DT>(weight * static_cast<double>(DENOMINATOR32) * int_rescaler);
                #ifdef ATOMIC
                atomicAdd(
                    reinterpret_cast<SUMMATION32_DT *>(w_output),
                    payload
                );
                #else
                *reinterpret_cast<SUMMATION32_DT *>(w_output) += payload;
                #endif
            #else
                payload = static_cast<SUMMATION32_DT>(weight);
                #ifdef ATOMIC
                atomicAdd(
                    w_output,
                    payload
                );
                #else
                __SUPER_DETAILED_TRACE__("[fill_outputs_by_sparse_firings] target_id %d, payload %f\n", target_id, payload);
                *w_output += payload;
                #endif
            #endif
        }
    }
}

// Kernel for gathering x gradients from sparse firings for sequence processing
// Grid: (n_sparse_firings, n_output_blocks)
// Each thread processes one firing
gather_x_gradients_for_sequence_logic(
    EXTERNAL_REAL_DT* r_weights,
    EXTERNAL_REAL_DT* r_output_gradients,
    NeuronShiftFiring* r_sparse_firings,
    GradientHashInfo* rw_before_detectors_gradients,
    uint32_t gradient_hash_width,
    uint64_t n_sparse_firings,
    uint32_t n_outputs,
    uint32_t n_detectors,
    uint32_t sequence_length,
    uint32_t n_output_blocks,
    uint32_t n_outputs_per_block,
    uint32_t n_lookup_neurons_per_detector,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    BaseSynapseMeta* r_synapse_metas,
    NeuronDataId_t first_synapse_id,
    bool is_alternative,
    uint8_t* lut_data,
    double first_synapse_meta_lr,
    double int_rescaler
) {
    uint64_t i = static_cast<uint64_t>(blockIdx.x) * blockDim.x + threadIdx.x;

    if(i >= n_sparse_firings) {
        return;
    }

    NeuronShiftFiring firing = r_sparse_firings[i];

    r_output_gradients += (firing.batch_index * sequence_length + firing.shift) * n_outputs;
    rw_before_detectors_gradients += firing.batch_index * gradient_hash_width;

    double grad_x = 0.0;
    double output_grad;
    double weight;
    double lr;

    if(r_lookup_neuron_synapses_infos != nullptr) {
        NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + firing.neuron_id);
        if(blockIdx.y < lookup_neuron_synapses_info.n_groups) {
            NeuronDataId_t current_group_id = 0;
            if(lookup_neuron_synapses_info.n_synapse_metas == 1) {
                current_group_id = lookup_neuron_synapses_info.first_group_id;
                current_group_id += blockIdx.y * SizeOfForwardSynapseGroup(n_outputs_per_block, true);
            } else {
                DelayInfo* delays_info = DelayInfos(lookup_neuron_synapses_info.delays_info_id, lut_data);
                uint32_t current_group = 0;
                for(uint32_t j=0;j < lookup_neuron_synapses_info.n_synapse_metas;j++) {
                    DelayInfo delay_info = delays_info[j];
                    if(delay_info != 0) {
                        uint32_t n_groups = DELAY_INFO_N_GROUPS(delay_info);
                        if(current_group + n_groups <= blockIdx.y) {
                            current_group += n_groups;
                            continue;
                        }
                        current_group_id = lookup_neuron_synapses_info.first_group_id + DELAY_INFO_BYTE_SHIFT_FROM_FIRST_GROUP(delay_info);
                        current_group_id += (blockIdx.y - current_group) * SizeOfForwardSynapseGroup(n_outputs_per_block, true);
                        break;
                    }
                }
            }

            ForwardSynapseGroup fws_group = *GetForwardSynapseGroup(current_group_id, lut_data);
            uint32_t sm_index = SynapseGroupSynapseMetaIndex(fws_group.meta_info);
            if(sm_index == 0) {
                lr = static_cast<double>(first_synapse_meta_lr);
            } else {
                lr = static_cast<double>((r_synapse_metas + sm_index)->lr);
            }

            uint32_t current_group_size = SynapseGroupSize(fws_group.meta_info);
            NeuronIndex_t* current_target_ptr = TargetNeuronsInForwardGroup(current_group_id, lut_data);
            long weight_shift = (reinterpret_cast<uint8_t *>(current_target_ptr) - (lut_data + first_synapse_id)) >> 2;
            EXTERNAL_REAL_DT* weight_ptr = r_weights + weight_shift;
            NeuronIndex_t target_id;
            uint32_t n_lookup_neurons = n_lookup_neurons_per_detector * n_detectors;
            for(uint32_t cursor = 0; cursor < current_group_size; cursor++) {
                target_id = current_target_ptr[cursor] - n_lookup_neurons;
                weight = static_cast<double>(weight_ptr[cursor]);
                output_grad = static_cast<double>(r_output_gradients[target_id]);
                grad_x += weight * output_grad;
            }
        }
    } else {
        lr = first_synapse_meta_lr;
        r_output_gradients += blockIdx.y * n_outputs_per_block;
        r_weights += static_cast<uint64_t>(firing.neuron_id) * n_outputs + blockIdx.y * n_outputs_per_block;
        if((blockIdx.y == n_output_blocks - 1) && (n_outputs % n_outputs_per_block)) {
            n_outputs_per_block = n_outputs % n_outputs_per_block;
        }
        for(uint32_t cursor = 0; cursor < n_outputs_per_block; cursor++) {
            weight = static_cast<double>(r_weights[cursor]);
            output_grad = static_cast<double>(r_output_gradients[cursor]);
            grad_x += weight * output_grad;
        }
    }

    if(fabs(grad_x) > EPS) {
        uint64_t hash = (static_cast<uint64_t>(firing.neuron_id) + static_cast<uint64_t>(firing.shift)) % gradient_hash_width;
        // Construct hash_key to match little-endian memory layout when read as uint64_t:
        // Memory: [neuron_id (4 bytes)][timestep (4 bytes)]
        // As uint64_t: lower 32 bits = neuron_id, upper 32 bits = timestep
        uint64_t hash_key = static_cast<uint64_t>(firing.neuron_id + 1) | (static_cast<uint64_t>(firing.shift) << 32);
        int32_t firing_id = is_alternative ? -static_cast<int32_t>(i + 1) : (i + 1);

        SUMMATION32_DT sum32_delta;
        #ifdef INTEGERS_INSTEAD_OF_FLOATS
            sum32_delta = static_cast<SUMMATION32_DT>(grad_x * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
        #else
            sum32_delta = static_cast<SUMMATION32_DT>(grad_x * lr);
        #endif
        
        #ifdef ATOMIC
        // Use the first 8 bytes (neuron_id + timestep) as a 64-bit key for atomic comparison
        uint64_t current_key;
        int32_t current_firing_id;
        while(true) {
            // Read the key atomically (first 8 bytes of GradientHashInfo)
            current_key = atomicAdd(reinterpret_cast<unsigned long long*>(rw_before_detectors_gradients + hash), 0);
            if(current_key == hash_key) {
                // Key matches, check firing_id filter
                current_firing_id = atomicAdd(&((rw_before_detectors_gradients + hash)->firing_id), 0);
                if(current_firing_id == firing_id) {
                    // Found matching entry with correct firing_id, update gradient value
                    atomicAdd(&((rw_before_detectors_gradients + hash)->gradient_value), sum32_delta);
                }
                // Key matches but firing_id doesn't match, just skip (don't update, don't probe)
                break;
            }
            if(current_key == 0) {
                // Empty slot, try to claim it using atomicCAS on the 64-bit key
                if(atomicCAS(reinterpret_cast<unsigned long long*>(rw_before_detectors_gradients + hash), 0, hash_key) == 0) {
                    // Successfully claimed the slot
                    current_firing_id = atomicAdd(&((rw_before_detectors_gradients + hash)->firing_id), 0);
                    if(current_firing_id == 0) {
                        atomicAdd(&((rw_before_detectors_gradients + hash)->gradient_value), sum32_delta);
                    }
                    break;
                }
                // Another thread claimed it, check again
                continue;
            }
            // Collision, linear probing
            hash++;
            if(hash == gradient_hash_width) {
                hash = 0;
            }
        }
        #else
        uint64_t current_key;
        while(true) {
            // Read the key (first 8 bytes: neuron_id + timestep)
            current_key = *reinterpret_cast<uint64_t*>(rw_before_detectors_gradients + hash);
            if(current_key == hash_key) {
                // Key matches, check firing_id filter
                if((rw_before_detectors_gradients + hash)->firing_id == firing_id) {
                    // Found matching entry with correct firing_id, update gradient value
                    (rw_before_detectors_gradients + hash)->gradient_value += sum32_delta;
                }
                // Key matches but firing_id doesn't match, just skip (don't update, don't probe)
                break;
            }
            if(current_key == 0) {
                // Empty slot, claim it
                (rw_before_detectors_gradients + hash)->neuron_id = firing.neuron_id + 1;
                (rw_before_detectors_gradients + hash)->timestep = firing.shift;
                (rw_before_detectors_gradients + hash)->firing_id = firing_id;
                (rw_before_detectors_gradients + hash)->gradient_value = sum32_delta;
                break;
            }
            // Collision, linear probing
            hash++;
            if(hash == gradient_hash_width) {
                hash = 0;
            }
        }
        #endif
    }
}

// Kernel for gathering weight gradients from sparse firings for sequence processing
// Grid: (n_sparse_firings, n_output_blocks)
// Each thread processes one firing
gather_w_gradients_for_sequence_old_logic(
    EXTERNAL_REAL_DT* r_output_gradients,
    NeuronShiftFiring* r_sparse_firings,
    EXTERNAL_REAL_DT* w_weights_gradients,
    uint64_t n_sparse_firings,
    uint32_t n_outputs,
    uint32_t n_detectors,
    uint32_t sequence_length,
    uint32_t n_output_blocks,
    uint32_t n_outputs_per_block,
    uint32_t n_lookup_neurons_per_detector,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    BaseSynapseMeta* r_synapse_metas,
    NeuronDataId_t first_synapse_id,
    uint8_t* lut_data,
    double external_lr,
    double first_synapse_meta_lr,
    double int_rescaler
) {
    uint64_t i = static_cast<uint64_t>(blockIdx.x) * blockDim.x + threadIdx.x;

    if(i >= n_sparse_firings) {
        return;
    }

    NeuronShiftFiring firing = r_sparse_firings[i];
    r_output_gradients += (firing.batch_index * sequence_length + firing.shift) * n_outputs;
    double output_grad;
    SUMMATION32_DT sum32_delta;
    double lr;

    if(r_lookup_neuron_synapses_infos != nullptr) {
        NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + firing.neuron_id);
        if(blockIdx.y < lookup_neuron_synapses_info.n_groups) {
            NeuronDataId_t current_group_id = 0;
            if(lookup_neuron_synapses_info.n_synapse_metas == 1) {
                current_group_id = lookup_neuron_synapses_info.first_group_id;
                current_group_id += blockIdx.y * SizeOfForwardSynapseGroup(n_outputs_per_block, true);
            } else {
                DelayInfo* delays_info = DelayInfos(lookup_neuron_synapses_info.delays_info_id, lut_data);
                uint32_t current_group = 0;
                for(uint32_t j=0;j < lookup_neuron_synapses_info.n_synapse_metas;j++) {
                    DelayInfo delay_info = delays_info[j];
                    if(delay_info != 0) {
                        uint32_t n_groups = DELAY_INFO_N_GROUPS(delay_info);
                        if(current_group + n_groups <= blockIdx.y) {
                            current_group += n_groups;
                            continue;
                        }
                        current_group_id = lookup_neuron_synapses_info.first_group_id + DELAY_INFO_BYTE_SHIFT_FROM_FIRST_GROUP(delay_info);
                        current_group_id += (blockIdx.y - current_group) * SizeOfForwardSynapseGroup(n_outputs_per_block, true);
                        break;
                    }
                }
            }

            ForwardSynapseGroup fws_group = *GetForwardSynapseGroup(current_group_id, lut_data);
            uint32_t sm_index = SynapseGroupSynapseMetaIndex(fws_group.meta_info);
            if(sm_index == 0) {
                lr = first_synapse_meta_lr;
            } else {
                lr = static_cast<double>((r_synapse_metas + sm_index)->lr);
            }
            if(external_lr >= 0) {
                lr *= -external_lr;
            }

            uint32_t current_group_size = SynapseGroupSize(fws_group.meta_info);
            NeuronIndex_t* current_target_ptr = TargetNeuronsInForwardGroup(current_group_id, lut_data);
            long weight_shift = (reinterpret_cast<uint8_t *>(current_target_ptr) - (lut_data + first_synapse_id)) >> 2;
            NeuronIndex_t target_id;
            uint32_t n_lookup_neurons = n_lookup_neurons_per_detector * n_detectors;
            for(uint32_t cursor = 0; cursor < current_group_size; cursor++, weight_shift++) {
                target_id = current_target_ptr[cursor] - n_lookup_neurons;
                output_grad = static_cast<double>(r_output_gradients[target_id]);

                if(fabs(output_grad) > EPS) {
                    #ifdef INTEGERS_INSTEAD_OF_FLOATS
                        sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
                        #ifdef ATOMIC
                        atomicAdd(
                            reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients) + weight_shift,
                            sum32_delta
                        );
                        #else
                        *reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients + weight_shift) += sum32_delta;
                        #endif
                    #else
                        sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr);
                        #ifdef ATOMIC
                        atomicAdd(
                            w_weights_gradients + weight_shift,
                            sum32_delta
                        );
                        #else
                        w_weights_gradients[weight_shift] += sum32_delta;
                        #endif
                    #endif
                }
            }
        }
    } else {
        r_output_gradients += blockIdx.y * n_outputs_per_block;
        uint64_t weight_grad_offset = static_cast<uint64_t>(firing.neuron_id) * n_outputs + blockIdx.y * n_outputs_per_block;
        w_weights_gradients += weight_grad_offset;

        if((blockIdx.y == n_output_blocks - 1) && (n_outputs % n_outputs_per_block)) {
            n_outputs_per_block = n_outputs % n_outputs_per_block;
        }

        lr = first_synapse_meta_lr;
        if(external_lr >= 0) {
            lr *= -external_lr;
        }

        // Compute weight gradients: weight_grad = output_grad * lr for each output in this block
        for(uint32_t cursor = 0; cursor < n_outputs_per_block; cursor++) {
            output_grad = static_cast<double>(r_output_gradients[cursor]);
            output_grad *= firing.payload;

            if(fabs(output_grad) > EPS) {
                #ifdef INTEGERS_INSTEAD_OF_FLOATS
                    sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
                    #ifdef ATOMIC
                    atomicAdd(
                        reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients) + cursor,
                        sum32_delta
                    );
                    #else
                    *reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients + cursor) += sum32_delta;
                    #endif
                #else
                    sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr);
                    #ifdef ATOMIC
                    atomicAdd(
                        w_weights_gradients + cursor,
                        sum32_delta
                    );
                    #else
                    w_weights_gradients[cursor] += sum32_delta;
                    #endif
                #endif
            }
        }
    }
}

// Helper function to lookup gradient value from hash table
lookup_gradient_from_hash(
    GradientHashInfo* hash_table,
    uint32_t gradient_hash_width,
    NeuronIndex_t neuron_id,
    uint32_t timestep,
    SUMMATION32_DT grad_value
) {
    // Construct hash_key to match little-endian memory layout when read as uint64_t:
    // Memory: [neuron_id (4 bytes)][timestep (4 bytes)]
    // As uint64_t: lower 32 bits = neuron_id, upper 32 bits = timestep
    uint64_t hash_key = static_cast<uint64_t>(neuron_id + 1) | (static_cast<uint64_t>(timestep) << 32);
    uint64_t hash = (static_cast<uint64_t>(neuron_id) + static_cast<uint64_t>(timestep)) % gradient_hash_width;
    uint64_t start_hash = hash;

    while(true) {
        uint64_t current_key = *reinterpret_cast<uint64_t*>(hash_table + hash);
        if(current_key == hash_key) {
            // Found matching entry, return gradient value
            grad_value = (hash_table + hash)->gradient_value;
            break;
        }
        if(current_key == 0) {
            // Empty slot, entry not found
            break;
        }
        // Collision, linear probing
        hash++;
        if(hash == gradient_hash_width) {
            hash = 0;
        }
        if(hash == start_hash) {
            // Wrapped around, entry not found
            break;
        }
    }
}

find_forward_synapse_group(
    NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info,
    uint32_t output_block_index,
    uint32_t n_outputs_per_block,
    uint8_t* lut_data,
    NeuronDataId_t current_group_id
) {
    if(lookup_neuron_synapses_info.n_synapse_metas == 1) {
        current_group_id = lookup_neuron_synapses_info.first_group_id;
        current_group_id += output_block_index * SizeOfForwardSynapseGroup(n_outputs_per_block, true);
    } else {
        DelayInfo* delays_info = DelayInfos(lookup_neuron_synapses_info.delays_info_id, lut_data);
        uint32_t current_group = 0;
        for(uint32_t j=0;j < lookup_neuron_synapses_info.n_synapse_metas;j++) {
            DelayInfo delay_info = delays_info[j];
            if(delay_info != 0) {
                uint32_t n_groups = DELAY_INFO_N_GROUPS(delay_info);
                if(current_group + n_groups <= output_block_index) {
                    current_group += n_groups;
                    continue;
                }
                current_group_id = lookup_neuron_synapses_info.first_group_id + DELAY_INFO_BYTE_SHIFT_FROM_FIRST_GROUP(delay_info);
                current_group_id += (output_block_index - current_group) * SizeOfForwardSynapseGroup(n_outputs_per_block, true);
                break;
            }
        }
    }
}

accumulate_grad_sum_sparse(
    NeuronDataId_t current_group_id,
    uint8_t* lut_data,
    NeuronDataId_t first_synapse_id,
    EXTERNAL_REAL_DT* r_output_gradients,
    EXTERNAL_REAL_DT* r_weights,
    uint32_t n_lookup_neurons,
    uint32_t n_outputs,
    double grad_sum
) {
    ForwardSynapseGroup fws_group = *GetForwardSynapseGroup(current_group_id, lut_data);
    int32_t n_outputs_in_current_block = SynapseGroupSize(fws_group.meta_info);
    NeuronIndex_t* current_target_ptr = TargetNeuronsInForwardGroup(current_group_id, lut_data);
    long weight_shift = (reinterpret_cast<uint8_t *>(current_target_ptr) - (lut_data + first_synapse_id)) >> 2;
    double weight;
    double output_grad;

    #ifdef ATOMIC
    uint4 targets_quad = make_uint4(0, 0, 0, 0);
    float4 weights_quad = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
    int32_t n_t_buffered = 0;
    int32_t n_w_buffered = 0;
    NeuronIndex_t target_id;
    EXTERNAL_REAL_DT* weight_ptr = r_weights + weight_shift;
    
    for(int32_t cursor = 0; cursor < n_outputs_in_current_block; cursor++, current_target_ptr++, weight_ptr++) {
        if(((reinterpret_cast<uintptr_t>(current_target_ptr) & 15) == 0) && (cursor < n_outputs_in_current_block - 3)) {
            asm volatile(
                "ld.global.v4.s32 {%0,%1,%2,%3}, [%4];"
                : "=r"(targets_quad.x), "=r"(targets_quad.y), "=r"(targets_quad.z), "=r"(targets_quad.w)
                : "l"(current_target_ptr)
            );
            target_id = targets_quad.x;
            n_t_buffered = 3;
        } else if(n_t_buffered > 0) {
            if(n_t_buffered == 3) {
                target_id = targets_quad.y;
            } else if(n_t_buffered == 2) {
                target_id = targets_quad.z;
            } else {
                target_id = targets_quad.w;
            }
            n_t_buffered--;
        } else {
            target_id = *current_target_ptr;
        }
        target_id -= n_lookup_neurons;
        
        // Vectorized read of weights
        if(((reinterpret_cast<uintptr_t>(weight_ptr) & 15) == 0) && (cursor < n_outputs_in_current_block - 3)) {
            asm volatile(
                "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                : "=f"(weights_quad.x), "=f"(weights_quad.y), "=f"(weights_quad.z), "=f"(weights_quad.w)
                : "l"(weight_ptr)
            );
            weight = static_cast<double>(weights_quad.x);
            n_w_buffered = 3;
        } else if(n_w_buffered > 0) {
            if(n_w_buffered == 3) {
                weight = static_cast<double>(weights_quad.y);
            } else if(n_w_buffered == 2) {
                weight = static_cast<double>(weights_quad.z);
            } else {
                weight = static_cast<double>(weights_quad.w);
            }
            n_w_buffered--;
        } else {
            weight = static_cast<double>(*weight_ptr);
        }
        output_grad = static_cast<double>(r_output_gradients[target_id]);
        grad_sum += weight * output_grad;
    }
    #else
    for(int32_t cursor = 0; cursor < n_outputs_in_current_block; cursor++, weight_shift++) {
        NeuronIndex_t target_id = current_target_ptr[cursor] - n_lookup_neurons;
        output_grad = static_cast<double>(r_output_gradients[target_id]);
        weight = static_cast<double>(r_weights[weight_shift]);
        grad_sum += weight * output_grad;
    }
    #endif
}

// ======================= SEQUENCE, BACKWARD =============================== //

propagate_through_detectors_for_sequence_sparse_logic(
    EXTERNAL_REAL_DT* r_output_gradients,
    EXTERNAL_REAL_DT* r_weights,
    int32_t* r_lookup_indices,
    EXTERNAL_REAL_DT* r_min_anchor_deltas,
    int32_t* r_min_anchor_delta_indices,
    int32_t* r_positional_lookup_indices,
    EXTERNAL_REAL_DT* r_positional_min_deltas,
    int32_t* r_positional_min_delta_indices,
    uint32_t n_detectors,
    uint32_t n_total_tiles,
    uint32_t sequence_length,
    uint32_t n_anchors_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t n_outputs_per_block,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    BaseSynapseMeta* r_synapse_metas,
    NeuronDataId_t first_synapse_id,
    uint8_t* lut_data,
    AnchorsPair* r_detectors,
    uint32_t n_lookup_neurons_per_detector,
    EXTERNAL_REAL_DT* w_input_gradients,
    EXTERNAL_REAL_DT* w_positional_embeddings_gradients,
    uint32_t n_inputs,
    uint32_t positional_dim,
    double int_rescaler
) {
    // blockIdx.x encodes: tile_id * n_detectors + detector_index
    // Extract tile_id and detector_index
    uint32_t tile_id = blockIdx.x / n_detectors;
    uint32_t detector_index = blockIdx.x % n_detectors;
    uint32_t tile_grid_w = (sequence_length + TILE - 1) / TILE;

    if(tile_id >= n_total_tiles) {
        return;
    }

    uint32_t i = tile_id / tile_grid_w;  // tile row
    uint32_t j = tile_id % tile_grid_w;  // tile col

    // Skip tiles below diagonal (bi > bj means all pairs have i > j, which we don't want)
    if(i > j) {
        return;
    }

    if(i == j) {
        i = i * TILE + (threadIdx.x / TILE);
        j = j * TILE + (threadIdx.x % TILE);
        if(i >= j) {
            return;
        }
    } else {
        i = i * TILE + (threadIdx.x / TILE);
        j = j * TILE + (threadIdx.x % TILE);
    }

    if(i >= sequence_length || j >= sequence_length) {
        return;
    }

    // blockIdx.y encodes: batch_index * n_output_blocks + output_block_index
    uint32_t batch_index = blockIdx.y / n_output_blocks;
    uint32_t output_block_index = blockIdx.y % n_output_blocks;
    r_output_gradients += (batch_index * sequence_length + j) * n_outputs;

    // Calculate offsets into lookup_indices arrays
    // Layout: [batch0_t0_det0, batch0_t0_det1, ..., batch0_t0_detN, batch0_t1_det0, ...]
    uint32_t n_detector_items = sequence_length * n_detectors;
    uint32_t batch_offset = batch_index * n_detector_items;

    // Get K: lookup index from first timestep (i)
    uint32_t k_offset = batch_offset + i * n_detectors + detector_index;
    int32_t K = r_lookup_indices[k_offset];
    int32_t k_min_anchor_delta_index = r_min_anchor_delta_indices[k_offset];
    EXTERNAL_REAL_DT k_min_anchor_delta = r_min_anchor_deltas[k_offset];

    // Get Q: lookup index from second timestep (j)
    uint32_t q_offset = batch_offset + j * n_detectors + detector_index;
    int32_t Q = r_lookup_indices[q_offset];
    int32_t q_min_anchor_delta_index = r_min_anchor_delta_indices[q_offset];
    EXTERNAL_REAL_DT q_min_anchor_delta = r_min_anchor_deltas[q_offset];

    // Get PE: positional lookup index from timestep difference (j - i - 1)
    uint32_t pe_offset = (j - i - 1) * n_detectors + detector_index;
    int32_t PE = r_positional_lookup_indices[pe_offset];
    int32_t pe_min_delta_index = r_positional_min_delta_indices[pe_offset];
    EXTERNAL_REAL_DT pe_min_delta = r_positional_min_deltas[pe_offset];
    w_input_gradients += batch_index * n_inputs * sequence_length;

    // Calculate concatenated index
    uint32_t concatenated_index = (
        (Q << (n_anchors_per_detector + positional_dim)) |
        (K << positional_dim) |
        PE
    ) + detector_index * n_lookup_neurons_per_detector;

    double du;
    uint32_t concatenated_index_alt;
    uint32_t flipping_anchor_index;
    uint32_t i_or_j;
    double main_grad_sum;
    double alt_grad_sum;
    double output_grad;
    #ifdef ATOMIC
    double weight;
    #endif
    #ifdef INTEGERS_INSTEAD_OF_FLOATS
    SUMMATION32_DT sum32_delta;
    #endif

    if(fabs(q_min_anchor_delta) < fabs(k_min_anchor_delta)) {
        concatenated_index_alt = (
            ((Q ^ (1 << q_min_anchor_delta_index)) << (n_anchors_per_detector + positional_dim)) |
            (K << positional_dim) |
            PE
        ) + detector_index * n_lookup_neurons_per_detector;

        du = static_cast<double>(q_min_anchor_delta);
        flipping_anchor_index = q_min_anchor_delta_index;
        i_or_j = j;
    } else {
        concatenated_index_alt = (
            (Q << (n_anchors_per_detector + positional_dim)) |
            ((K ^ (1 << k_min_anchor_delta_index)) << positional_dim) |
            PE
        ) + detector_index * n_lookup_neurons_per_detector;

        du = static_cast<double>(k_min_anchor_delta);
        flipping_anchor_index = k_min_anchor_delta_index;
        i_or_j = i;
    }

    if(du > 0) {
        du = 1.0 / (1.0 + fabs(du));
        du *= 0.5 * du;
    } else {
        du = 1.0 / (1.0 + fabs(du));
        du *= -0.5 * du;
    }
    uint32_t n_lookup_neurons = n_lookup_neurons_per_detector * n_detectors;

    main_grad_sum = 0.0;
    alt_grad_sum = 0.0;
    NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info;
    
    // Sparse connectivity: gather output gradients using ForwardSynapseGroups
    lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + concatenated_index);
    NeuronDataId_t current_group_id;

    if(output_block_index < lookup_neuron_synapses_info.n_groups) {
        PFX(find_forward_synapse_group)(
            lookup_neuron_synapses_info,
            output_block_index,
            n_outputs_per_block,
            lut_data,
            current_group_id
        );

        ATOMIC_PFX(accumulate_grad_sum_sparse)(
            current_group_id,
            lut_data,
            first_synapse_id,
            r_output_gradients,
            r_weights,
            n_lookup_neurons,
            n_outputs,
            main_grad_sum
        );
    }

    lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + concatenated_index_alt);

    // Process alt concatenated_index_alt
    if(output_block_index < lookup_neuron_synapses_info.n_groups) {
        PFX(find_forward_synapse_group)(
            lookup_neuron_synapses_info,
            output_block_index,
            n_outputs_per_block,
            lut_data,
            current_group_id
        );

        ATOMIC_PFX(accumulate_grad_sum_sparse)(
            current_group_id,
            lut_data,
            first_synapse_id,
            r_output_gradients,
            r_weights,
            n_lookup_neurons,
            n_outputs,
            alt_grad_sum
        );
    }
    du *= main_grad_sum - alt_grad_sum;

    if(fabs(du) > EPS) {
        AnchorsPair ap = r_detectors[detector_index * n_anchors_per_detector + flipping_anchor_index];
        EXTERNAL_REAL_DT* w_input_grad = w_input_gradients + i_or_j * n_inputs;

        #ifdef INTEGERS_INSTEAD_OF_FLOATS
            sum32_delta = static_cast<SUMMATION32_DT>(du * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(
                reinterpret_cast<SUMMATION32_DT *>(w_input_grad) + ap.anchor1_id,
                sum32_delta
            );
            atomicAdd(
                reinterpret_cast<SUMMATION32_DT *>(w_input_grad) + ap.anchor2_id,
                -sum32_delta
            );
            #else
            *reinterpret_cast<SUMMATION32_DT *>(w_input_grad + ap.anchor1_id) += sum32_delta;
            *reinterpret_cast<SUMMATION32_DT *>(w_input_grad + ap.anchor2_id) -= sum32_delta;
            #endif
        #else
            #ifdef ATOMIC
            atomicAdd(
                w_input_grad + ap.anchor1_id,
                du
            );
            atomicAdd(
                w_input_grad + ap.anchor2_id,
                -du
            );
            #else
            w_input_grad[ap.anchor1_id] += du;
            w_input_grad[ap.anchor2_id] -= du;
            #endif
        #endif
    }

    if((fabs(pe_min_delta) < fabs(q_min_anchor_delta)) && (fabs(pe_min_delta) < fabs(k_min_anchor_delta))) {
        concatenated_index_alt = (
            (Q << (n_anchors_per_detector + positional_dim)) |
            (K << positional_dim) |
            (PE ^ (1 << r_positional_min_delta_indices[pe_offset]))
        ) + detector_index * n_lookup_neurons_per_detector;

        du = static_cast<double>(pe_min_delta);
        if(du > 0) {
            du = 1.0 / (1.0 + fabs(du));
            du *= 0.5 * du;
        } else {
            du = 1.0 / (1.0 + fabs(du));
            du *= -0.5 * du;
        }
        alt_grad_sum = 0.0;
        // Sparse connectivity: gather output gradients using ForwardSynapseGroups
        lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + concatenated_index_alt);

        if(output_block_index < lookup_neuron_synapses_info.n_groups) {
            PFX(find_forward_synapse_group)(
                lookup_neuron_synapses_info,
                output_block_index,
                n_outputs_per_block,
                lut_data,
                current_group_id
            );

            ATOMIC_PFX(accumulate_grad_sum_sparse)(
                current_group_id,
                lut_data,
                first_synapse_id,
                r_output_gradients,
                r_weights,
                n_lookup_neurons,
                n_outputs,
                alt_grad_sum
            );
        }
        du *= main_grad_sum - alt_grad_sum;
        w_positional_embeddings_gradients += pe_offset * positional_dim;
        #ifdef INTEGERS_INSTEAD_OF_FLOATS
            sum32_delta = static_cast<SUMMATION32_DT>(du * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(
                reinterpret_cast<SUMMATION32_DT *>(w_positional_embeddings_gradients) + pe_min_delta_index,
                sum32_delta
            );
            #else
            *reinterpret_cast<SUMMATION32_DT *>(w_positional_embeddings_gradients + pe_min_delta_index) += sum32_delta;
            #endif
        #else
            #ifdef ATOMIC
            atomicAdd(
                w_positional_embeddings_gradients + pe_min_delta_index,
                du
            );
            #else
            w_positional_embeddings_gradients[pe_min_delta_index] += du;
            #endif
        #endif
    }
}

propagate_through_detectors_for_sequence_fc_logic(
    EXTERNAL_REAL_DT* r_output_gradients,
    EXTERNAL_REAL_DT* r_weights,
    int32_t* r_lookup_indices,
    EXTERNAL_REAL_DT* r_min_anchor_deltas,
    int32_t* r_min_anchor_delta_indices,
    int32_t* r_positional_lookup_indices,
    EXTERNAL_REAL_DT* r_positional_min_deltas,
    int32_t* r_positional_min_delta_indices,
    uint32_t n_detectors,
    uint32_t n_total_tiles,
    uint32_t sequence_length,
    uint32_t n_anchors_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t n_outputs_per_block,
    AnchorsPair* r_detectors,
    uint32_t n_lookup_neurons_per_detector,
    EXTERNAL_REAL_DT* w_input_gradients,
    EXTERNAL_REAL_DT* w_positional_embeddings_gradients,
    uint32_t n_inputs,
    uint32_t positional_dim,
    double int_rescaler
) {
    // blockIdx.x encodes: tile_id * n_detectors + detector_index
    // Extract tile_id and detector_index
    uint32_t tile_id = blockIdx.x / n_detectors;
    uint32_t detector_index = blockIdx.x % n_detectors;
    uint32_t tile_grid_w = (sequence_length + TILE - 1) / TILE;

    if(tile_id >= n_total_tiles) {
        return;
    }

    uint32_t i = tile_id / tile_grid_w;  // tile row
    uint32_t j = tile_id % tile_grid_w;  // tile col

    // Skip tiles below diagonal (bi > bj means all pairs have i > j, which we don't want)
    if(i > j) {
        return;
    }

    if(i == j) {
        i = i * TILE + (threadIdx.x / TILE);
        j = j * TILE + (threadIdx.x % TILE);
        if(i >= j) {
            return;
        }
    } else {
        i = i * TILE + (threadIdx.x / TILE);
        j = j * TILE + (threadIdx.x % TILE);
    }

    if(i >= sequence_length || j >= sequence_length) {
        return;
    }

    // blockIdx.y encodes: batch_index * n_output_blocks + output_block_index
    uint32_t batch_index = blockIdx.y / n_output_blocks;
    uint32_t output_block_index = blockIdx.y % n_output_blocks;
    r_output_gradients += (batch_index * sequence_length + j) * n_outputs;

    int32_t n_outputs_in_current_block;
    r_weights += output_block_index * n_outputs_per_block;
    r_output_gradients += output_block_index * n_outputs_per_block;
    if((output_block_index == n_output_blocks - 1) && (n_outputs % n_outputs_per_block)) {
        n_outputs_in_current_block = n_outputs % n_outputs_per_block;
    } else {
        n_outputs_in_current_block = n_outputs_per_block;
    }

    // Calculate offsets into lookup_indices arrays
    // Layout: [batch0_t0_det0, batch0_t0_det1, ..., batch0_t0_detN, batch0_t1_det0, ...]
    uint32_t n_detector_items = sequence_length * n_detectors;
    uint32_t batch_offset = batch_index * n_detector_items;

    // Get K: lookup index from first timestep (i)
    uint32_t k_offset = batch_offset + i * n_detectors + detector_index;
    int32_t K = r_lookup_indices[k_offset];
    int32_t k_min_anchor_delta_index = r_min_anchor_delta_indices[k_offset];
    EXTERNAL_REAL_DT k_min_anchor_delta = r_min_anchor_deltas[k_offset];

    // Get Q: lookup index from second timestep (j)
    uint32_t q_offset = batch_offset + j * n_detectors + detector_index;
    int32_t Q = r_lookup_indices[q_offset];
    int32_t q_min_anchor_delta_index = r_min_anchor_delta_indices[q_offset];
    EXTERNAL_REAL_DT q_min_anchor_delta = r_min_anchor_deltas[q_offset];

    // Get PE: positional lookup index from timestep difference (j - i - 1)
    uint32_t pe_offset = (j - i - 1) * n_detectors + detector_index;
    int32_t PE = r_positional_lookup_indices[pe_offset];
    int32_t pe_min_delta_index = r_positional_min_delta_indices[pe_offset];
    EXTERNAL_REAL_DT pe_min_delta = r_positional_min_deltas[pe_offset];
    w_input_gradients += batch_index * n_inputs * sequence_length;

    // Calculate concatenated index
    uint32_t concatenated_index = (
        (Q << (n_anchors_per_detector + positional_dim)) |
        (K << positional_dim) |
        PE
    ) + detector_index * n_lookup_neurons_per_detector;

    double du;
    uint32_t concatenated_index_alt;
    uint32_t flipping_anchor_index;
    uint32_t i_or_j;
    double main_grad_sum;
    double alt_grad_sum;
    double output_grad;
    #ifdef ATOMIC
    double weight;
    #endif
    #ifdef INTEGERS_INSTEAD_OF_FLOATS
    SUMMATION32_DT sum32_delta;
    #endif

    if(fabs(q_min_anchor_delta) < fabs(k_min_anchor_delta)) {
        concatenated_index_alt = (
            ((Q ^ (1 << q_min_anchor_delta_index)) << (n_anchors_per_detector + positional_dim)) |
            (K << positional_dim) |
            PE
        ) + detector_index * n_lookup_neurons_per_detector;

        du = static_cast<double>(q_min_anchor_delta);
        flipping_anchor_index = q_min_anchor_delta_index;
        i_or_j = j;
    } else {
        concatenated_index_alt = (
            (Q << (n_anchors_per_detector + positional_dim)) |
            ((K ^ (1 << k_min_anchor_delta_index)) << positional_dim) |
            PE
        ) + detector_index * n_lookup_neurons_per_detector;

        du = static_cast<double>(k_min_anchor_delta);
        flipping_anchor_index = k_min_anchor_delta_index;
        i_or_j = i;
    }

    if(du > 0) {
        du = 1.0 / (1.0 + fabs(du));
        du *= 0.5 * du;
    } else {
        du = 1.0 / (1.0 + fabs(du));
        du *= -0.5 * du;
    }

    main_grad_sum = 0.0;
    alt_grad_sum = 0.0;
    
    // Fully connected: direct indexing
    uint64_t weights_main_shift = static_cast<uint64_t>(concatenated_index) * n_outputs;
    uint64_t weights_alt_shift = static_cast<uint64_t>(concatenated_index_alt) * n_outputs;

    #ifdef ATOMIC
    float4 output_grad_quad = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
    float4 main_weights_quad = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
    float4 alt_weights_quad = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
    int32_t n_output_grad_buffered = 0;
    int32_t n_main_w_buffered = 0;
    int32_t n_alt_w_buffered = 0;
    EXTERNAL_REAL_DT* r_output_grad_ptr = r_output_gradients;
    EXTERNAL_REAL_DT* r_main_weights_ptr = r_weights + weights_main_shift;
    EXTERNAL_REAL_DT* r_alt_weights_ptr = r_weights + weights_alt_shift;

    for(int32_t cursor = 0; cursor < n_outputs_in_current_block; cursor++) {
        // Vectorized read of output gradients
        if(((reinterpret_cast<uintptr_t>(r_output_grad_ptr) & 15) == 0) && (cursor < n_outputs_in_current_block - 3)) {
            asm volatile(
                "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                : "=f"(output_grad_quad.x), "=f"(output_grad_quad.y), "=f"(output_grad_quad.z), "=f"(output_grad_quad.w)
                : "l"(r_output_grad_ptr)
            );
            output_grad = static_cast<double>(output_grad_quad.x);
            n_output_grad_buffered = 3;
        } else if(n_output_grad_buffered > 0) {
            if(n_output_grad_buffered == 3) {
                output_grad = static_cast<double>(output_grad_quad.y);
            } else if(n_output_grad_buffered == 2) {
                output_grad = static_cast<double>(output_grad_quad.z);
            } else {
                output_grad = static_cast<double>(output_grad_quad.w);
            }
            n_output_grad_buffered--;
        } else {
            output_grad = static_cast<double>(*r_output_grad_ptr);
        }

        // Vectorized read of main weights
        if(((reinterpret_cast<uintptr_t>(r_main_weights_ptr) & 15) == 0) && (cursor < n_outputs_in_current_block - 3)) {
            asm volatile(
                "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                : "=f"(main_weights_quad.x), "=f"(main_weights_quad.y), "=f"(main_weights_quad.z), "=f"(main_weights_quad.w)
                : "l"(r_main_weights_ptr)
            );
            weight = static_cast<double>(main_weights_quad.x);
            n_main_w_buffered = 3;
        } else if(n_main_w_buffered > 0) {
            if(n_main_w_buffered == 3) {
                weight = static_cast<double>(main_weights_quad.y);
            } else if(n_main_w_buffered == 2) {
                weight = static_cast<double>(main_weights_quad.z);
            } else {
                weight = static_cast<double>(main_weights_quad.w);
            }
            n_main_w_buffered--;
        } else {
            weight = static_cast<double>(*r_main_weights_ptr);
        }

        main_grad_sum += weight * output_grad;

        // Vectorized read of alt weights
        if(((reinterpret_cast<uintptr_t>(r_alt_weights_ptr) & 15) == 0) && (cursor < n_outputs_in_current_block - 3)) {
            asm volatile(
                "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                : "=f"(alt_weights_quad.x), "=f"(alt_weights_quad.y), "=f"(alt_weights_quad.z), "=f"(alt_weights_quad.w)
                : "l"(r_alt_weights_ptr)
            );
            weight = static_cast<double>(alt_weights_quad.x);
            n_alt_w_buffered = 3;
        } else if(n_alt_w_buffered > 0) {
            if(n_alt_w_buffered == 3) {
                weight = static_cast<double>(alt_weights_quad.y);
            } else if(n_alt_w_buffered == 2) {
                weight = static_cast<double>(alt_weights_quad.z);
            } else {
                weight = static_cast<double>(alt_weights_quad.w);
            }
            n_alt_w_buffered--;
        } else {
            weight = static_cast<double>(*r_alt_weights_ptr);
        }

        alt_grad_sum += weight * output_grad;

        r_output_grad_ptr++;
        r_main_weights_ptr++;
        r_alt_weights_ptr++;
    }
    #else
    for(int32_t cursor = 0; cursor < n_outputs_in_current_block; cursor++, weights_main_shift++, weights_alt_shift++) {
        output_grad = static_cast<double>(r_output_gradients[cursor]);
        main_grad_sum += static_cast<double>(r_weights[weights_main_shift]) * output_grad;
        alt_grad_sum += static_cast<double>(r_weights[weights_alt_shift]) * output_grad;
    }
    #endif
    du *= main_grad_sum - alt_grad_sum;

    if(fabs(du) > EPS) {
        AnchorsPair ap = r_detectors[detector_index * n_anchors_per_detector + flipping_anchor_index];
        EXTERNAL_REAL_DT* w_input_grad = w_input_gradients + i_or_j * n_inputs;

        #ifdef INTEGERS_INSTEAD_OF_FLOATS
            sum32_delta = static_cast<SUMMATION32_DT>(du * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(
                reinterpret_cast<SUMMATION32_DT *>(w_input_grad) + ap.anchor1_id,
                sum32_delta
            );
            atomicAdd(
                reinterpret_cast<SUMMATION32_DT *>(w_input_grad) + ap.anchor2_id,
                -sum32_delta
            );
            #else
            *reinterpret_cast<SUMMATION32_DT *>(w_input_grad + ap.anchor1_id) += sum32_delta;
            *reinterpret_cast<SUMMATION32_DT *>(w_input_grad + ap.anchor2_id) -= sum32_delta;
            #endif
        #else
            #ifdef ATOMIC
            atomicAdd(
                w_input_grad + ap.anchor1_id,
                du
            );
            atomicAdd(
                w_input_grad + ap.anchor2_id,
                -du
            );
            #else
            w_input_grad[ap.anchor1_id] += du;
            w_input_grad[ap.anchor2_id] -= du;
            #endif
        #endif
    }

    if((fabs(pe_min_delta) < fabs(q_min_anchor_delta)) && (fabs(pe_min_delta) < fabs(k_min_anchor_delta))) {
        concatenated_index_alt = (
            (Q << (n_anchors_per_detector + positional_dim)) |
            (K << positional_dim) |
            (PE ^ (1 << r_positional_min_delta_indices[pe_offset]))
        ) + detector_index * n_lookup_neurons_per_detector;

        du = static_cast<double>(pe_min_delta);
        if(du > 0) {
            du = 1.0 / (1.0 + fabs(du));
            du *= 0.5 * du;
        } else {
            du = 1.0 / (1.0 + fabs(du));
            du *= -0.5 * du;
        }
        alt_grad_sum = 0.0;
        
        // Fully connected: direct indexing
        uint64_t weights_alt_shift = static_cast<uint64_t>(concatenated_index_alt) * n_outputs;

        #ifdef ATOMIC
        float4 output_grad_quad = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
        float4 alt_weights_quad = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
        int32_t n_output_grad_buffered = 0;
        int32_t n_alt_w_buffered = 0;
        EXTERNAL_REAL_DT* r_output_grad_ptr = r_output_gradients;
        EXTERNAL_REAL_DT* r_alt_weights_ptr = r_weights + weights_alt_shift;

        for(int32_t cursor = 0; cursor < n_outputs_in_current_block; cursor++) {
            // Vectorized read of output gradients
            if(((reinterpret_cast<uintptr_t>(r_output_grad_ptr) & 15) == 0) && (cursor < n_outputs_in_current_block - 3)) {
                asm volatile(
                    "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                    : "=f"(output_grad_quad.x), "=f"(output_grad_quad.y), "=f"(output_grad_quad.z), "=f"(output_grad_quad.w)
                    : "l"(r_output_grad_ptr)
                );
                output_grad = static_cast<double>(output_grad_quad.x);
                n_output_grad_buffered = 3;
            } else if(n_output_grad_buffered > 0) {
                if(n_output_grad_buffered == 3) {
                    output_grad = static_cast<double>(output_grad_quad.y);
                } else if(n_output_grad_buffered == 2) {
                    output_grad = static_cast<double>(output_grad_quad.z);
                } else {
                    output_grad = static_cast<double>(output_grad_quad.w);
                }
                n_output_grad_buffered--;
            } else {
                output_grad = static_cast<double>(*r_output_grad_ptr);
            }

            // Vectorized read of alt weights
            if(((reinterpret_cast<uintptr_t>(r_alt_weights_ptr) & 15) == 0) && (cursor < n_outputs_in_current_block - 3)) {
                asm volatile(
                    "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                    : "=f"(alt_weights_quad.x), "=f"(alt_weights_quad.y), "=f"(alt_weights_quad.z), "=f"(alt_weights_quad.w)
                    : "l"(r_alt_weights_ptr)
                );
                weight = static_cast<double>(alt_weights_quad.x);
                n_alt_w_buffered = 3;
            } else if(n_alt_w_buffered > 0) {
                if(n_alt_w_buffered == 3) {
                    weight = static_cast<double>(alt_weights_quad.y);
                } else if(n_alt_w_buffered == 2) {
                    weight = static_cast<double>(alt_weights_quad.z);
                } else {
                    weight = static_cast<double>(alt_weights_quad.w);
                }
                n_alt_w_buffered--;
            } else {
                weight = static_cast<double>(*r_alt_weights_ptr);
            }

            alt_grad_sum += weight * output_grad;

            r_output_grad_ptr++;
            r_alt_weights_ptr++;
        }
        #else
        for(int32_t cursor = 0; cursor < n_outputs_in_current_block; cursor++, weights_alt_shift++) {
            output_grad = static_cast<double>(r_output_gradients[cursor]);
            alt_grad_sum += static_cast<double>(r_weights[weights_alt_shift]) * output_grad;
        }
        #endif
        du *= main_grad_sum - alt_grad_sum;
        w_positional_embeddings_gradients += pe_offset * positional_dim;
        #ifdef INTEGERS_INSTEAD_OF_FLOATS
            sum32_delta = static_cast<SUMMATION32_DT>(du * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(
                reinterpret_cast<SUMMATION32_DT *>(w_positional_embeddings_gradients) + pe_min_delta_index,
                sum32_delta
            );
            #else
            *reinterpret_cast<SUMMATION32_DT *>(w_positional_embeddings_gradients + pe_min_delta_index) += sum32_delta;
            #endif
        #else
            #ifdef ATOMIC
            atomicAdd(
                w_positional_embeddings_gradients + pe_min_delta_index,
                du
            );
            #else
            w_positional_embeddings_gradients[pe_min_delta_index] += du;
            #endif
        #endif
    }
}

gather_w_gradients_for_sequence_logic(
    EXTERNAL_REAL_DT* r_output_gradients,
    int32_t* r_lookup_indices,
    int32_t* r_positional_lookup_indices,
    uint32_t n_detectors,
    uint32_t n_total_tiles,
    uint32_t sequence_length,
    uint32_t n_anchors_per_detector,
    uint32_t n_outputs,
    uint32_t n_output_blocks,
    uint32_t n_outputs_per_block,
    NoDelaysIndexedSynapsesInfo* r_lookup_neuron_synapses_infos,
    BaseSynapseMeta* r_synapse_metas,
    NeuronDataId_t first_synapse_id,
    uint8_t* lut_data,
    uint32_t n_lookup_neurons_per_detector,
    EXTERNAL_REAL_DT* w_weights_gradients,
    uint32_t positional_dim,
    double external_lr,
    double first_synapse_meta_lr,
    double int_rescaler
) {
    // blockIdx.x encodes: tile_id * n_detectors + detector_index
    // Extract tile_id and detector_index
    uint32_t tile_id = blockIdx.x / n_detectors;
    uint32_t detector_index = blockIdx.x % n_detectors;
    uint32_t tile_grid_w = (sequence_length + TILE - 1) / TILE;

    if(tile_id >= n_total_tiles) {
        return;
    }

    uint32_t bi = tile_id / tile_grid_w;  // tile row
    uint32_t bj = tile_id % tile_grid_w;  // tile col

    // Skip tiles below diagonal (bi > bj means all pairs have i > j, which we don't want)
    if(bi > bj) {
        return;
    }

    // Calculate tile start positions
    uint32_t i0 = bi * TILE;
    uint32_t j0 = bj * TILE;

    // Get thread coordinates within tile
    uint32_t i = i0 + (threadIdx.x / TILE);  // first timestep (row)
    uint32_t j = j0 + (threadIdx.x % TILE);  // second timestep (col)

    // Check if this (i, j) pair is valid (i < j and within bounds)
    bool valid_pair = false;
    if(bi < bj) {
        // Tile strictly above diagonal - all elements are valid (i < j)
        if(i < sequence_length && j < sequence_length) {
            valid_pair = true;
        }
    } else {
        // Diagonal tile - only process where i < j
        if(i < sequence_length && j < sequence_length && i < j) {
            valid_pair = true;
        }
    }

    if(valid_pair) {
        // blockIdx.y encodes: batch_index * n_output_blocks + output_block_index
        uint32_t batch_index = blockIdx.y / n_output_blocks;
        uint32_t output_block_index = blockIdx.y % n_output_blocks;
        r_output_gradients += (batch_index * sequence_length + j) * n_outputs;

        int32_t n_outputs_in_current_block;
        if(r_lookup_neuron_synapses_infos == nullptr) {
            w_weights_gradients += output_block_index * n_outputs_per_block;
            r_output_gradients += output_block_index * n_outputs_per_block;
            if((output_block_index == n_output_blocks - 1) && (n_outputs % n_outputs_per_block)) {
                n_outputs_in_current_block = n_outputs % n_outputs_per_block;
            } else {
                n_outputs_in_current_block = n_outputs_per_block;
            }
        }

        // Calculate offsets into lookup_indices arrays
        // Layout: [batch0_t0_det0, batch0_t0_det1, ..., batch0_t0_detN, batch0_t1_det0, ...]
        uint32_t n_detector_items = sequence_length * n_detectors;
        uint32_t batch_offset = batch_index * n_detector_items;


        // Get K: lookup index from first timestep (i)
        uint32_t k_offset = batch_offset + i * n_detectors + detector_index;
        int32_t K = r_lookup_indices[k_offset];

        // Get Q: lookup index from second timestep (j)
        uint32_t q_offset = batch_offset + j * n_detectors + detector_index;
        int32_t Q = r_lookup_indices[q_offset];

        // Get PE: positional lookup index from timestep difference (j - i - 1)
        uint32_t pe_offset = (j - i - 1) * n_detectors + detector_index;
        int32_t PE = r_positional_lookup_indices[pe_offset];

        // Calculate concatenated index
        uint32_t concatenated_index = (
            (Q << (n_anchors_per_detector + positional_dim)) |
            (K << positional_dim) |
            PE
        ) + detector_index * n_lookup_neurons_per_detector;

        double output_grad;
        SUMMATION32_DT sum32_delta;
        uint32_t n_lookup_neurons = n_lookup_neurons_per_detector * n_detectors;
        double lr;

        if(r_lookup_neuron_synapses_infos != nullptr) {
            // Sparse connectivity: gather weight gradients using ForwardSynapseGroups
            NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info = *(r_lookup_neuron_synapses_infos + concatenated_index);
            
            if(output_block_index < lookup_neuron_synapses_info.n_groups) {
                NeuronDataId_t current_group_id;
                PFX(find_forward_synapse_group)(
                    lookup_neuron_synapses_info,
                    output_block_index,
                    n_outputs_per_block,
                    lut_data,
                    current_group_id
                );

                ForwardSynapseGroup fws_group = *GetForwardSynapseGroup(current_group_id, lut_data);
                uint32_t sm_index = SynapseGroupSynapseMetaIndex(fws_group.meta_info);
                if(sm_index == 0) {
                    lr = first_synapse_meta_lr;
                } else {
                    lr = static_cast<double>((r_synapse_metas + sm_index)->lr);
                }
                if(external_lr >= 0) {
                    lr *= -external_lr;
                }

                uint32_t current_group_size = SynapseGroupSize(fws_group.meta_info);
                NeuronIndex_t* current_target_ptr = TargetNeuronsInForwardGroup(current_group_id, lut_data);
                long weight_shift = (reinterpret_cast<uint8_t *>(current_target_ptr) - (lut_data + first_synapse_id)) >> 2;
                
                #ifdef ATOMIC
                uint4 targets_quad = make_uint4(0, 0, 0, 0);
                int32_t n_t_buffered = 0;
                NeuronIndex_t target_id;
                
                for(uint32_t cursor = 0; cursor < current_group_size; cursor++, current_target_ptr++, weight_shift++) {
                    // Vectorized read of target IDs
                    if(((reinterpret_cast<uintptr_t>(current_target_ptr) & 15) == 0) && (cursor < current_group_size - 3)) {
                        asm volatile(
                            "ld.global.v4.s32 {%0,%1,%2,%3}, [%4];"
                            : "=r"(targets_quad.x), "=r"(targets_quad.y), "=r"(targets_quad.z), "=r"(targets_quad.w)
                            : "l"(current_target_ptr)
                        );
                        target_id = targets_quad.x;
                        n_t_buffered = 3;
                    } else if(n_t_buffered > 0) {
                        if(n_t_buffered == 3) {
                            target_id = targets_quad.y;
                        } else if(n_t_buffered == 2) {
                            target_id = targets_quad.z;
                        } else {
                            target_id = targets_quad.w;
                        }
                        n_t_buffered--;
                    } else {
                        target_id = *current_target_ptr;
                    }
                    target_id -= n_lookup_neurons;
                    output_grad = static_cast<double>(r_output_gradients[target_id]);

                    if(fabs(output_grad) > EPS) {
                        #ifdef INTEGERS_INSTEAD_OF_FLOATS
                            sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
                            #ifdef ATOMIC
                            atomicAdd(
                                reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients) + weight_shift,
                                sum32_delta
                            );
                            #else
                            *reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients + weight_shift) += sum32_delta;
                            #endif
                        #else
                            sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr);
                            #ifdef ATOMIC
                            atomicAdd(
                                w_weights_gradients + weight_shift,
                                sum32_delta
                            );
                            #else
                            w_weights_gradients[weight_shift] += sum32_delta;
                            #endif
                        #endif
                    }
                }
                #else
                for(uint32_t cursor = 0; cursor < current_group_size; cursor++, weight_shift++) {
                    NeuronIndex_t target_id = current_target_ptr[cursor] - n_lookup_neurons;
                    output_grad = static_cast<double>(r_output_gradients[target_id]);
                    if(fabs(output_grad) > EPS) {
                        #ifdef INTEGERS_INSTEAD_OF_FLOATS
                            sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
                            #ifdef ATOMIC
                            atomicAdd(
                                reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients) + weight_shift,
                                sum32_delta
                            );
                            #else
                            *reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients + weight_shift) += sum32_delta;
                            #endif
                        #else
                            sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr);
                            #ifdef ATOMIC
                            atomicAdd(
                                w_weights_gradients + weight_shift,
                                sum32_delta
                            );
                            #else
                            w_weights_gradients[weight_shift] += sum32_delta;
                            #endif
                        #endif
                    }
                }
                #endif
            }
        } else {
            // Fully connected: direct indexing
            lr = first_synapse_meta_lr;
            if(external_lr >= 0) {
                lr *= -external_lr;
            }
            uint64_t weights_main_shift = static_cast<uint64_t>(concatenated_index) * n_outputs;
            
            #ifdef ATOMIC
            float4 output_grad_quad = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
            int32_t n_output_grad_buffered = 0;
            EXTERNAL_REAL_DT* r_output_grad_ptr = r_output_gradients;
            
            for(int32_t cursor = 0; cursor < n_outputs_in_current_block; cursor++, weights_main_shift++) {
                // Vectorized read of output gradients
                if(((reinterpret_cast<uintptr_t>(r_output_grad_ptr) & 15) == 0) && (cursor < n_outputs_in_current_block - 3)) {
                    asm volatile(
                        "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                        : "=f"(output_grad_quad.x), "=f"(output_grad_quad.y), "=f"(output_grad_quad.z), "=f"(output_grad_quad.w)
                        : "l"(r_output_grad_ptr)
                    );
                    output_grad = static_cast<double>(output_grad_quad.x);
                    n_output_grad_buffered = 3;
                } else if(n_output_grad_buffered > 0) {
                    if(n_output_grad_buffered == 3) {
                        output_grad = static_cast<double>(output_grad_quad.y);
                    } else if(n_output_grad_buffered == 2) {
                        output_grad = static_cast<double>(output_grad_quad.z);
                    } else {
                        output_grad = static_cast<double>(output_grad_quad.w);
                    }
                    n_output_grad_buffered--;
                } else {
                    output_grad = static_cast<double>(*r_output_grad_ptr);
                }
                
                if(fabs(output_grad) > EPS) {
                    #ifdef INTEGERS_INSTEAD_OF_FLOATS
                        sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
                        #ifdef ATOMIC
                        atomicAdd(
                            reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients) + weights_main_shift,
                            sum32_delta
                        );
                        #else
                        *reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients + weights_main_shift) += sum32_delta;
                        #endif
                    #else
                        sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr);
                        #ifdef ATOMIC
                        atomicAdd(
                            w_weights_gradients + weights_main_shift,
                            sum32_delta
                        );
                        #else
                        w_weights_gradients[weights_main_shift] += sum32_delta;
                        #endif
                    #endif
                }
                
                r_output_grad_ptr++;
            }
            #else
            for(int32_t cursor = 0; cursor < n_outputs_in_current_block; cursor++, weights_main_shift++) {
                output_grad = static_cast<double>(r_output_gradients[cursor]);
                if(fabs(output_grad) > EPS) {
                    #ifdef INTEGERS_INSTEAD_OF_FLOATS
                        sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
                        #ifdef ATOMIC
                        atomicAdd(
                            reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients) + weights_main_shift,
                            sum32_delta
                        );
                        #else
                        *reinterpret_cast<SUMMATION32_DT *>(w_weights_gradients + weights_main_shift) += sum32_delta;
                        #endif
                    #else
                        sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr);
                        #ifdef ATOMIC
                        atomicAdd(
                            w_weights_gradients + weights_main_shift,
                            sum32_delta
                        );
                        #else
                        w_weights_gradients[weights_main_shift] += sum32_delta;
                        #endif
                    #endif
                }
            }
            #endif
        }
    }
}
