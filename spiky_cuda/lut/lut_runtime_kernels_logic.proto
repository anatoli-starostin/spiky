check_detectors_logic(
    EXTERNAL_REAL_DT* input,
    uint32_t n_inputs,
    AnchorsPair* detectors,
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    int32_t* target_lookup_indices,
    EXTERNAL_REAL_DT* target_min_anchor_deltas,
    int32_t* target_min_anchor_delta_indices
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;

    if(i < n_detectors) {
        // Offset input for current batch (sequence_length == 1)
        EXTERNAL_REAL_DT* current_input = input + blockIdx.y * n_inputs;
        target_lookup_indices += blockIdx.y * n_detectors + i;
        target_min_anchor_deltas += blockIdx.y * n_detectors + i;
        target_min_anchor_delta_indices += blockIdx.y * n_detectors + i;

        // Get detector's anchor pairs
        AnchorsPair* detector_anchors = detectors + i * n_anchors_per_detector;

        // Calculate differences between anchor pairs and form bit representation
        int32_t lookup_index = 0;
        EXTERNAL_REAL_DT min_delta = 0.0;
        int32_t min_delta_anchor_idx = -1;
        AnchorsPair ap;

        for(uint32_t anchor_idx = 0; anchor_idx < n_anchors_per_detector; anchor_idx++) {
            ap = detector_anchors[anchor_idx];

            // Skip if anchor IDs are invalid
            if(ap.anchor1_id == std::numeric_limits<NeuronIndex_t>::max() ||
               ap.anchor2_id == std::numeric_limits<NeuronIndex_t>::max()) {
                continue;
            }

            EXTERNAL_REAL_DT delta = current_input[ap.anchor1_id] - current_input[ap.anchor2_id];

            // Form bit representation: 1 if delta > 0, 0 if delta <= 0
            if(delta > 0.0) {
                lookup_index |= (1 << anchor_idx);
            }

            // Track anchor pair with minimum absolute delta
            if((min_delta_anchor_idx == -1) || (fabs(delta) < fabs(min_delta))) {
                min_delta = delta;
                min_delta_anchor_idx = anchor_idx;
            }
        }

        // Store lookup index and related data
        *target_lookup_indices = lookup_index;
        *target_min_anchor_deltas = min_delta;
        *target_min_anchor_delta_indices = min_delta_anchor_idx;

        __SUPER_DETAILED_TRACE__("check_detectors_logic: detector_index %d, lookup_index %d, min_delta %f, min_delta_anchor_idx %d\n", i, lookup_index, min_delta, min_delta_anchor_idx);
    }
}

fire_detectors_logic(
    EXTERNAL_REAL_DT* input,
    uint32_t n_inputs,
    AnchorsPair* detectors,
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    uint32_t n_lookup_neurons_per_detector,
    int32_t* target_lookup_indices,
    EXTERNAL_REAL_DT* target_min_anchor_deltas,
    int32_t* target_min_anchor_delta_indices,
    NoDelaysIndexedSynapsesInfo* lookup_neuron_synapses_infos,
    Firing* firings_buffer,
    uint64_t* firings_counter_ptr,
    uint32_t synapse_group_size,
    uint8_t* lut_data,
    int device
) {
    uint32_t tid = threadIdx.x;
    uint32_t i = blockIdx.x * blockDim.x + tid; // blockDim.x should be power of 2

    NoDelaysIndexedSynapsesInfo lookup_neuron_synapses_info;
    lookup_neuron_synapses_info.n_groups = 0;

    if(i < n_detectors) {
        // Offset input for current batch (sequence_length == 1)
        EXTERNAL_REAL_DT* current_input = input + blockIdx.y * n_inputs;
        target_lookup_indices += blockIdx.y * n_detectors + i;
        target_min_anchor_deltas += blockIdx.y * n_detectors + i;
        target_min_anchor_delta_indices += blockIdx.y * n_detectors + i;

        // Get detector's anchor pairs
        AnchorsPair* detector_anchors = detectors + i * n_anchors_per_detector;

        // Calculate differences between anchor pairs and form bit representation
        int32_t lookup_index = 0;
        EXTERNAL_REAL_DT min_delta = 0.0;
        int32_t min_delta_anchor_idx = -1;
        AnchorsPair ap;

        for(uint32_t anchor_idx = 0; anchor_idx < n_anchors_per_detector; anchor_idx++) {
            ap = detector_anchors[anchor_idx];

            // Skip if anchor IDs are invalid
            if(ap.anchor1_id == std::numeric_limits<NeuronIndex_t>::max() ||
               ap.anchor2_id == std::numeric_limits<NeuronIndex_t>::max()) {
                continue;
            }

            EXTERNAL_REAL_DT delta = current_input[ap.anchor1_id] - current_input[ap.anchor2_id];

            // Form bit representation: 1 if delta > 0, 0 if delta <= 0
            if(delta > 0.0) {
                lookup_index |= (1 << anchor_idx);
            }

            // Track anchor pair with minimum absolute delta
            if((min_delta_anchor_idx == -1) || (fabs(delta) < fabs(min_delta))) {
                min_delta = delta;
                min_delta_anchor_idx = anchor_idx;
            }
        }

        // Store lookup index and related data
        *target_lookup_indices = lookup_index;
        *target_min_anchor_deltas = min_delta;
        *target_min_anchor_delta_indices = min_delta_anchor_idx;

        __SUPER_DETAILED_TRACE__("fire_detectors_logic: detector_index %d, lookup_index %d, min_delta %f, min_delta_anchor_idx %d\n", i, lookup_index, min_delta, min_delta_anchor_idx);

        if(firings_buffer != nullptr) {
            lookup_neuron_synapses_info = *(lookup_neuron_synapses_infos + i * n_lookup_neurons_per_detector + lookup_index);
        }
    }

    if(firings_buffer != nullptr) {
        uint64_t firings_offset;
        if(device == -1) {
            if(lookup_neuron_synapses_info.n_groups > 0) {
                firings_offset = *firings_counter_ptr;
                (*firings_counter_ptr) += lookup_neuron_synapses_info.n_groups;
            }
        } else {
            #ifdef ATOMIC
            extern __shared__ __align__(16) uint8_t __sm[];
            uint32_t *sdata = reinterpret_cast<uint32_t *>(__sm);
            sdata[tid] = lookup_neuron_synapses_info.n_groups;
            __syncthreads();
            uint32_t t;
            int offset;
            int idx;

            // upsweep (reduce)
            for(offset = 1; offset < blockDim.x; offset <<= 1) {
                idx = ((tid + 1) * (offset << 1)) - 1;
                if(idx < blockDim.x) {
                    t = sdata[idx - offset];
                    if(t > 0) {
                        sdata[idx] += t;
                    }
                }
                __syncthreads();
            }

            // inject global shift
            if(tid == 0) {
                sdata[blockDim.x - 1] = atomicAdd(
                    reinterpret_cast<unsigned long long*>(firings_counter_ptr),
                    static_cast<unsigned long long>(sdata[blockDim.x - 1])
                );
            }
            __syncthreads();

            // downsweep
            for(offset = blockDim.x >> 1; offset > 0; offset >>= 1) {
                idx = ((tid + 1) * (offset << 1)) - 1;
                if (idx < blockDim.x) {
                    t = sdata[idx - offset];
                    sdata[idx - offset] = sdata[idx];
                    if(t > 0) {
                        sdata[idx] += t;
                    }
                }
                __syncthreads();
            }

            firings_offset = sdata[tid];
            #endif
        }

        if(lookup_neuron_synapses_info.n_groups > 0) {
            if(lookup_neuron_synapses_info.n_synapse_metas == 1) {
                NeuronDataId_t current_group_id = lookup_neuron_synapses_info.first_group_id;
                for(uint32_t j=0;j < lookup_neuron_synapses_info.n_groups;j++) {
                    firings_buffer[firings_offset + j] = Firing{
                        blockIdx.y, 1.0, current_group_id
                    };
                    current_group_id += SizeOfForwardSynapseGroup(synapse_group_size, true);
                }
            } else {
                DelayInfo* delays_info = DelayInfos(lookup_neuron_synapses_info.delays_info_id, lut_data);
                for(uint32_t j=0;j < lookup_neuron_synapses_info.n_synapse_metas;j++) {
                    DelayInfo delay_info = delays_info[j];
                    if(delay_info != 0) {
                        uint32_t n_groups = DELAY_INFO_N_GROUPS(delay_info);
                        NeuronDataId_t current_group_id = lookup_neuron_synapses_info.first_group_id + DELAY_INFO_BYTE_SHIFT_FROM_FIRST_GROUP(delay_info);
                        for(uint32_t k=0;k < n_groups;k++) {
                            firings_buffer[firings_offset + k] = Firing{
                                blockIdx.y, 1.0, current_group_id
                            };
                            current_group_id += SizeOfForwardSynapseGroup(synapse_group_size, true);
                        }
                        firings_offset += n_groups;
                    }
                }
            }
        }
    }
}

fill_outputs_logic(
    EXTERNAL_REAL_DT* weights,
    NeuronDataId_t first_synapse_id,
    Firing* firings_buffer,
    uint64_t n_firings,
    EXTERNAL_REAL_DT* target_output,
    uint32_t n_lookup_neurons,
    uint32_t n_outputs,
    uint8_t* lut_data,
    double int_rescaler
) {
    uint64_t i = static_cast<uint64_t>(blockIdx.x) * blockDim.x + threadIdx.x;
    if(i < n_firings) {
        Firing firing = firings_buffer[i];
        target_output += firing.batch_index * n_outputs;

        ForwardSynapseGroup* fws_group_ptr = GetForwardSynapseGroup(firing.data_id, lut_data);
        int32_t current_group_size = static_cast<int32_t>(SynapseGroupSize(fws_group_ptr->meta_info));
        NeuronIndex_t* current_target_ptr = TargetNeuronsInForwardGroup(firing.data_id, lut_data);
        long weight_shift = (reinterpret_cast<uint8_t *>(current_target_ptr) - (lut_data + first_synapse_id)) >> 2;
        EXTERNAL_REAL_DT* weight_ptr = weights + weight_shift;
        uint4 targets_quad = make_uint4(0, 0, 0, 0);
        float4 weights_quad = make_float4(0.0, 0.0, 0.0, 0.0);

        NeuronIndex_t target_id;
        double weight;
        SUMMATION32_DT payload;

        __SUPER_DETAILED_TRACE__("[fill_outputs_logic] i %d, current_group_size %d, n_firings %d, weight_shift %ld\n", i, current_group_size, n_firings, weight_shift);

        int32_t n_t_buffered = 0;
        int32_t n_w_buffered = 0;
        for(int32_t cursor=0;cursor < current_group_size;cursor++, current_target_ptr++, weight_ptr++) {
            if(((reinterpret_cast<uintptr_t>(current_target_ptr) & 15) == 0) && (cursor < current_group_size - 3)) {
                targets_quad = *reinterpret_cast<uint4 *>(current_target_ptr);
                target_id = targets_quad.x;
                n_t_buffered = 3;
            } else if(n_t_buffered > 0) {
                if(n_t_buffered == 3) {
                    target_id = targets_quad.y;
                } else if(n_t_buffered == 2) {
                    target_id = targets_quad.z;
                } else {
                    target_id = targets_quad.w;
                }
                n_t_buffered--;
            } else {
                target_id = *current_target_ptr;
            }

            if(((reinterpret_cast<uintptr_t>(weight_ptr) & 15) == 0) && (cursor < current_group_size - 3)) {
                weights_quad = *reinterpret_cast<float4 *>(weight_ptr);
                weight = static_cast<double>(weights_quad.x);
                n_w_buffered = 3;
            } else if(n_w_buffered > 0) {
                if(n_w_buffered == 3) {
                    weight = static_cast<double>(weights_quad.y);
                } else if(n_w_buffered == 2) {
                    weight = static_cast<double>(weights_quad.z);
                } else {
                    weight = static_cast<double>(weights_quad.w);
                }
                n_w_buffered--;
            } else {
                weight = static_cast<double>(*weight_ptr);
            }

            target_id -= n_lookup_neurons;
            __SUPER_DETAILED_TRACE__("[fill_outputs_logic] target_id %d, weight %f\n", target_id, weight);

            #ifdef INTEGERS_INSTEAD_OF_FLOATS
                payload = static_cast<SUMMATION32_DT>(weight * static_cast<double>(DENOMINATOR32) * int_rescaler);
                #ifdef ATOMIC
                atomicAdd(
                    reinterpret_cast<SUMMATION32_DT *>(target_output + target_id),
                    payload
                );
                #else
                *reinterpret_cast<SUMMATION32_DT *>(target_output + target_id) += payload;
                #endif
            #else
                payload = static_cast<SUMMATION32_DT>(weight);
                #ifdef ATOMIC
                atomicAdd(
                    target_output + target_id,
                    payload
                );
                #else
                __SUPER_DETAILED_TRACE__("[fill_outputs_logic] target_id %d, payload %f\n", target_id, payload);
                target_output[target_id] += payload;
                #endif
            #endif
        }
    }
}

fill_outputs_fully_connected_logic(
    EXTERNAL_REAL_DT* weights,
    int32_t* lookup_indices,
    EXTERNAL_REAL_DT* target_output,
    uint32_t n_outputs,
    uint32_t n_detectors,
    uint32_t n_detector_blocks,
    uint32_t n_lookup_neurons_per_detector,
    int32_t n_detectors_per_block,
    double int_rescaler
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    if(i < n_outputs * n_detector_blocks) {
        uint32_t output_index = i / n_detector_blocks;
        uint32_t detector_block_index = i % n_detector_blocks;
        __SUPER_DETAILED_TRACE__("fill_outputs_fully_connected_logic: batch_index %d, output_index %d, detector_block_index %d\n", blockIdx.y, output_index, detector_block_index);
        lookup_indices += blockIdx.y * n_detectors;
        lookup_indices += detector_block_index * n_detectors_per_block;
        target_output += blockIdx.y * n_outputs;
        uint64_t weights_per_detector = static_cast<uint64_t>(n_lookup_neurons_per_detector) * n_outputs;
        __SUPER_DETAILED_TRACE__("fill_outputs_fully_connected_logic: weights_per_detector %lu\n", weights_per_detector);
        weights += weights_per_detector * detector_block_index * n_detectors_per_block;
        int4 li_quad = make_int4(0, 0, 0, 0);
        int32_t current_lookup_index;
        int32_t n_buffered = 0;
        double weight_sum = 0.0;
        if((detector_block_index == n_detector_blocks - 1) && (n_detectors % n_detectors_per_block)) {
            n_detectors_per_block = n_detectors % n_detectors_per_block;
        }
        __SUPER_DETAILED_TRACE__("fill_outputs_fully_connected_logic: n_detectors_per_block %d\n", n_detectors_per_block);
        for(int32_t cursor=0;cursor < n_detectors_per_block;cursor++, lookup_indices++, weights+=weights_per_detector) {
            if(((reinterpret_cast<uintptr_t>(lookup_indices) & 15) == 0) && (cursor < n_detectors_per_block - 3)) {
//                #ifdef ATOMIC
//                asm volatile(
//                    "ld.global.v4.s32 {%0,%1,%2,%3}, [%4];"
//                    : "=r"(li_quad.x), "=r"(li_quad.y), "=r"(li_quad.z), "=r"(li_quad.w)
//                    : "l"(lookup_indices)
//                );
//                #else
                li_quad = *reinterpret_cast<int4 *>(lookup_indices);
//                #endif
                current_lookup_index = li_quad.x;
                n_buffered = 3;
            } else if(n_buffered > 0) {
                if(n_buffered == 3) {
                    current_lookup_index = li_quad.y;
                } else if(n_buffered == 2) {
                    current_lookup_index = li_quad.z;
                } else {
                    current_lookup_index = li_quad.w;
                }
                n_buffered--;
            } else {
                current_lookup_index = *lookup_indices;
            }
            __SUPER_DETAILED_TRACE__("fill_outputs_fully_connected_logic: current_lookup_index %d, output_index %d, weight %f\n", current_lookup_index, output_index, weights[current_lookup_index * n_outputs + output_index]);
            weight_sum += static_cast<double>(weights[current_lookup_index * n_outputs + output_index]);
        }

        __SUPER_DETAILED_TRACE__("fill_outputs_fully_connected_logic: weight_sum %f\n", weight_sum);

        SUMMATION32_DT payload;
        #ifdef INTEGERS_INSTEAD_OF_FLOATS
            payload = static_cast<SUMMATION32_DT>(weight_sum * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(
                reinterpret_cast<SUMMATION32_DT *>(target_output + output_index),
                payload
            );
            #else
            *reinterpret_cast<SUMMATION32_DT *>(target_output + output_index) += payload;
            #endif
        #else
            payload = static_cast<SUMMATION32_DT>(weight_sum);
            #ifdef ATOMIC
            atomicAdd(
                target_output + output_index,
                payload
            );
            #else
            __SUPER_DETAILED_TRACE__("fill_outputs_fully_connected_logic: payload %f\n", payload);
            target_output[output_index] += payload;
            #endif
        #endif
    }
}

fire_detectors_by_lookup_indices_logic(
    uint32_t n_detectors,
    int32_t* lookup_indices,
    int32_t* min_anchor_delta_indices,
    uint32_t n_lookup_neurons_per_detector,
    NoDelaysIndexedSynapsesInfo* lookup_neuron_synapses_infos,
    Firing* firings_buffer,
    uint64_t* firings_counter_ptr,
    uint32_t synapse_group_size,
    uint8_t* lut_data,
    int device
) {
    uint32_t tid = threadIdx.x;
    uint32_t i = blockIdx.x * blockDim.x + tid; // blockDim.x should be power of 2

    NoDelaysIndexedSynapsesInfo chosen_lookup_neuron_synapses_info;
    NoDelaysIndexedSynapsesInfo chosen_alternative_lookup_neuron_synapses_info;
    chosen_lookup_neuron_synapses_info.n_groups = 0;
    chosen_alternative_lookup_neuron_synapses_info.n_groups = 0;

    if(i < n_detectors) {
        lookup_indices += blockIdx.y * n_detectors;
        min_anchor_delta_indices += blockIdx.y * n_detectors;

        int32_t lookup_index = lookup_indices[i];
        chosen_lookup_neuron_synapses_info = *(lookup_neuron_synapses_infos + i * n_lookup_neurons_per_detector + lookup_index);
        chosen_alternative_lookup_neuron_synapses_info = *(lookup_neuron_synapses_infos + i * n_lookup_neurons_per_detector + (lookup_index ^ (1 << min_anchor_delta_indices[i])));
    }

    uint64_t firings_offset;
    if(device == -1) {
        if((chosen_lookup_neuron_synapses_info.n_groups + chosen_lookup_neuron_synapses_info.n_groups) > 0) {
            firings_offset = *firings_counter_ptr;
            (*firings_counter_ptr) += chosen_lookup_neuron_synapses_info.n_groups + chosen_alternative_lookup_neuron_synapses_info.n_groups;
        }
    } else {
        #ifdef ATOMIC
        extern __shared__ __align__(16) uint8_t __sm[];
        uint32_t *sdata = reinterpret_cast<uint32_t *>(__sm);
        sdata[tid] = chosen_lookup_neuron_synapses_info.n_groups + chosen_alternative_lookup_neuron_synapses_info.n_groups;
        __syncthreads();
        uint32_t t;
        int offset;
        int idx;

        // upsweep (reduce)
        for(offset = 1; offset < blockDim.x; offset <<= 1) {
            idx = ((tid + 1) * (offset << 1)) - 1;
            if(idx < blockDim.x) {
                t = sdata[idx - offset];
                if(t > 0) {
                    sdata[idx] += t;
                }
            }
            __syncthreads();
        }

        // inject global shift
        if(tid == 0) {
            sdata[blockDim.x - 1] = atomicAdd(
                reinterpret_cast<unsigned long long*>(firings_counter_ptr),
                static_cast<unsigned long long>(sdata[blockDim.x - 1])
            );
        }
        __syncthreads();

        // downsweep
        for(offset = blockDim.x >> 1; offset > 0; offset >>= 1) {
            idx = ((tid + 1) * (offset << 1)) - 1;
            if (idx < blockDim.x) {
                t = sdata[idx - offset];
                sdata[idx - offset] = sdata[idx];
                if(t > 0) {
                    sdata[idx] += t;
                }
            }
            __syncthreads();
        }

        firings_offset = sdata[tid];
        #endif
    }

    if((chosen_lookup_neuron_synapses_info.n_groups + chosen_lookup_neuron_synapses_info.n_groups) > 0) {
        if(chosen_lookup_neuron_synapses_info.n_synapse_metas == 1) {
            NeuronDataId_t current_group_id = chosen_lookup_neuron_synapses_info.first_group_id;
            for(uint32_t j=0;j < chosen_lookup_neuron_synapses_info.n_groups;j++) {
                firings_buffer[firings_offset + j] = Firing{
                    blockIdx.y, 1.0, current_group_id
                };
                current_group_id += SizeOfForwardSynapseGroup(synapse_group_size, true);
            }
            firings_offset += chosen_lookup_neuron_synapses_info.n_groups;
            current_group_id = chosen_alternative_lookup_neuron_synapses_info.first_group_id;
            for(uint32_t j=0;j < chosen_alternative_lookup_neuron_synapses_info.n_groups;j++) {
                firings_buffer[firings_offset + j] = Firing{
                    blockIdx.y, 1.0, current_group_id
                };
                current_group_id += SizeOfForwardSynapseGroup(synapse_group_size, true);
            }
        } else {
            DelayInfo* delays_info = DelayInfos(chosen_lookup_neuron_synapses_info.delays_info_id, lut_data);
            for(uint32_t j=0;j < chosen_lookup_neuron_synapses_info.n_synapse_metas;j++) {
                DelayInfo delay_info = delays_info[j];
                if(delay_info != 0) {
                    uint32_t n_groups = DELAY_INFO_N_GROUPS(delay_info);
                    NeuronDataId_t current_group_id = chosen_lookup_neuron_synapses_info.first_group_id + DELAY_INFO_BYTE_SHIFT_FROM_FIRST_GROUP(delay_info);
                    for(uint32_t k=0;k < n_groups;k++) {
                        firings_buffer[firings_offset + k] = Firing{
                            blockIdx.y, 1.0, current_group_id
                        };
                        current_group_id += SizeOfForwardSynapseGroup(synapse_group_size, true);
                    }
                    firings_offset += n_groups;
                }
            }
            delays_info = DelayInfos(chosen_alternative_lookup_neuron_synapses_info.delays_info_id, lut_data);
            for(uint32_t j=0;j < chosen_alternative_lookup_neuron_synapses_info.n_synapse_metas;j++) {
                DelayInfo delay_info = delays_info[j];
                if(delay_info != 0) {
                    uint32_t n_groups = DELAY_INFO_N_GROUPS(delay_info);
                    NeuronDataId_t current_group_id = chosen_alternative_lookup_neuron_synapses_info.first_group_id + DELAY_INFO_BYTE_SHIFT_FROM_FIRST_GROUP(delay_info);
                    for(uint32_t k=0;k < n_groups;k++) {
                        firings_buffer[firings_offset + k] = Firing{
                            blockIdx.y, 1.0, current_group_id
                        };
                        current_group_id += SizeOfForwardSynapseGroup(synapse_group_size, true);
                    }
                    firings_offset += n_groups;
                }
            }
        }
    }
}

gather_gradients_logic(
    EXTERNAL_REAL_DT* weights,
    NeuronDataId_t first_synapse_id,
    Firing* firings,
    uint64_t n_firings,
    EXTERNAL_REAL_DT* output_gradients,
    SUMMATION32_DT* target_before_detectors_gradients,
    EXTERNAL_REAL_DT* target_weights_gradients,
    uint32_t n_lookup_neurons,
    uint32_t n_outputs,
    uint8_t* lut_data,
    REAL_DT first_synapse_meta_lr,
    BaseSynapseMeta* synapse_metas,
    double int_rescaler
) {
    uint64_t i = static_cast<uint64_t>(blockIdx.x) * blockDim.x + threadIdx.x;
    if(i < n_firings) {
        Firing firing = firings[i];
        output_gradients += firing.batch_index * n_outputs;
        target_before_detectors_gradients += firing.batch_index * n_lookup_neurons;

        ForwardSynapseGroup fws_group = *GetForwardSynapseGroup(firing.data_id, lut_data);
        double lr;
        uint32_t sm_index = SynapseGroupSynapseMetaIndex(fws_group.meta_info);
        if(sm_index == 0) {
            lr = static_cast<double>(first_synapse_meta_lr);
        } else {
            lr = static_cast<double>((synapse_metas + sm_index)->lr);
        }

        int32_t current_group_size = static_cast<int32_t>(SynapseGroupSize(fws_group.meta_info));
        NeuronIndex_t* current_target_ptr = TargetNeuronsInForwardGroup(firing.data_id, lut_data);
        long weight_shift = (reinterpret_cast<uint8_t *>(current_target_ptr) - (lut_data + first_synapse_id)) >> 2;
        EXTERNAL_REAL_DT* weight_ptr = weights + weight_shift;
        uint4 targets_quad = make_uint4(0, 0, 0, 0);
        float4 weights_quad = make_float4(0.0, 0.0, 0.0, 0.0);

        NeuronIndex_t target_id;
        double weight;
        double output_grad;
        double grad_x = 0.0;
        SUMMATION32_DT sum32_delta;

        __SUPER_DETAILED_TRACE__("[gather_gradients_logic] i %d, current_group_size %d, n_firings %d, weight_shift %ld\n", i, current_group_size, n_firings, weight_shift);

        int32_t n_t_buffered = 0;
        int32_t n_w_buffered = 0;
        for(int32_t cursor=0;cursor < current_group_size;cursor++, current_target_ptr++, weight_ptr++, weight_shift++) {
            if(((reinterpret_cast<uintptr_t>(current_target_ptr) & 15) == 0) && (cursor < current_group_size - 3)) {
                targets_quad = *reinterpret_cast<uint4 *>(current_target_ptr);
                target_id = targets_quad.x;
                n_t_buffered = 3;
            } else if(n_t_buffered > 0) {
                if(n_t_buffered == 3) {
                    target_id = targets_quad.y;
                } else if(n_t_buffered == 2) {
                    target_id = targets_quad.z;
                } else {
                    target_id = targets_quad.w;
                }
                n_t_buffered--;
            } else {
                target_id = *current_target_ptr;
            }

            if(((reinterpret_cast<uintptr_t>(weight_ptr) & 15) == 0) && (cursor < current_group_size - 3)) {
                weights_quad = *reinterpret_cast<float4 *>(weight_ptr);
                weight = static_cast<double>(weights_quad.x);
                n_w_buffered = 3;
            } else if(n_w_buffered > 0) {
                if(n_w_buffered == 3) {
                    weight = static_cast<double>(weights_quad.y);
                } else if(n_w_buffered == 2) {
                    weight = static_cast<double>(weights_quad.z);
                } else {
                    weight = static_cast<double>(weights_quad.w);
                }
                n_w_buffered--;
            } else {
                weight = static_cast<double>(*weight_ptr);
            }

            __SUPER_DETAILED_TRACE__("[gather_gradients_logic] target_id %d, weight %f\n", target_id, weight);

            target_id -= n_lookup_neurons;
            output_grad = static_cast<double>(output_gradients[target_id]);

            if(fabs(output_grad) > EPS) {
                grad_x += weight * output_grad;
                #ifdef INTEGERS_INSTEAD_OF_FLOATS
                    sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
                    #ifdef ATOMIC
                    atomicAdd(
                        reinterpret_cast<SUMMATION32_DT *>(target_weights_gradients) + weight_shift,
                        sum32_delta
                    );
                    #else
                    *reinterpret_cast<SUMMATION32_DT *>(target_weights_gradients + weight_shift) += sum32_delta;
                    #endif
                #else
                    sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr);
                    #ifdef ATOMIC
                    atomicAdd(
                        target_weights_gradients + weight_shift,
                        sum32_delta
                    );
                    #else
                    target_weights_gradients[weight_shift] += sum32_delta;
                    #endif
                #endif
            }
        }

        #ifdef INTEGERS_INSTEAD_OF_FLOATS
            sum32_delta = static_cast<SUMMATION32_DT>(grad_x * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(
                target_before_detectors_gradients + fws_group.source_neuron_index,
                sum32_delta
            );
            #else
            target_before_detectors_gradients[fws_group.source_neuron_index] += sum32_delta;
            #endif
        #else
            sum32_delta = static_cast<SUMMATION32_DT>(grad_x * lr);
            #ifdef ATOMIC
            atomicAdd(
                target_before_detectors_gradients + fws_group.source_neuron_index,
                sum32_delta
            );
            #else
            target_before_detectors_gradients[fws_group.source_neuron_index] += sum32_delta;
            #endif
        #endif
    }
}

gather_gradients_fully_connected_logic(
    EXTERNAL_REAL_DT* weights,
    EXTERNAL_REAL_DT* output_gradients,
    int32_t* lookup_indices,
    int32_t* min_anchor_delta_indices,
    SUMMATION32_DT* target_before_detectors_gradients,
    EXTERNAL_REAL_DT* target_weights_gradients,
    uint32_t n_outputs,
    uint32_t n_detectors,
    uint32_t n_output_blocks,
    int32_t n_outputs_per_block,
    uint32_t n_lookup_neurons_per_detector,
    double lr,
    double int_rescaler
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;
    if(i < n_detectors * n_output_blocks) {
        uint32_t detector_index = i / n_output_blocks;
        uint32_t output_block_index = i % n_output_blocks;
        lookup_indices += blockIdx.y * n_detectors;
        int32_t lookup_index = lookup_indices[detector_index];
        if(min_anchor_delta_indices != nullptr) {
            min_anchor_delta_indices += blockIdx.y * n_detectors;
            lookup_index ^= (1 << min_anchor_delta_indices[detector_index]);
        }
        uint64_t weight_shift = static_cast<uint64_t>(n_lookup_neurons_per_detector) * n_outputs * detector_index;
        weight_shift += n_outputs * lookup_index + output_block_index * n_outputs_per_block;
        weights += weight_shift;
        target_weights_gradients += weight_shift;

        output_gradients += blockIdx.y * n_outputs + output_block_index * n_outputs_per_block;
        target_before_detectors_gradients += blockIdx.y * n_detectors * n_lookup_neurons_per_detector;
        target_before_detectors_gradients += detector_index * n_lookup_neurons_per_detector;
        if((output_block_index == n_output_blocks - 1) && (n_outputs % n_outputs_per_block)) {
            n_outputs_per_block = n_outputs % n_outputs_per_block;
        }
        double grad_x = 0.0;
        int32_t n_o_buffered = 0;
        int32_t n_w_buffered = 0;
        float4 o_quad = make_float4(0.0, 0.0, 0.0, 0.0);
        float4 w_quad = make_float4(0.0, 0.0, 0.0, 0.0);
        double output_grad;
        double weight;
        SUMMATION32_DT sum32_delta;
        for(int32_t cursor=0;cursor < n_outputs_per_block;cursor++, output_gradients++, weights++) {
            if(((reinterpret_cast<uintptr_t>(output_gradients) & 15) == 0) && (cursor < n_outputs_per_block - 3)) {
                #ifdef ATOMIC
                asm volatile(
                    "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                    : "=f"(o_quad.x), "=f"(o_quad.y), "=f"(o_quad.z), "=f"(o_quad.w)
                    : "l"(output_gradients)
                );
                #else
                o_quad = *reinterpret_cast<float4 *>(output_gradients);
                #endif
                output_grad = static_cast<double>(o_quad.x);
                n_o_buffered = 3;
            } else if(n_o_buffered > 0) {
                if(n_o_buffered == 3) {
                    output_grad = static_cast<double>(o_quad.y);
                } else if(n_o_buffered == 2) {
                    output_grad = static_cast<double>(o_quad.z);
                } else {
                    output_grad = static_cast<double>(o_quad.w);
                }
                n_o_buffered--;
            } else {
                output_grad = static_cast<double>(*output_gradients);
            }

            if(((reinterpret_cast<uintptr_t>(weights) & 15) == 0) && (cursor < n_outputs_per_block - 3)) {
                #ifdef ATOMIC
                asm volatile(
                    "ld.global.v4.f32 {%0,%1,%2,%3}, [%4];"
                    : "=f"(w_quad.x), "=f"(w_quad.y), "=f"(w_quad.z), "=f"(w_quad.w)
                    : "l"(weights)
                );
                #else
                w_quad = *reinterpret_cast<float4 *>(weights);
                #endif
                weight = static_cast<double>(w_quad.x);
                n_w_buffered = 3;
            } else if(n_w_buffered > 0) {
                if(n_w_buffered == 3) {
                    weight = static_cast<double>(w_quad.y);
                } else if(n_w_buffered == 2) {
                    weight = static_cast<double>(w_quad.z);
                } else {
                    weight = static_cast<double>(w_quad.w);
                }
                n_w_buffered--;
            } else {
                weight = static_cast<double>(*weights);
            }

            if(fabs(output_grad) > EPS) {
                grad_x += weight * output_grad;
                #ifdef INTEGERS_INSTEAD_OF_FLOATS
                    sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
                    #ifdef ATOMIC
                    atomicAdd(
                        reinterpret_cast<SUMMATION32_DT *>(target_weights_gradients) + cursor,
                        sum32_delta
                    );
                    #else
                    *reinterpret_cast<SUMMATION32_DT *>(target_weights_gradients + cursor) += sum32_delta;
                    #endif
                #else
                    sum32_delta = static_cast<SUMMATION32_DT>(output_grad * lr);
                    #ifdef ATOMIC
                    atomicAdd(
                        target_weights_gradients + cursor,
                        sum32_delta
                    );
                    #else
                    target_weights_gradients[cursor] += sum32_delta;
                    #endif
                #endif
            }
        }

        #ifdef INTEGERS_INSTEAD_OF_FLOATS
            sum32_delta = static_cast<SUMMATION32_DT>(grad_x * lr * static_cast<double>(DENOMINATOR32) * int_rescaler);
            #ifdef ATOMIC
            atomicAdd(
                target_before_detectors_gradients + lookup_index,
                sum32_delta
            );
            #else
            target_before_detectors_gradients[lookup_index] += sum32_delta;
            #endif
        #else
            sum32_delta = static_cast<SUMMATION32_DT>(grad_x * lr);
            #ifdef ATOMIC
            atomicAdd(
                target_before_detectors_gradients + lookup_index,
                sum32_delta
            );
            #else
            target_before_detectors_gradients[lookup_index] += sum32_delta;
            #endif
        #endif
    }
}

propagate_through_detectors_logic(
    int32_t* lookup_indices,
    EXTERNAL_REAL_DT* min_anchor_deltas,
    int32_t* min_anchor_delta_indices,
    uint32_t n_detectors,
    uint32_t n_anchors_per_detector,
    AnchorsPair* detectors,
    uint32_t n_lookup_neurons_per_detector,
    SUMMATION32_DT* before_detectors_gradients,
    EXTERNAL_REAL_DT* target_input_gradients,
    uint32_t n_inputs,
    double int_rescaler
) {
    uint32_t i = blockIdx.x * blockDim.x + threadIdx.x;

    if(i < n_detectors) {
        uint64_t shift = blockIdx.y * n_detectors;
        lookup_indices += shift;
        min_anchor_deltas += shift;
        min_anchor_delta_indices += shift;
        before_detectors_gradients += shift * n_lookup_neurons_per_detector;
        target_input_gradients += blockIdx.y * n_inputs;

        int32_t lookup_index = lookup_indices[i];
        int32_t min_anchor_delta_index = min_anchor_delta_indices[i];
        int32_t alt_lookup_neuron_idx = lookup_index ^ (1 << min_anchor_delta_index);

        double du = static_cast<double>(min_anchor_deltas[i]);
        if(du > 0) {
            du = 1.0 / (1.0 + fabs(du));
            du *= 0.5 * du;
        } else {
            du = 1.0 / (1.0 + fabs(du));
            du *= -0.5 * du;
        }
        du *= static_cast<double>(before_detectors_gradients[lookup_index]) - static_cast<double>(before_detectors_gradients[alt_lookup_neuron_idx]);

        if(fabs(du) > EPS) {
            AnchorsPair ap = detectors[i * n_anchors_per_detector + min_anchor_delta_index];
        
            #ifdef INTEGERS_INSTEAD_OF_FLOATS
                SUMMATION32_DT sum32_delta = static_cast<SUMMATION32_DT>(du * static_cast<double>(DENOMINATOR32) * int_rescaler);
                #ifdef ATOMIC
                atomicAdd(
                    reinterpret_cast<SUMMATION32_DT *>(target_input_gradients) + ap.anchor1_id,
                    sum32_delta
                );
                atomicAdd(
                    reinterpret_cast<SUMMATION32_DT *>(target_input_gradients) + ap.anchor2_id,
                    -sum32_delta
                );
                #else
                *reinterpret_cast<SUMMATION32_DT *>(target_input_gradients + ap.anchor1_id) += sum32_delta;
                *reinterpret_cast<SUMMATION32_DT *>(target_input_gradients + ap.anchor2_id) -= sum32_delta;
                #endif
            #else
                #ifdef ATOMIC
                atomicAdd(
                    target_input_gradients + ap.anchor1_id,
                    du
                );
                atomicAdd(
                    target_input_gradients + ap.anchor2_id,
                    -du
                );
                #else
                target_input_gradients[ap.anchor1_id] += du;
                target_input_gradients[ap.anchor2_id] -= du;
                #endif
            #endif
        }
    }
}

convert_integers_to_floats_logic(
    EXTERNAL_REAL_DT* buffer,
    uint64_t n,
    double int_rescaler
) {
    uint64_t i = static_cast<uint64_t>(blockIdx.x) * blockDim.x + threadIdx.x;
    if(i < n) {
        #ifdef INTEGERS_INSTEAD_OF_FLOATS
        buffer += n * blockIdx.y;
        double y = static_cast<double>(*reinterpret_cast<SUMMATION32_DT *>(buffer + i)) * static_cast<double>(DENOMINATOR32_RECIPROC);
        buffer[i] = static_cast<EXTERNAL_REAL_DT>(y / int_rescaler);
        #endif
    }
}
