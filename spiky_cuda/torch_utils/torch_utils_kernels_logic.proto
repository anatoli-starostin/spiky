densify_logic(
    int32_t* rw_source,
    uint64_t n_elements,
    int32_t* w_target_values,
    int64_t* w_target_indices,
    uint64_t* rw_counter_ptr,
    bool erase_input,
    int device
) {
    uint64_t i = static_cast<uint64_t>(blockIdx.x) * blockDim.x + threadIdx.x;

    int64_t idx = -1;
    int32_t value = 0;

    if(i < n_elements) {
        value = rw_source[i];

        if(value != 0) {
            idx = static_cast<int64_t>(i);
            // Erase source if requested
            if(erase_input) {
                rw_source[i] = 0;
            }
        }
    }

    if(device == -1) {
        // CPU implementation
        if(idx >= 0) {
            uint64_t offset = (*rw_counter_ptr)++;
            w_target_indices[offset] = idx;
            w_target_values[offset] = value;
        }
    } else {
        #ifdef ATOMIC
        extern __shared__ __align__(16) uint8_t __sm[];
        int64_t *sdata = reinterpret_cast<int64_t *>(__sm);

        uint32_t tid = threadIdx.x;
        sdata[tid] = (idx >= 0) ? 1 : 0;
        __syncthreads();

        uint64_t t;
        int offset;
        int idx_shared;

        // upsweep (reduce)
        for(offset = 1; offset < blockDim.x; offset <<= 1) {
            idx_shared = ((tid + 1) * (offset << 1)) - 1;
            if(idx_shared < blockDim.x) {
                t = sdata[idx_shared - offset];
                if(t > 0) {
                    sdata[idx_shared] += t;
                }
            }
            __syncthreads();
        }

        // inject global shift
        if(tid == 0) {
            sdata[blockDim.x - 1] = atomicAdd(
                reinterpret_cast<unsigned long long*>(rw_counter_ptr),
                static_cast<unsigned long long>(sdata[blockDim.x - 1])
            );
        }
        __syncthreads();

        // downsweep
        for(offset = blockDim.x >> 1; offset > 0; offset >>= 1) {
            idx_shared = ((tid + 1) * (offset << 1)) - 1;
            if (idx_shared < blockDim.x) {
                t = sdata[idx_shared - offset];
                sdata[idx_shared - offset] = sdata[idx_shared];
                if(t > 0) {
                    sdata[idx_shared] += t;
                }
            }
            __syncthreads();
        }

        if(idx >= 0) {
            int64_t offset = sdata[tid];
            w_target_indices[offset] = idx;
            w_target_values[offset] = value;
        }
        #endif
    }
}

count_nonzero_logic(
    int4* array,
    uint64_t n_quads,
    uint64_t n_elements,
    uint64_t* aux_buffer,
    int device
) {
    unsigned int tid = threadIdx.x;
    uint64_t quad_idx = static_cast<uint64_t>(blockIdx.x) * blockDim.x + tid;
    uint32_t cnt = 0;

    if(quad_idx < n_quads) {
        uint64_t base_idx = quad_idx << 2;
        bool is_last_quad = (quad_idx == n_quads - 1);
        uint64_t remaining = n_elements - base_idx;
        
        if(is_last_quad && remaining < 4) {
            // Last incomplete quad - read individual int32_t values to avoid segfault
            int32_t* elem_ptr = reinterpret_cast<int32_t*>(array) + base_idx;
            if(remaining > 0 && elem_ptr[0] != 0) cnt++;
            if(remaining > 1 && elem_ptr[1] != 0) cnt++;
            if(remaining > 2 && elem_ptr[2] != 0) cnt++;
        } else {
            // Complete quad - safe to read int4
            int4 quad = array[quad_idx];
            if(quad.x != 0) cnt++;
            if(quad.y != 0) cnt++;
            if(quad.z != 0) cnt++;
            if(quad.w != 0) cnt++;
        }
    }

    if(device == -1) {
        if(cnt > 0) {
            aux_buffer[0] += cnt;
        }
    } else {
        #ifdef ATOMIC
        extern __shared__ __align__(16) uint8_t __sm[];
        uint32_t *sdata = reinterpret_cast<uint32_t *>(__sm);
        sdata[tid] = cnt;
        __syncthreads();

        for(unsigned int s = blockDim.x >> 1; s > 0; s >>= 1){
            if(tid < s) {
                sdata[tid] += sdata[tid + s];
            }
            __syncthreads();
        }
        if(tid == 0) {
            if(sdata[0] > 0) {
                atomicAdd(reinterpret_cast<unsigned long long*>(aux_buffer), static_cast<unsigned long long>(sdata[0]));
            }
        }
        #endif
    }
}

