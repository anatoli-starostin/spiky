densify_logic(
    int4* __restrict__ rw_source,
    uint64_t n_quads,
    int32_t* __restrict__ w_target_values,
    int64_t* __restrict__ w_target_indices,
    uint32_t* __restrict__ rw_counter_ptr,
    bool erase_input,
    int device
) {
    unsigned int tid = threadIdx.x;
    uint64_t quad_idx = static_cast<uint64_t>(blockIdx.x) * blockDim.x + tid;

    #ifndef NO_CUDA
    __align__(16) int4 quad = make_int4(0, 0, 0, 0);
    #else
    int4 quad = make_int4(0, 0, 0, 0);
    #endif
    uint32_t n_nonzero = 0;

    if(quad_idx < n_quads) {
        quad = rw_source[quad_idx];

        if(quad.x != 0) {
            n_nonzero++;
        }
        if(quad.y != 0) {
            n_nonzero++;
        }
        if(quad.z != 0) {
            n_nonzero++;
        }
        if(quad.w != 0) {
            n_nonzero++;
        }

        if(erase_input) {
            rw_source[quad_idx] = make_int4(0, 0, 0, 0);
        }
    }

    if(device == -1) {
        if(n_nonzero > 0) {
            uint32_t offset = *rw_counter_ptr;
            uint64_t base_idx = quad_idx << 2;
            if(quad.x != 0) {
                w_target_indices[offset] = base_idx;
                w_target_values[offset++] = quad.x;
            }
            if(quad.y != 0) {
                w_target_indices[offset] = base_idx + 1;
                w_target_values[offset++] = quad.y;
            }
            if(quad.z != 0) {
                w_target_indices[offset] = base_idx + 2;
                w_target_values[offset++] = quad.z;
            }
            if(quad.w != 0) {
                w_target_indices[offset] = base_idx + 3;
                w_target_values[offset++] = quad.w;
            }
            *rw_counter_ptr = offset + n_nonzero;
        }
    } else {
        #ifdef ATOMIC
        extern __shared__ __align__(16) uint8_t __sm[];
        uint32_t *sdata = reinterpret_cast<uint32_t *>(__sm);

        sdata[tid] = n_nonzero;
        __syncthreads();

        uint32_t t;
        int offset;
        int idx_shared;

        // upsweep (reduce)
        for(offset = 1; offset < blockDim.x; offset <<= 1) {
            idx_shared = ((tid + 1) * (offset << 1)) - 1;
            if(idx_shared < blockDim.x) {
                t = sdata[idx_shared - offset];
                if(t > 0) {
                    sdata[idx_shared] += t;
                }
            }
            __syncthreads();
        }

        // inject global shift
        if(tid == 0) {
            sdata[blockDim.x - 1] = atomicAdd(
                rw_counter_ptr,
                sdata[blockDim.x - 1]
            );
        }
        __syncthreads();

        // downsweep
        for(offset = blockDim.x >> 1; offset > 0; offset >>= 1) {
            idx_shared = ((tid + 1) * (offset << 1)) - 1;
            if (idx_shared < blockDim.x) {
                t = sdata[idx_shared - offset];
                sdata[idx_shared - offset] = sdata[idx_shared];
                if(t > 0) {
                    sdata[idx_shared] += t;
                }
            }
            __syncthreads();
        }

        if(n_nonzero > 0) {
            uint32_t offset = sdata[tid];
            uint64_t base_idx = quad_idx << 2;
            if(quad.x != 0) {
                w_target_indices[offset] = base_idx;
                w_target_values[offset++] = quad.x;
            }
            if(quad.y != 0) {
                w_target_indices[offset] = base_idx + 1;
                w_target_values[offset++] = quad.y;
            }
            if(quad.z != 0) {
                w_target_indices[offset] = base_idx + 2;
                w_target_values[offset++] = quad.z;
            }
            if(quad.w != 0) {
                w_target_indices[offset] = base_idx + 3;
                w_target_values[offset++] = quad.w;
            }
        }
        #endif
    }
}

count_nonzero_logic(
    int4* array,
    uint64_t n_quads,
    uint32_t* aux_buffer,
    int device
) {
    unsigned int tid = threadIdx.x;
    uint64_t quad_idx = static_cast<uint64_t>(blockIdx.x) * blockDim.x + tid;
    uint32_t cnt = 0;

    if(quad_idx < n_quads) {
        __align__(16) int4 quad = array[quad_idx];
        if(quad.x != 0) cnt++;
        if(quad.y != 0) cnt++;
        if(quad.z != 0) cnt++;
        if(quad.w != 0) cnt++;
    }

    if(device == -1) {
        if(cnt > 0) {
            aux_buffer[0] += cnt;
        }
    } else {
        #ifdef ATOMIC
        extern __shared__ __align__(16) uint8_t __sm[];
        uint32_t *sdata = reinterpret_cast<uint32_t *>(__sm);
        sdata[tid] = cnt;
        __syncthreads();

        for(unsigned int s = blockDim.x >> 1; s > 0; s >>= 1){
            if(tid < s) {
                sdata[tid] += sdata[tid + s];
            }
            __syncthreads();
        }
        if(tid == 0) {
            if(sdata[0] > 0) {
                atomicAdd(aux_buffer, sdata[0]);
            }
        }
        #endif
    }
}

densify_new_logic(
    int4* __restrict__ rw_source,
    uint64_t n_quads,
    int32_t* __restrict__ w_target_values,
    int64_t* __restrict__ w_target_indices,
    uint32_t* __restrict__ rw_counter_ptr,
    bool erase_input,
    int device
)
{
#ifdef ATOMIC
    int tpb    = blockDim.x;       // multiple of 32 guaranteed
    int tid    = threadIdx.x;
    int lane   = tid & 31;
    int warpId = tid >> 5;
    int NWARPS = tpb >> 5;         // tpb / 32

    const unsigned FULL = 0xffffffffu;

    __shared__ uint32_t warp_sums[32];   // up to 1024 threads
    __shared__ uint32_t block_base;

    uint64_t quad_idx = uint64_t(blockIdx.x) * tpb + tid;

    // --- load one quad + compute nnz mask ---
    int4 quad = make_int4(0, 0, 0, 0);
    uint32_t mask = 0;

    if (quad_idx < n_quads) {
        quad = rw_source[quad_idx];

        mask  = (quad.x != 0);
        mask |= (uint32_t)(quad.y != 0) << 1;
        mask |= (uint32_t)(quad.z != 0) << 2;
        mask |= (uint32_t)(quad.w != 0) << 3;
    }

    uint32_t local_nnz = __popc(mask);   // 0..4

    // --- warp inclusive scan on local_nnz ---
    uint32_t prefix = local_nnz;
    #pragma unroll
    for (int off = 1; off < 32; off <<= 1) {
        uint32_t x = __shfl_up_sync(FULL, prefix, off);
        if (lane >= off) prefix += x;
    }

    if (lane == 31)
        warp_sums[warpId] = prefix;

    __syncthreads();

    // --- warp 0 scans warp sums ---
    if (warpId == 0) {
        uint32_t wv = (lane < NWARPS) ? warp_sums[lane] : 0;

        #pragma unroll
        for (int off = 1; off < NWARPS; off <<= 1) {
            uint32_t x = __shfl_up_sync(FULL, wv, off);
            if (lane >= off) wv += x;
        }

        if (lane < NWARPS)
            warp_sums[lane] = wv;
    }

    __syncthreads();

    if (warpId > 0)
        prefix += warp_sums[warpId - 1];

    // --- one atomic per block ---
    if (tid == 0) {
        uint32_t block_sum = warp_sums[NWARPS - 1];
        block_base = atomicAdd(rw_counter_ptr, block_sum);
    }

    __syncthreads();

    // global inclusive prefix
    prefix += block_base;

    // convert inclusive â†’ exclusive
    prefix -= local_nnz;

    // --- write out using mask and prefix directly ---
    if (local_nnz > 0) {
        uint64_t base_idx = quad_idx << 2;  // *4 ints

        if (mask & 1) {
            w_target_indices[prefix++] = base_idx;
        }
        if (mask & 2) {
            w_target_indices[prefix++] = base_idx + 1;
        }
        if (mask & 4) {
            w_target_indices[prefix++] = base_idx + 2;
        }
        if (mask & 8) {
            w_target_indices[prefix++] = base_idx + 3;
        }

        prefix -= local_nnz;

        if (mask & 1) {
            w_target_values[prefix++]  = quad.x;
        }
        if (mask & 2) {
            w_target_values[prefix++]  = quad.y;
        }
        if (mask & 4) {
            w_target_values[prefix++]  = quad.z;
        }
        if (mask & 8) {
            w_target_values[prefix++]  = quad.w;
        }

        if(erase_input) {
            if(local_nnz > 1) {
                void* p = &rw_source[quad_idx];  // 16-byte aligned
                asm volatile (
                    "st.global.v2.u64 [%0], {%1, %2};\n"
                    :
                    : "l"(p), "l"(0ull), "l"(0ull)
                    : "memory"
                );
            } else {
                if (mask & 1) {
                    rw_source[quad_idx].x = 0;
                }
                if (mask & 2) {
                    rw_source[quad_idx].y = 0;
                }
                if (mask & 4) {
                    rw_source[quad_idx].z = 0;
                }
                if (mask & 8) {
                    rw_source[quad_idx].w = 0;
                }
            }
        }
    }
#endif
}
