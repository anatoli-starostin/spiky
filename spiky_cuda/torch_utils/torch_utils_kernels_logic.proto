densify_logic(
    int4* rw_source,
    uint64_t n_quads,
    uint64_t n_elements,
    int32_t* w_target_values,
    int64_t* w_target_indices,
    uint32_t* rw_counter_ptr,
    bool erase_input,
    int device
) {
    unsigned int tid = threadIdx.x;
    uint64_t quad_idx = static_cast<uint64_t>(blockIdx.x) * blockDim.x + tid;
    
    // Store up to 4 non-zero elements per thread
    int32_t values[4];
    int64_t indices[4];
    uint32_t n_nonzero = 0;
    uint64_t base_idx = quad_idx << 2;

    if(quad_idx < n_quads) {
        bool is_last_quad = (quad_idx == n_quads - 1);
        uint64_t remaining = n_elements - base_idx;
        
        if(is_last_quad && remaining < 4) {
            // Last incomplete quad - read individual int32_t values to avoid segfault
            int32_t* elem_ptr = reinterpret_cast<int32_t*>(rw_source) + base_idx;
            for(uint32_t j = 0; j < remaining; j++) {
                int32_t val = elem_ptr[j];
                if(val != 0) {
                    values[n_nonzero] = val;
                    indices[n_nonzero] = static_cast<int64_t>(base_idx + j);
                    n_nonzero++;
                    // Erase source if requested
                    if(erase_input) {
                        elem_ptr[j] = 0;
                    }
                }
            }
        } else {
            // Complete quad - safe to read int4
            int4 quad = rw_source[quad_idx];
            if(quad.x != 0) {
                values[n_nonzero] = quad.x;
                indices[n_nonzero] = static_cast<int64_t>(base_idx);
                n_nonzero++;
            }
            if(quad.y != 0) {
                values[n_nonzero] = quad.y;
                indices[n_nonzero] = static_cast<int64_t>(base_idx + 1);
                n_nonzero++;
            }
            if(quad.z != 0) {
                values[n_nonzero] = quad.z;
                indices[n_nonzero] = static_cast<int64_t>(base_idx + 2);
                n_nonzero++;
            }
            if(quad.w != 0) {
                values[n_nonzero] = quad.w;
                indices[n_nonzero] = static_cast<int64_t>(base_idx + 3);
                n_nonzero++;
            }
            // Erase source if requested
            if(erase_input) {
                rw_source[quad_idx] = make_int4(0, 0, 0, 0);
            }
        }
    }

    if(device == -1) {
        // CPU implementation
        for(uint32_t j = 0; j < n_nonzero; j++) {
            uint32_t offset = (*rw_counter_ptr)++;
            w_target_indices[offset] = indices[j];
            w_target_values[offset] = values[j];
        }
    } else {
        #ifdef ATOMIC
        extern __shared__ __align__(16) uint8_t __sm[];
        uint32_t *sdata = reinterpret_cast<uint32_t *>(__sm);

        sdata[tid] = n_nonzero;
        __syncthreads();

        uint32_t t;
        int offset;
        int idx_shared;

        // upsweep (reduce)
        for(offset = 1; offset < blockDim.x; offset <<= 1) {
            idx_shared = ((tid + 1) * (offset << 1)) - 1;
            if(idx_shared < blockDim.x) {
                t = sdata[idx_shared - offset];
                if(t > 0) {
                    sdata[idx_shared] += t;
                }
            }
            __syncthreads();
        }

        // inject global shift
        if(tid == 0) {
            sdata[blockDim.x - 1] = atomicAdd(
                rw_counter_ptr,
                sdata[blockDim.x - 1]
            );
        }
        __syncthreads();

        // downsweep
        for(offset = blockDim.x >> 1; offset > 0; offset >>= 1) {
            idx_shared = ((tid + 1) * (offset << 1)) - 1;
            if (idx_shared < blockDim.x) {
                t = sdata[idx_shared - offset];
                sdata[idx_shared - offset] = sdata[idx_shared];
                if(t > 0) {
                    sdata[idx_shared] += t;
                }
            }
            __syncthreads();
        }

        // Write all non-zero elements from this thread with vectorized writes
        uint32_t base_offset = static_cast<uint32_t>(sdata[tid]);
        
        if(n_nonzero == 4 && (base_offset & 3) == 0) {
            // Write quad (int4) for values - base_offset aligned to 4 elements (16 bytes)
            int4* values_quad_ptr = reinterpret_cast<int4*>(w_target_values + base_offset);
            *values_quad_ptr = *reinterpret_cast<int4*>(values);
            // Write indices with duplex writes
            longlong2* indices_duplex_ptr0 = reinterpret_cast<longlong2*>(w_target_indices + base_offset);
            *indices_duplex_ptr0 = *reinterpret_cast<longlong2*>(indices);
            longlong2* indices_duplex_ptr1 = reinterpret_cast<longlong2*>(w_target_indices + base_offset + 2);
            *indices_duplex_ptr1 = *reinterpret_cast<longlong2*>(indices + 2);
        } else if(n_nonzero == 3 && (base_offset & 1) == 0) {
            // Write duplex (int2) + single for values - base_offset aligned to 2 elements (8 bytes)
            int2* values_duplex_ptr = reinterpret_cast<int2*>(w_target_values + base_offset);
            *values_duplex_ptr = *reinterpret_cast<int2*>(values);
            w_target_values[base_offset + 2] = values[2];
            // Write indices with duplex + single
            longlong2* indices_duplex_ptr = reinterpret_cast<longlong2*>(w_target_indices + base_offset);
            *indices_duplex_ptr = *reinterpret_cast<longlong2*>(indices);
            w_target_indices[base_offset + 2] = indices[2];
        } else if(n_nonzero == 2 && (base_offset & 1) == 0) {
            // Write duplex (int2) for values - base_offset aligned to 2 elements (8 bytes)
            int2* values_duplex_ptr = reinterpret_cast<int2*>(w_target_values + base_offset);
            *values_duplex_ptr = *reinterpret_cast<int2*>(values);
            // Write indices with duplex
            longlong2* indices_duplex_ptr = reinterpret_cast<longlong2*>(w_target_indices + base_offset);
            *indices_duplex_ptr = *reinterpret_cast<longlong2*>(indices);
        } else {
            // Fallback: write individually without loop
            if(n_nonzero > 0) {
                w_target_indices[base_offset] = indices[0];
                w_target_values[base_offset] = values[0];
            }
            if(n_nonzero > 1) {
                w_target_indices[base_offset + 1] = indices[1];
                w_target_values[base_offset + 1] = values[1];
            }
            if(n_nonzero > 2) {
                w_target_indices[base_offset + 2] = indices[2];
                w_target_values[base_offset + 2] = values[2];
            }
            if(n_nonzero > 3) {
                w_target_indices[base_offset + 3] = indices[3];
                w_target_values[base_offset + 3] = values[3];
            }
        }
        #endif
    }
}

count_nonzero_logic(
    int4* array,
    uint64_t n_quads,
    uint64_t n_elements,
    uint32_t* aux_buffer,
    int device
) {
    unsigned int tid = threadIdx.x;
    uint64_t quad_idx = static_cast<uint64_t>(blockIdx.x) * blockDim.x + tid;
    uint32_t cnt = 0;

    if(quad_idx < n_quads) {
        uint64_t base_idx = quad_idx << 2;
        bool is_last_quad = (quad_idx == n_quads - 1);
        uint64_t remaining = n_elements - base_idx;
        
        if(is_last_quad && remaining < 4) {
            // Last incomplete quad - read individual int32_t values to avoid segfault
            int32_t* elem_ptr = reinterpret_cast<int32_t*>(array) + base_idx;
            if(remaining > 0 && elem_ptr[0] != 0) cnt++;
            if(remaining > 1 && elem_ptr[1] != 0) cnt++;
            if(remaining > 2 && elem_ptr[2] != 0) cnt++;
        } else {
            // Complete quad - safe to read int4
            int4 quad = array[quad_idx];
            if(quad.x != 0) cnt++;
            if(quad.y != 0) cnt++;
            if(quad.z != 0) cnt++;
            if(quad.w != 0) cnt++;
        }
    }

    if(device == -1) {
        if(cnt > 0) {
            aux_buffer[0] += cnt;
        }
    } else {
        #ifdef ATOMIC
        extern __shared__ __align__(16) uint8_t __sm[];
        uint32_t *sdata = reinterpret_cast<uint32_t *>(__sm);
        sdata[tid] = cnt;
        __syncthreads();

        for(unsigned int s = blockDim.x >> 1; s > 0; s >>= 1){
            if(tid < s) {
                sdata[tid] += sdata[tid + s];
            }
            __syncthreads();
        }
        if(tid == 0) {
            if(sdata[0] > 0) {
                atomicAdd(aux_buffer, sdata[0]);
            }
        }
        #endif
    }
}

