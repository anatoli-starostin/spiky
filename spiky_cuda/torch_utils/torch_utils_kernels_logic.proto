densify_logic(
    int4* rw_source,
    uint64_t n_quads,
    int32_t* w_target_values,
    int64_t* w_target_indices,
    uint32_t* rw_counter_ptr,
    bool erase_input,
    int device
) {
    unsigned int tid = threadIdx.x;
    uint64_t quad_idx = static_cast<uint64_t>(blockIdx.x) * blockDim.x + tid;
    
    __align__(16) int4 quad = make_int4(0, 0, 0, 0);
    uint32_t n_nonzero = 0;

    if(quad_idx < n_quads) {
        int4 quad = rw_source[quad_idx];

        if(quad.x != 0) {
            n_nonzero++;
        }
        if(quad.y != 0) {
            n_nonzero++;
        }
        if(quad.z != 0) {
            n_nonzero++;
        }
        if(quad.w != 0) {
            n_nonzero++;
        }

        if(erase_input) {
            rw_source[quad_idx] = make_int4(0, 0, 0, 0);
        }
    }

    if(device == -1) {
        if(n_nonzero > 0) {
            uint32_t offset = *rw_counter_ptr;
            uint64_t base_idx = quad_idx << 2;
            if(quad.x != 0) {
                w_target_indices[offset] = base_idx;
                w_target_values[offset++] = quad.x;
            }
            if(quad.y != 0) {
                w_target_indices[offset] = base_idx + 1;
                w_target_values[offset++] = quad.y;
            }
            if(quad.z != 0) {
                w_target_indices[offset] = base_idx + 2;
                w_target_values[offset++] = quad.z;
            }
            if(quad.w != 0) {
                w_target_indices[offset] = base_idx + 3;
                w_target_values[offset++] = quad.w;
            }
            *rw_counter_ptr = offset + n_nonzero;
        }
    } else {
        #ifdef ATOMIC
        extern __shared__ __align__(16) uint8_t __sm[];
        uint32_t *sdata = reinterpret_cast<uint32_t *>(__sm);

        sdata[tid] = n_nonzero;
        __syncthreads();

        uint32_t t;
        int offset;
        int idx_shared;

        // upsweep (reduce)
        for(offset = 1; offset < blockDim.x; offset <<= 1) {
            idx_shared = ((tid + 1) * (offset << 1)) - 1;
            if(idx_shared < blockDim.x) {
                t = sdata[idx_shared - offset];
                if(t > 0) {
                    sdata[idx_shared] += t;
                }
            }
            __syncthreads();
        }

        // inject global shift
        if(tid == 0) {
            sdata[blockDim.x - 1] = atomicAdd(
                rw_counter_ptr,
                sdata[blockDim.x - 1]
            );
        }
        __syncthreads();

        // downsweep
        for(offset = blockDim.x >> 1; offset > 0; offset >>= 1) {
            idx_shared = ((tid + 1) * (offset << 1)) - 1;
            if (idx_shared < blockDim.x) {
                t = sdata[idx_shared - offset];
                sdata[idx_shared - offset] = sdata[idx_shared];
                if(t > 0) {
                    sdata[idx_shared] += t;
                }
            }
            __syncthreads();
        }

        if(n_nonzero > 0) {
            uint32_t offset = sdata[tid];
            uint64_t base_idx = quad_idx << 2;
            if(quad.x != 0) {
                w_target_indices[offset] = base_idx;
                w_target_values[offset++] = quad.x;
            }
            if(quad.y != 0) {
                w_target_indices[offset] = base_idx + 1;
                w_target_values[offset++] = quad.y;
            }
            if(quad.z != 0) {
                w_target_indices[offset] = base_idx + 2;
                w_target_values[offset++] = quad.z;
            }
            if(quad.w != 0) {
                w_target_indices[offset] = base_idx + 3;
                w_target_values[offset++] = quad.w;
            }
        }
        #endif
    }
}

count_nonzero_logic(
    int4* array,
    uint64_t n_quads,
    uint32_t* aux_buffer,
    int device
) {
    unsigned int tid = threadIdx.x;
    uint64_t quad_idx = static_cast<uint64_t>(blockIdx.x) * blockDim.x + tid;
    uint32_t cnt = 0;

    if(quad_idx < n_quads) {
        __align__(16) int4 quad = array[quad_idx];
        if(quad.x != 0) cnt++;
        if(quad.y != 0) cnt++;
        if(quad.z != 0) cnt++;
        if(quad.w != 0) cnt++;
    }

    if(device == -1) {
        if(cnt > 0) {
            aux_buffer[0] += cnt;
        }
    } else {
        #ifdef ATOMIC
        extern __shared__ __align__(16) uint8_t __sm[];
        uint32_t *sdata = reinterpret_cast<uint32_t *>(__sm);
        sdata[tid] = cnt;
        __syncthreads();

        for(unsigned int s = blockDim.x >> 1; s > 0; s >>= 1){
            if(tid < s) {
                sdata[tid] += sdata[tid + s];
            }
            __syncthreads();
        }
        if(tid == 0) {
            if(sdata[0] > 0) {
                atomicAdd(aux_buffer, sdata[0]);
            }
        }
        #endif
    }
}
